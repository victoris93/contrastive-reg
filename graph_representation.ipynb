{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/store2/work/mrenaudi/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import asyncio\n",
    "import submitit\n",
    "import pickle\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "from nilearn.connectome import sym_matrix_to_vec\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    ")\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, TensorDataset\n",
    "from tqdm.auto import tqdm\n",
    "from augmentations import augs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "multi_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0\n",
    "AUGMENTATION = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_feat = \"/data/parietal/store/work/dwassermann/data/victoria_mat_age/matrices.npy\"\n",
    "path_target = \"/data/parietal/store/work/dwassermann/data/victoria_mat_age/data_mat_age_demian/participants.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim_feat, input_dim_target, hidden_dim_feats, output_dim, dropout_rate):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        self.feat_gcn = nn.Sequential(\n",
    "            nn.GCNConv(input_dim_feat, hidden_dim_feats),\n",
    "            nn.ReLu(),\n",
    "            nn.GCNConv(hidden_dim_feats, hidden_dim_feats),\n",
    "            nn.ReLu(),\n",
    "            nn.GCNConv(hidden_dim_feats, output_dim),\n",
    "            nn.global_mean_pool()\n",
    "        )\n",
    "        self.init_weights_gcn(self.feat_gcn)\n",
    "        \n",
    "        self.target_mlp = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_dim_target),\n",
    "            nn.Linear(input_dim_target, hidden_dim_feats),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(hidden_dim_feats, output_dim)\n",
    "        )\n",
    "        self.init_weights(self.target_gcn)\n",
    "        \n",
    "        self.decode_target = nn.Sequential(\n",
    "            nn.Linear(output_dim, hidden_dim_feats),\n",
    "            nn.BatchNorm1d(hidden_dim_feats),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim_feats, input_dim_target)\n",
    "        )\n",
    "        self.init_weights(self.decode_target)\n",
    "    \n",
    "    \n",
    "        \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "        elif isinstance(m, GCNConv):\n",
    "            # Initialize weights for GCNConv layers\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    def transform_feat(self, x):\n",
    "        features = self.feat_gcn(x)\n",
    "        #features = nn.functional.normalize(features, p=2, dim=1)\n",
    "        return features\n",
    "    \n",
    "    def transform_targets(self, y):\n",
    "        targets = self.target_mlp(y)\n",
    "        targets = nn.functional.normalize(targets, p=2, dim=1)\n",
    "        return targets\n",
    " \n",
    "    def decode_targets(self, embedding):\n",
    "        return self.decode_target(embedding)\n",
    "\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        x_embedding = self.transform_feat(x)\n",
    "        y_embedding = self.transform_feat(y)\n",
    "        return x_embedding, y_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatData(Dataset):\n",
    "    def __init__(self, path_feat, path_targets, target_name, threshold=0):\n",
    "        # self.matrices = np.load(path_feat, mmap_mode=\"r\")\n",
    "        self.matrices = np.load(path_feat, mmap_mode=\"r\").astype(np.float32)\n",
    "        self.target = torch.tensor(\n",
    "            np.expand_dims(\n",
    "                pd.read_csv(path_targets)[target_name].values, axis=1\n",
    "            ),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        if threshold > 0:\n",
    "            self.matrices = self.threshold(self.matrices, threshold)\n",
    "        self.matrices = torch.from_numpy(self.matrices).to(torch.float32)\n",
    "        gc.collect()\n",
    "\n",
    "    def threshold(self, matrices, threshold):\n",
    "        perc = np.percentile(np.abs(matrices), threshold, axis=2, keepdims=True)\n",
    "        mask = np.abs(matrices) >= perc\n",
    "        thresh_mat = matrices * mask\n",
    "        return thresh_mat\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.matrices)\n",
    "    def __getitem__(self, idx):\n",
    "        matrix = self.matrices[idx]\n",
    "        target = self.target[idx]\n",
    "        return matrix, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelizedSupCon(nn.Module):\n",
    "    \"\"\"Supervised contrastive loss: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\n",
    "    Based on: https://github.com/HobbitLong/SupContrast\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        method: str,\n",
    "        temperature: float = 0.07,\n",
    "        contrast_mode: str = \"all\",\n",
    "        base_temperature: float = 0.07,\n",
    "        krnl_sigma: float = 1.0,\n",
    "        kernel: callable = None,\n",
    "        delta_reduction: str = \"sum\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "        self.method = method\n",
    "        self.kernel = kernel\n",
    "        self.krnl_sigma = krnl_sigma\n",
    "        self.delta_reduction = delta_reduction\n",
    "\n",
    "        if kernel is not None and method == \"supcon\":\n",
    "            raise ValueError(\"Kernel must be none if method=supcon\")\n",
    "\n",
    "        if kernel is None and method != \"supcon\":\n",
    "            raise ValueError(\"Kernel must not be none if method != supcon\")\n",
    "\n",
    "        if delta_reduction not in [\"mean\", \"sum\"]:\n",
    "            raise ValueError(f\"Invalid reduction {delta_reduction}\")\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"{self.__class__.__name__} \"\n",
    "            f\"(t={self.temperature}, \"\n",
    "            f\"method={self.method}, \"\n",
    "            f\"kernel={self.kernel is not None}, \"\n",
    "            f\"delta_reduction={self.delta_reduction})\"\n",
    "        )\n",
    "\n",
    "    def forward(self, features, labels=None):\n",
    "        \"\"\"Compute loss for model. If `labels` is None,\n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, n_features].\n",
    "                input has to be rearranged to [bsz, n_views, n_features] and labels [bsz],\n",
    "            labels: ground truth of shape [bsz].\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "        device = features.device\n",
    "\n",
    "        if len(features.shape) != 3:\n",
    "            raise ValueError(\n",
    "                \"`features` needs to be [bsz, n_views, n_feats],\"\n",
    "                \"3 dimensions are required\"\n",
    "            )\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        n_views = features.shape[1]\n",
    "        if labels is None:\n",
    "            mask = torch.eye(batch_size, device=device)\n",
    "\n",
    "        else:\n",
    "            labels = labels.view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError(\"Num of labels does not match num of features\")\n",
    "\n",
    "            if self.kernel is None:\n",
    "                mask = torch.eq(labels, labels.T)\n",
    "            else:\n",
    "                mask = self.kernel(labels, krnl_sigma=self.krnl_sigma)\n",
    "\n",
    "        view_count = features.shape[1]\n",
    "        features = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == \"one\":\n",
    "            features = features[:, 0]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == \"all\":\n",
    "            features = features\n",
    "            anchor_count = view_count\n",
    "        else:\n",
    "            raise ValueError(\"Unknown mode: {}\".format(self.contrast_mode))\n",
    "\n",
    "        # Tile mask\n",
    "        mask = mask.repeat(anchor_count, view_count)\n",
    "        # Inverse of torch-eye to remove self-contrast (diagonal)\n",
    "        inv_diagonal = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * n_views, device=device).view(-1, 1),\n",
    "            0,\n",
    "        )\n",
    "\n",
    "        # compute similarity\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(features, features.T), self.temperature\n",
    "        )\n",
    "\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        alignment = logits\n",
    "\n",
    "        # base case is:\n",
    "        # - supcon if kernel = none\n",
    "        # - y-aware is kernel != none\n",
    "        uniformity = torch.exp(logits) * inv_diagonal\n",
    "\n",
    "        if self.method == \"threshold\":\n",
    "            repeated = mask.unsqueeze(-1).repeat(\n",
    "                1, 1, mask.shape[0]\n",
    "            )  # repeat kernel mask\n",
    "\n",
    "            delta = (mask[:, None].T - repeated.T).transpose(\n",
    "                1, 2\n",
    "            )  # compute the difference w_k - w_j for every k,j\n",
    "            delta = (delta > 0.0).float()\n",
    "\n",
    "            # for each z_i, repel only samples j s.t. K(z_i, z_j) < K(z_i, z_k)\n",
    "            uniformity = uniformity.unsqueeze(-1).repeat(1, 1, mask.shape[0])\n",
    "\n",
    "            if self.delta_reduction == \"mean\":\n",
    "                uniformity = (uniformity * delta).mean(-1)\n",
    "            else:\n",
    "                uniformity = (uniformity * delta).sum(-1)\n",
    "\n",
    "        elif self.method == \"expw\":\n",
    "            # exp weight e^(s_j(1-w_j))\n",
    "            uniformity = torch.exp(logits * (1 - mask)) * inv_diagonal\n",
    "\n",
    "        uniformity = torch.log(uniformity.sum(1, keepdim=True))\n",
    "\n",
    "        # positive mask contains the anchor-positive pairs\n",
    "        # excluding <self,self> on the diagonal\n",
    "        positive_mask = mask * inv_diagonal\n",
    "\n",
    "        log_prob = (\n",
    "            alignment - uniformity\n",
    "        )  # log(alignment/uniformity) = log(alignment) - log(uniformity)\n",
    "        log_prob = (positive_mask * log_prob).sum(1) / positive_mask.sum(\n",
    "            1\n",
    "        )  # compute mean of log-likelihood over positive\n",
    "\n",
    "        # loss\n",
    "        loss = -(self.temperature / self.base_temperature) * log_prob\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x, krnl_sigma):\n",
    "    x = x - x.T\n",
    "    return torch.exp(-(x**2) / (2 * (krnl_sigma**2))) / (math.sqrt(2 * torch.pi))\n",
    "\n",
    "def gaussian_kernel_on_similarity_matrix(x, krnl_sigma):\n",
    "    return torch.exp(-(x**2) / (2 * (krnl_sigma**2))) / (math.sqrt(2 * torch.pi))\n",
    "\n",
    "\n",
    "\n",
    "def gaussian_kernel_original(x, krnl_sigma):\n",
    "    x = x - x.T\n",
    "    return torch.exp(-(x**2) / (2 * (krnl_sigma**2))) / (\n",
    "        math.sqrt(2 * torch.pi) * krnl_sigma\n",
    "    )\n",
    "\n",
    "\n",
    "def cauchy(x, krnl_sigma):\n",
    "    x = x - x.T\n",
    "    return 1.0 / (krnl_sigma * (x**2) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataset, test_dataset, model=None, device=device, kernel=cauchy, num_epochs=100, batch_size=32):\n",
    "    input_dim_feat = 499500\n",
    "    # the rest is arbitrary\n",
    "    hidden_dim_feat = 1000\n",
    "    input_dim_target = 1\n",
    "    output_dim = 2\n",
    "\n",
    "    num_epochs = 100\n",
    "\n",
    "    lr = 0.1  # too low values return nan loss\n",
    "    kernel = cauchy\n",
    "    batch_size = 32  # too low values return nan loss\n",
    "    dropout_rate = 0\n",
    "    weight_decay = 0\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    if model is None:\n",
    "        model = MLP(\n",
    "            input_dim_feat,\n",
    "            input_dim_target,\n",
    "            hidden_dim_feat,\n",
    "            output_dim,\n",
    "            dropout_rate=dropout_rate,\n",
    "        ).to(device)\n",
    "        \n",
    "    criterion_pft = KernelizedSupCon(\n",
    "    method=\"expw\", temperature=0.03, base_temperature=0.03, kernel=kernel, krnl_sigma=1\n",
    "    )\n",
    "    \n",
    "    criterion_ptt = KernelizedSupCon(\n",
    "        method=\"expw\", temperature=0.03, base_temperature=0.03, kernel=kernel, krnl_sigma=1\n",
    "    )\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.1)\n",
    "\n",
    "    loss_terms = []\n",
    "    validation = []\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    with tqdm(range(num_epochs), desc=\"Epochs\", leave=False) as pbar:\n",
    "        for epoch in pbar:\n",
    "            model.train()\n",
    "            loss_terms_batch = defaultdict(lambda:0)\n",
    "            for features, targets in train_loader:\n",
    "                \n",
    "                bsz = targets.shape[0]\n",
    "                n_views = features.shape[1]\n",
    "                n_feat = features.shape[-1]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                features = features.view(bsz * n_views, n_feat)\n",
    "                features = features.to(device)\n",
    "                targets = targets.to(device)\n",
    "                out_feat, out_target = model(features, torch.cat(n_views*[targets], dim=0))\n",
    "                \n",
    "                joint_embedding = nn.functional.mse_loss(out_feat, out_target)\n",
    "                \n",
    "                out_feat = torch.split(out_feat, [bsz]*n_views, dim=0)\n",
    "                out_feat = torch.cat([f.unsqueeze(1) for f in out_feat], dim=1)\n",
    "                kernel_feature = criterion_pft(out_feat, targets)\n",
    "\n",
    "                out_target_decoded = model.decode_target(out_target)\n",
    "                #cosine_target = torch.ones(len(out_target), device=device)                \n",
    "                out_target = torch.split(out_target, [bsz]*n_views, dim=0)\n",
    "                out_target = torch.cat([f.unsqueeze(1) for f in out_target], dim=1)\n",
    "        \n",
    "                kernel_target = criterion_ptt(out_target, targets)\n",
    "                #joint_embedding = 1000 * nn.functional.cosine_embedding_loss(out_feat, out_target, cosine_target)\n",
    "                target_decoding = .1 * nn.functional.mse_loss(torch.cat(n_views*[targets], dim=0), out_target_decoded)\n",
    "\n",
    "                loss = kernel_feature + kernel_target + joint_embedding + target_decoding\n",
    "                loss.backward()\n",
    "                                # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                loss_terms_batch['loss'] += loss.item() / len(train_loader)\n",
    "                loss_terms_batch['kernel_feature'] += kernel_feature.item() / len(train_loader)\n",
    "                loss_terms_batch['kernel_target'] += kernel_target.item() / len(train_loader)\n",
    "                loss_terms_batch['joint_embedding'] += joint_embedding.item() / len(train_loader)\n",
    "                loss_terms_batch['target_decoding'] += target_decoding.item() / len(train_loader)\n",
    "            loss_terms_batch['epoch'] = epoch\n",
    "            loss_terms.append(loss_terms_batch)\n",
    "\n",
    "            model.eval()\n",
    "            mae_batch = 0\n",
    "            with torch.no_grad():\n",
    "                for (features, targets) in test_loader:\n",
    "                    bsz = targets.shape[0]\n",
    "                    n_views = 1\n",
    "                    n_feat = features.shape[-1]\n",
    "                    \n",
    "                    if len(features.shape) > 2:\n",
    "                        n_views = features.shape[1]\n",
    "                        features = features.view(bsz * n_views, n_feat)\n",
    "                        \n",
    "                    features, targets = features.to(device), targets.to(device)\n",
    "                    \n",
    "                    out_feat = model.transform_feat(features)\n",
    "                    out_target_decoded = model.decode_target(out_feat)\n",
    "                    \n",
    "                    mae_batch += (targets - out_target_decoded).abs().mean() / len(test_loader)\n",
    "                validation.append(mae_batch.item())\n",
    "            scheduler.step(mae_batch)\n",
    "            if np.log10(scheduler._last_lr[0]) < -4:\n",
    "                break\n",
    "            \n",
    "            pbar.set_postfix_str(\n",
    "                f\"Epoch {epoch} \"\n",
    "                f\"| Loss {loss_terms[-1]['loss']:.02f} \"\n",
    "                f\"| val MAE {validation[-1]:.02f}\"\n",
    "                f\"| log10 lr {np.log10(scheduler._last_lr[0])}\"\n",
    "            )\n",
    "            \n",
    "    loss_terms = pd.DataFrame(loss_terms)\n",
    "    return loss_terms, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 67108.86it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 150603.38it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 155344.59it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 158574.82it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 80273.76it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 274.91it/s]\n"
     ]
    }
   ],
   "source": [
    "class Experiment(submitit.helpers.Checkpointable):\n",
    "    def __init__(self):\n",
    "        self.results = None\n",
    "\n",
    "    def __call__(self, train, test_size, indices, train_ratio, experiment_size, experiment, dataset, random_state=None, device=None, path: Path = None):\n",
    "        if self.results is None:\n",
    "            if device is None:\n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                print(f\"Device {device}, ratio {train_ratio}\", flush=True)\n",
    "            if not isinstance(random_state, np.random.RandomState):\n",
    "                random_state = np.random.RandomState(random_state)\n",
    "\n",
    "            # if dataset is None:\n",
    "            #     print(\"Loading data\", flush=True)\n",
    "            #     dataset = MatData(\n",
    "            #         data_path / \"vectorized_matrices.npy\",\n",
    "            #         data_path / \"participants.csv\",\n",
    "            #         \"age\",\n",
    "            #         threshold=threshold\n",
    "            #     )\n",
    "\n",
    "            # print(\"Data loaded\", flush=True)\n",
    "            predictions = {}\n",
    "            losses = []\n",
    "            experiment_indices = random_state.choice(indices, experiment_size, replace=False)\n",
    "            train_indices, test_indices = train_test_split(experiment_indices, test_size=test_size, random_state=random_state)\n",
    "            train_dataset = Subset(dataset, train_indices)\n",
    "            test_dataset = Subset(dataset, test_indices)\n",
    "            ### Augmentation\n",
    "            n_views = 1\n",
    "            \n",
    "            train_features = train_dataset.dataset.matrices[train_dataset.indices].numpy()\n",
    "            train_targets = train_dataset.dataset.target[train_dataset.indices].numpy()\n",
    "\n",
    "            test_features= train_dataset.dataset.matrices[test_dataset.indices].numpy()\n",
    "            test_targets = train_dataset.dataset.target[test_dataset.indices].numpy()\n",
    "\n",
    "            if AUGMENTATION is not None:\n",
    "                transform = augs[AUGMENTATION]\n",
    "                aug_features = np.array([transform(sample) for sample in train_features])\n",
    "\n",
    "                train_features = sym_matrix_to_vec(train_features, discard_diagonal=True)\n",
    "                aug_features = sym_matrix_to_vec(train_features, discard_diagonal=True)\n",
    "\n",
    "                n_views = n_views + aug_features.shape[1]\n",
    "                n_features = train_features.shape[-1]\n",
    "                n_samples = len(train_dataset)\n",
    "\n",
    "                new_train_features = np.zeros((n_samples, n_views, n_features))\n",
    "                new_train_features[:, 0, :] = train_features\n",
    "                new_train_features[:, 1:, :] = aug_features\n",
    "            else:\n",
    "                train_features = sym_matrix_to_vec(train_features, discard_diagonal=True)\n",
    "                train_features = np.expand_dims(train_features, axis = 1)\n",
    "            \n",
    "            train_dataset = TensorDataset(torch.from_numpy(train_features).to(torch.float32), torch.from_numpy(train_targets).to(torch.float32))\n",
    "            test_features = sym_matrix_to_vec(test_features, discard_diagonal=True)\n",
    "            test_dataset = TensorDataset(torch.from_numpy(test_features).to(torch.float32), torch.from_numpy(test_targets).to(torch.float32))\n",
    "\n",
    "            loss_terms, model = train(train_dataset, test_dataset, device=device)\n",
    "            losses.append(loss_terms.eval(\"train_ratio = @train_ratio\").eval(\"experiment = @experiment\"))\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                train_dataset = Subset(dataset, train_indices)\n",
    "                train_features = train_dataset.dataset.matrices[train_dataset.indices].numpy()\n",
    "                train_targets = train_dataset.dataset.target[train_dataset.indices].numpy()\n",
    "                train_features = np.array([sym_matrix_to_vec(i, discard_diagonal=True) for i in train_features])\n",
    "                train_dataset = TensorDataset(torch.from_numpy(train_features).to(torch.float32), torch.from_numpy(train_targets).to(torch.float32))\n",
    "                for label, d, d_indices in (('train', train_dataset, train_indices), ('test', test_dataset, test_indices)):\n",
    "                    X, y = zip(*d)\n",
    "                    X = torch.stack(X).to(device)\n",
    "                    y = torch.stack(y).to(device)\n",
    "                    y_pred = model.decode_target(model.transform_feat(X))\n",
    "                    predictions[(train_ratio, experiment, label)] = (y.cpu().numpy(), y_pred.cpu().numpy(), d_indices)\n",
    "\n",
    "            self.results = (losses, predictions)\n",
    "\n",
    "        if path:\n",
    "            self.save(path)\n",
    "        \n",
    "        return self.results\n",
    "\n",
    "    def checkpoint(self, *args, **kwargs):\n",
    "        print(\"Checkpointing\", flush=True)\n",
    "        return super().checkpoint(*args, **kwargs)\n",
    "\n",
    "    def save(self, path: Path):\n",
    "        with open(path, \"wb\") as o:\n",
    "            pickle.dump(self.results, o, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "random_state = np.random.RandomState(seed=42)\n",
    "dataset = MatData(path_feat, path_target, \"age\", threshold=THRESHOLD)\n",
    "n_sub = len(dataset)\n",
    "test_ratio = .2\n",
    "test_size = int(test_ratio * n_sub)\n",
    "indices = np.arange(n_sub)\n",
    "experiments = 20\n",
    "\n",
    "if multi_gpu:\n",
    "    log_folder = Path(\"log_folder\")\n",
    "    executor = submitit.AutoExecutor(folder=str(log_folder / \"%j\"))\n",
    "    executor.update_parameters(\n",
    "        timeout_min=40,\n",
    "        slurm_partition=\"gpu_short\",\n",
    "        gpus_per_node=1,\n",
    "        tasks_per_node=1,\n",
    "        nodes=1,\n",
    "        cpus_per_task=30\n",
    "        #slurm_qos=\"qos_gpu-t3\",\n",
    "        #slurm_constraint=\"v100-32g\",\n",
    "        #slurm_mem=\"10G\",\n",
    "        #slurm_additional_parameters={\"requeue\": True}\n",
    "    )\n",
    "    # srun -n 1  --verbose -A hjt@v100 -c 10 -C v100-32g   --gres=gpu:1 --time 5  python\n",
    "    experiment_jobs = []\n",
    "    # module_purge = submitit.helpers.CommandFunction(\"module purge\".split())\n",
    "    # module_load = submitit.helpers.CommandFunction(\"module load pytorch-gpu/py3/2.0.1\".split())\n",
    "    with executor.batch():\n",
    "        for train_ratio in tqdm(np.linspace(.1, 1., 5)):\n",
    "            train_size = int(n_sub * (1 - test_ratio) * train_ratio)\n",
    "            experiment_size = test_size + train_size\n",
    "            for experiment in tqdm(range(experiments)):\n",
    "                run_experiment = Experiment()\n",
    "                job = executor.submit(run_experiment, train, test_size, indices, train_ratio, experiment_size, experiment, dataset, random_state=random_state, device=None)\n",
    "                experiment_jobs.append(job)\n",
    "\n",
    "    async def get_result(experiment_jobs):\n",
    "        experiment_results = []\n",
    "        for aws in tqdm(asyncio.as_completed([j.awaitable().result() for j in experiment_jobs]), total=len(experiment_jobs)):\n",
    "            res = await aws\n",
    "            experiment_results.append(res)\n",
    "        return experiment_results\n",
    "    experiment_results = asyncio.run(get_result(experiment_jobs))\n",
    "else:\n",
    "    experiment_results = []\n",
    "    for train_ratio in tqdm(np.linspace(.1, 1., 5), desc=\"Training Size\"):\n",
    "        train_size = int(n_sub * (1 - test_ratio) * train_ratio)\n",
    "        experiment_size = test_size + train_size\n",
    "        for experiment in tqdm(range(experiments), desc=\"Experiment\"):\n",
    "            run_experiment = Experiment()\n",
    "            job = run_experiment(train,  test_size, indices, train_ratio, experiment_size, experiment, dataset, random_state=random_state, device=None)\n",
    "            experiment_results.append(job)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, predictions = zip(*experiment_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_metrics = predictions[0]\n",
    "for prediction in predictions[1:]:\n",
    "    prediction_metrics |= prediction\n",
    "prediction_metrics = [\n",
    "    k + (np.abs(v[0]-v[1]).mean(),)\n",
    "    for k, v in prediction_metrics.items()\n",
    "]\n",
    "prediction_metrics = pd.DataFrame(prediction_metrics, columns=[\"train ratio\", \"experiment\", \"dataset\", \"MAE\"])\n",
    "prediction_metrics[\"train size\"] = (prediction_metrics[\"train ratio\"] * len(dataset) * (1 - test_ratio)).astype(int)\n",
    "\n",
    "prediction_metrics.to_csv(f\"results/prediction_metrics_thresh{int(THRESHOLD * 100)}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
