{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from cmath import isinf\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import math\n",
    "from utils_v import compute_target_score, estimate_target, save_model\n",
    "from cmath import isinf\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split, KFold, LearningCurveDisplay, learning_curve\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_percentage_error, r2_score\n",
    "from helper_classes import MatData, KernelizedSupCon, MLP, cauchy, rbf, gaussian_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs3/well/margulies/users/cpy397/contrastive-learning\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_dataset(dataset):\n",
    "    features = torch.vstack([dataset[i][0] for i in range(len(dataset))])\n",
    "    targets = torch.vstack([dataset[i][1] for i in range(len(dataset))])\n",
    "    print(features.shape)\n",
    "    print(targets.shape)\n",
    "    \n",
    "    features_mean = features.mean(dim=0)\n",
    "    features_std = features.std(dim=0)\n",
    "    targets_mean = targets.mean(dim=0)\n",
    "    targets_std = targets.std(dim=0)\n",
    "    \n",
    "    features_std[features_std == 0] = 1\n",
    "    targets_std[targets_std == 0] = 1\n",
    "    \n",
    "    standardized_features = (features - features_mean) / features_std\n",
    "    standardized_targets = (targets - targets_mean) / targets_std\n",
    "    \n",
    "    standardized_dataset = TensorDataset(standardized_features, standardized_targets)\n",
    "    \n",
    "    return standardized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomContrastiveLoss(nn.Module): # my custom loss for my mental health\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(CustomContrastiveLoss, self).__init__()\n",
    "        self.margin = margin  # how far apart the dissimilar pairs should be pushed\n",
    "\n",
    "    def forward(self, features, targets):\n",
    "        positive_dist = torch.norm(features - targets, p=2, dim=1)\n",
    "\n",
    "        # Euclidean distances for negative pairs and apply margin\n",
    "        # For each target, calculate its distance to all other features and targets\n",
    "        batch_size = features.shape[0]\n",
    "        repulsion_loss = 0.0\n",
    "        for i in range(batch_size):\n",
    "            for j in range(batch_size):\n",
    "                if i != j:\n",
    "                    # Distance from target i to features j\n",
    "                    dist_to_features = torch.norm(targets[i] - features[j], p=2)\n",
    "                    # Distance from target i to targets j\n",
    "                    dist_to_targets = torch.norm(targets[i] - targets[j], p=2)\n",
    "                    # Apply margin and accumulate repulsion loss\n",
    "                    repulsion_loss += F.relu(self.margin - dist_to_features) + F.relu(self.margin - dist_to_targets)\n",
    "\n",
    "        # Mean over all pairs for both attraction and repulsion components\n",
    "        attraction_loss = positive_dist.mean()\n",
    "        repulsion_loss /= (batch_size * (batch_size - 1) * 2)  # Normalize by number of negative pairs\n",
    "\n",
    "        # Total loss: minimize attraction while maximizing repulsion\n",
    "        total_loss = attraction_loss + repulsion_loss\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MatData(\"vectorized_matrices_la5c.npy\", \"hopkins_age.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, test_indices = train_test_split(np.arange(len(dataset)), test_size = 0.2, random_state=42) #train_size = 5\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "test_dataset = Subset(dataset, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dim_feat = 499500 # vectorized mat, diagonal discarded\n",
    "# input_dim_target = 59\n",
    "# # the rest is arbitrary\n",
    "# hidden_dim_feat_1 = 1024\n",
    "# hidden_dim_feat_2 = 512\n",
    "# hidden_dim_target_1 = 24\n",
    "# hidden_dim_target_2 = 8\n",
    "# output_dim = 2\n",
    "# num_epochs = 1000\n",
    "\n",
    "input_dim_feat = 499500 # vectorized mat, diagonal discarded\n",
    "input_dim_target = 59\n",
    "# the rest is arbitrary\n",
    "hidden_dim_feat = 1000\n",
    "hidden_dim_target = 24\n",
    "output_dim = 2\n",
    "num_epochs = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 10\n",
    "base_temperature = 10 # too low values return nan loss\n",
    "lr = 0.001 # too low values return nan loss\n",
    "kernel = gaussian_kernel\n",
    "batch_size = 5 # too low values return nan loss\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold 0\n",
      "torch.Size([93, 499500])\n",
      "torch.Size([93, 59])\n",
      "torch.Size([24, 499500])\n",
      "torch.Size([24, 59])\n",
      "Fold 0 | Epoch 0 | Mean Loss 1.4825827479362488\n",
      "Fold 0 | Epoch 1 | Mean Loss 1.337328314781189\n",
      "Fold 0 | Epoch 2 | Mean Loss 1.256672203540802\n",
      "Fold 0 | Epoch 3 | Mean Loss 1.332231044769287\n",
      "Fold 0 | Epoch 4 | Mean Loss 1.1552929878234863\n",
      "Fold 0 | Epoch 5 | Mean Loss 1.2705029249191284\n",
      "Fold 0 | Epoch 6 | Mean Loss 1.2278813123703003\n",
      "Fold 0 | Epoch 7 | Mean Loss 1.1675033569335938\n",
      "Fold 0 | Epoch 8 | Mean Loss 1.21854829788208\n",
      "Fold 0 | Epoch 9 | Mean Loss 1.0855105519294739\n",
      "Fold 0 | Epoch 10 | Mean Loss 1.1449102759361267\n",
      "Fold 0 | Epoch 11 | Mean Loss 1.1026806831359863\n",
      "Fold 0 | Epoch 12 | Mean Loss 1.0730002522468567\n",
      "Fold 0 | Epoch 13 | Mean Loss 0.9333019852638245\n",
      "Fold 0 | Epoch 14 | Mean Loss 0.9965769946575165\n",
      "Fold 0 | Epoch 15 | Mean Loss 0.960633784532547\n",
      "Fold 0 | Epoch 16 | Mean Loss 0.9562373459339142\n",
      "Fold 0 | Epoch 17 | Mean Loss 0.9295369386672974\n",
      "Fold 0 | Epoch 18 | Mean Loss 0.9344128668308258\n",
      "Fold 0 | Epoch 19 | Mean Loss 0.9910093545913696\n",
      "Fold 0 | Epoch 20 | Mean Loss 0.9419313669204712\n",
      "Fold 0 | Epoch 21 | Mean Loss 0.9417800903320312\n",
      "Fold 0 | Epoch 22 | Mean Loss 0.9242215156555176\n",
      "Fold 0 | Epoch 23 | Mean Loss 0.8959043025970459\n",
      "Fold 0 | Epoch 24 | Mean Loss 0.9652421474456787\n",
      "Fold 0 | Epoch 25 | Mean Loss 0.9539444446563721\n",
      "Fold 0 | Epoch 26 | Mean Loss 0.9076156318187714\n",
      "Fold 0 | Epoch 27 | Mean Loss 0.8492054641246796\n",
      "Fold 0 | Epoch 28 | Mean Loss 0.8426919877529144\n",
      "Fold 0 | Epoch 29 | Mean Loss 0.8426006138324738\n",
      "Fold 0 | Epoch 30 | Mean Loss 0.8307555019855499\n",
      "Fold 0 | Epoch 31 | Mean Loss 0.8656807541847229\n",
      "Fold 0 | Epoch 32 | Mean Loss 0.8028629422187805\n",
      "Fold 0 | Epoch 33 | Mean Loss 0.8167852759361267\n",
      "Fold 0 | Epoch 34 | Mean Loss 0.7852801084518433\n",
      "Fold 0 | Epoch 35 | Mean Loss 0.810370922088623\n",
      "Fold 0 | Epoch 36 | Mean Loss 0.8468980193138123\n",
      "Fold 0 | Epoch 37 | Mean Loss 0.7745660543441772\n",
      "Fold 0 | Epoch 38 | Mean Loss 0.8104345798492432\n",
      "Fold 0 | Epoch 39 | Mean Loss 0.7805638909339905\n",
      "Fold 0 | Epoch 40 | Mean Loss 0.7980267405509949\n",
      "Fold 0 | Epoch 41 | Mean Loss 0.7979459762573242\n",
      "Fold 0 | Epoch 42 | Mean Loss 0.7409054934978485\n",
      "Fold 0 | Epoch 43 | Mean Loss 0.7438167929649353\n",
      "Fold 0 | Epoch 44 | Mean Loss 0.7090597152709961\n",
      "Fold 0 | Epoch 45 | Mean Loss 0.7833715081214905\n",
      "Fold 0 | Epoch 46 | Mean Loss 0.7600562572479248\n",
      "Fold 0 | Epoch 47 | Mean Loss 0.7171597480773926\n",
      "Fold 0 | Epoch 48 | Mean Loss 0.7700315713882446\n",
      "Fold 0 | Epoch 49 | Mean Loss 0.7219996154308319\n",
      "Fold 0 | Epoch 50 | Mean Loss 0.6912871599197388\n",
      "Fold 0 | Epoch 51 | Mean Loss 0.7367146909236908\n",
      "Fold 0 | Epoch 52 | Mean Loss 0.7008791267871857\n",
      "Fold 0 | Epoch 53 | Mean Loss 0.7177676856517792\n",
      "Fold 0 | Epoch 54 | Mean Loss 0.6731078326702118\n",
      "Fold 0 | Epoch 55 | Mean Loss 0.6843049228191376\n",
      "Fold 0 | Epoch 56 | Mean Loss 0.6685879826545715\n",
      "Fold 0 | Epoch 57 | Mean Loss 0.6864625811576843\n",
      "Fold 0 | Epoch 58 | Mean Loss 0.711569756269455\n",
      "Fold 0 | Epoch 59 | Mean Loss 0.653630793094635\n",
      "Fold 0 | Epoch 60 | Mean Loss 0.7491554915904999\n",
      "Fold 0 | Epoch 61 | Mean Loss 0.7037540078163147\n",
      "Fold 0 | Epoch 62 | Mean Loss 0.7340748608112335\n",
      "Fold 0 | Epoch 63 | Mean Loss 0.6753493845462799\n",
      "Fold 0 | Epoch 64 | Mean Loss 0.6818588972091675\n",
      "Fold 0 | Epoch 65 | Mean Loss 0.6577189862728119\n",
      "Fold 0 | Epoch 66 | Mean Loss 0.6385975182056427\n",
      "Fold 0 | Epoch 67 | Mean Loss 0.6460124552249908\n",
      "Fold 0 | Epoch 68 | Mean Loss 0.6538092494010925\n",
      "Fold 0 | Epoch 69 | Mean Loss 0.662156492471695\n",
      "Fold 0 | Epoch 70 | Mean Loss 0.6446578502655029\n",
      "Fold 0 | Epoch 71 | Mean Loss 0.6262959837913513\n",
      "Fold 0 | Epoch 72 | Mean Loss 0.661260724067688\n",
      "Fold 0 | Epoch 73 | Mean Loss 0.651837021112442\n",
      "Fold 0 | Epoch 74 | Mean Loss 0.6907782554626465\n",
      "Fold 0 | Epoch 75 | Mean Loss 0.6034777164459229\n",
      "Fold 0 | Epoch 76 | Mean Loss 0.6096198558807373\n",
      "Fold 0 | Epoch 77 | Mean Loss 0.6776604652404785\n",
      "Fold 0 | Epoch 78 | Mean Loss 0.6099252104759216\n",
      "Fold 0 | Epoch 79 | Mean Loss 0.6262123584747314\n",
      "Fold 0 | Epoch 80 | Mean Loss 0.5958127975463867\n",
      "Fold 0 | Epoch 81 | Mean Loss 0.6287463903427124\n",
      "Fold 0 | Epoch 82 | Mean Loss 0.6214559674263\n",
      "Fold 0 | Epoch 83 | Mean Loss 0.5689084827899933\n",
      "Fold 0 | Epoch 84 | Mean Loss 0.5676777064800262\n",
      "Fold 0 | Epoch 85 | Mean Loss 0.6183798313140869\n",
      "Fold 0 | Epoch 86 | Mean Loss 0.5955418348312378\n",
      "Fold 0 | Epoch 87 | Mean Loss 0.5500323474407196\n",
      "Fold 0 | Epoch 88 | Mean Loss 0.6155776977539062\n",
      "Fold 0 | Epoch 89 | Mean Loss 0.5808066129684448\n",
      "Fold 0 | Epoch 90 | Mean Loss 0.5764777362346649\n",
      "Fold 0 | Epoch 91 | Mean Loss 0.5808327496051788\n",
      "Fold 0 | Epoch 92 | Mean Loss 0.5686911940574646\n",
      "Fold 0 | Epoch 93 | Mean Loss 0.5814246535301208\n",
      "Fold 0 | Epoch 94 | Mean Loss 0.6249188780784607\n",
      "Fold 0 | Epoch 95 | Mean Loss 0.5930562019348145\n",
      "Fold 0 | Epoch 96 | Mean Loss 0.5704405903816223\n",
      "Fold 0 | Epoch 97 | Mean Loss 0.5652373731136322\n",
      "Fold 0 | Epoch 98 | Mean Loss 0.5393936932086945\n",
      "Fold 0 | Epoch 99 | Mean Loss 0.5892736911773682\n",
      "==> Saving...\n",
      "Training target estimator\n",
      "Training target estimator\n",
      "Starting fold 1\n",
      "torch.Size([93, 499500])\n",
      "torch.Size([93, 59])\n",
      "torch.Size([24, 499500])\n",
      "torch.Size([24, 59])\n",
      "Fold 1 | Epoch 0 | Mean Loss 1.6887719631195068\n",
      "Fold 1 | Epoch 1 | Mean Loss 1.38554447889328\n",
      "Fold 1 | Epoch 2 | Mean Loss 1.3311076164245605\n",
      "Fold 1 | Epoch 3 | Mean Loss 1.3358800411224365\n",
      "Fold 1 | Epoch 4 | Mean Loss 1.2540635466575623\n",
      "Fold 1 | Epoch 5 | Mean Loss 1.1751291155815125\n",
      "Fold 1 | Epoch 6 | Mean Loss 1.260589599609375\n",
      "Fold 1 | Epoch 7 | Mean Loss 1.0180924534797668\n",
      "Fold 1 | Epoch 8 | Mean Loss 1.0251155495643616\n",
      "Fold 1 | Epoch 9 | Mean Loss 0.9730479419231415\n",
      "Fold 1 | Epoch 10 | Mean Loss 0.8915213644504547\n",
      "Fold 1 | Epoch 11 | Mean Loss 0.9772510230541229\n",
      "Fold 1 | Epoch 12 | Mean Loss 0.9443606734275818\n",
      "Fold 1 | Epoch 13 | Mean Loss 0.8994096517562866\n",
      "Fold 1 | Epoch 14 | Mean Loss 0.9164132475852966\n",
      "Fold 1 | Epoch 15 | Mean Loss 0.8690716624259949\n",
      "Fold 1 | Epoch 16 | Mean Loss 0.8619210720062256\n",
      "Fold 1 | Epoch 17 | Mean Loss 0.8733893632888794\n",
      "Fold 1 | Epoch 18 | Mean Loss 0.9118558764457703\n",
      "Fold 1 | Epoch 19 | Mean Loss 0.8110536336898804\n",
      "Fold 1 | Epoch 20 | Mean Loss 0.8214877843856812\n",
      "Fold 1 | Epoch 21 | Mean Loss 0.8188429772853851\n",
      "Fold 1 | Epoch 22 | Mean Loss 0.8486650586128235\n",
      "Fold 1 | Epoch 23 | Mean Loss 0.8065379858016968\n",
      "Fold 1 | Epoch 24 | Mean Loss 0.7836862504482269\n",
      "Fold 1 | Epoch 25 | Mean Loss 0.794508159160614\n",
      "Fold 1 | Epoch 26 | Mean Loss 0.8481557965278625\n",
      "Fold 1 | Epoch 27 | Mean Loss 0.7536908984184265\n",
      "Fold 1 | Epoch 28 | Mean Loss 0.7611568868160248\n",
      "Fold 1 | Epoch 29 | Mean Loss 0.7734397053718567\n",
      "Fold 1 | Epoch 30 | Mean Loss 0.7736373245716095\n",
      "Fold 1 | Epoch 31 | Mean Loss 0.7848217487335205\n",
      "Fold 1 | Epoch 32 | Mean Loss 0.7041971683502197\n",
      "Fold 1 | Epoch 33 | Mean Loss 0.7848867774009705\n",
      "Fold 1 | Epoch 34 | Mean Loss 0.7398727536201477\n",
      "Fold 1 | Epoch 35 | Mean Loss 0.6906078159809113\n",
      "Fold 1 | Epoch 36 | Mean Loss 0.7068580985069275\n",
      "Fold 1 | Epoch 37 | Mean Loss 0.7254638075828552\n",
      "Fold 1 | Epoch 38 | Mean Loss 0.7086892127990723\n",
      "Fold 1 | Epoch 39 | Mean Loss 0.7167581915855408\n",
      "Fold 1 | Epoch 40 | Mean Loss 0.6666695773601532\n",
      "Fold 1 | Epoch 41 | Mean Loss 0.6718454957008362\n",
      "Fold 1 | Epoch 42 | Mean Loss 0.6554398536682129\n",
      "Fold 1 | Epoch 43 | Mean Loss 0.6622605919837952\n",
      "Fold 1 | Epoch 44 | Mean Loss 0.7100290656089783\n",
      "Fold 1 | Epoch 45 | Mean Loss 0.6804584562778473\n",
      "Fold 1 | Epoch 46 | Mean Loss 0.6807158291339874\n",
      "Fold 1 | Epoch 47 | Mean Loss 0.6855104565620422\n",
      "Fold 1 | Epoch 48 | Mean Loss 0.7011634707450867\n",
      "Fold 1 | Epoch 49 | Mean Loss 0.6514919102191925\n",
      "Fold 1 | Epoch 50 | Mean Loss 0.6361343860626221\n",
      "Fold 1 | Epoch 51 | Mean Loss 0.6246636807918549\n",
      "Fold 1 | Epoch 52 | Mean Loss 0.6167437136173248\n",
      "Fold 1 | Epoch 53 | Mean Loss 0.6412162780761719\n",
      "Fold 1 | Epoch 54 | Mean Loss 0.6329422891139984\n",
      "Fold 1 | Epoch 55 | Mean Loss 0.6208013892173767\n",
      "Fold 1 | Epoch 56 | Mean Loss 0.6799002885818481\n",
      "Fold 1 | Epoch 57 | Mean Loss 0.6562100052833557\n",
      "Fold 1 | Epoch 58 | Mean Loss 0.6014882624149323\n",
      "Fold 1 | Epoch 59 | Mean Loss 0.6773073673248291\n",
      "Fold 1 | Epoch 60 | Mean Loss 0.5875542163848877\n",
      "Fold 1 | Epoch 61 | Mean Loss 0.632859468460083\n",
      "Fold 1 | Epoch 62 | Mean Loss 0.6056614816188812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 | Epoch 63 | Mean Loss 0.6341090202331543\n",
      "Fold 1 | Epoch 64 | Mean Loss 0.6327820420265198\n",
      "Fold 1 | Epoch 65 | Mean Loss 0.6037974059581757\n",
      "Fold 1 | Epoch 66 | Mean Loss 0.6020501554012299\n",
      "Fold 1 | Epoch 67 | Mean Loss 0.595058023929596\n",
      "Fold 1 | Epoch 68 | Mean Loss 0.6380346119403839\n",
      "Fold 1 | Epoch 69 | Mean Loss 0.6262335777282715\n",
      "Fold 1 | Epoch 70 | Mean Loss 0.5423419922590256\n",
      "Fold 1 | Epoch 71 | Mean Loss 0.6186633706092834\n",
      "Fold 1 | Epoch 72 | Mean Loss 0.5708389282226562\n",
      "Fold 1 | Epoch 73 | Mean Loss 0.5799882113933563\n",
      "Fold 1 | Epoch 74 | Mean Loss 0.5564121007919312\n",
      "Fold 1 | Epoch 75 | Mean Loss 0.5704135894775391\n",
      "Fold 1 | Epoch 76 | Mean Loss 0.5580191314220428\n",
      "Fold 1 | Epoch 77 | Mean Loss 0.5495609641075134\n",
      "Fold 1 | Epoch 78 | Mean Loss 0.5331036746501923\n",
      "Fold 1 | Epoch 79 | Mean Loss 0.5665600001811981\n",
      "Fold 1 | Epoch 80 | Mean Loss 0.5675654709339142\n",
      "Fold 1 | Epoch 81 | Mean Loss 0.5629291534423828\n",
      "Fold 1 | Epoch 82 | Mean Loss 0.5202614963054657\n",
      "Fold 1 | Epoch 83 | Mean Loss 0.5576366782188416\n",
      "Fold 1 | Epoch 84 | Mean Loss 0.592934638261795\n",
      "Fold 1 | Epoch 85 | Mean Loss 0.5538297593593597\n",
      "Fold 1 | Epoch 86 | Mean Loss 0.4967081546783447\n",
      "Fold 1 | Epoch 87 | Mean Loss 0.49300166964530945\n",
      "Fold 1 | Epoch 88 | Mean Loss 0.5153670608997345\n",
      "Fold 1 | Epoch 89 | Mean Loss 0.5369638800621033\n",
      "Fold 1 | Epoch 90 | Mean Loss 0.5193864405155182\n",
      "Fold 1 | Epoch 91 | Mean Loss 0.5105764269828796\n",
      "Fold 1 | Epoch 92 | Mean Loss 0.5251546502113342\n",
      "Fold 1 | Epoch 93 | Mean Loss 0.5241153985261917\n",
      "Fold 1 | Epoch 94 | Mean Loss 0.5357556641101837\n",
      "Fold 1 | Epoch 95 | Mean Loss 0.5277141481637955\n",
      "Fold 1 | Epoch 96 | Mean Loss 0.5343858003616333\n",
      "Fold 1 | Epoch 97 | Mean Loss 0.4464227706193924\n",
      "Fold 1 | Epoch 98 | Mean Loss 0.5260062366724014\n",
      "Fold 1 | Epoch 99 | Mean Loss 0.5518518388271332\n",
      "==> Saving...\n",
      "Training target estimator\n",
      "Training target estimator\n",
      "Starting fold 2\n",
      "torch.Size([94, 499500])\n",
      "torch.Size([94, 59])\n",
      "torch.Size([23, 499500])\n",
      "torch.Size([23, 59])\n",
      "Fold 2 | Epoch 0 | Mean Loss 1.462887942790985\n",
      "Fold 2 | Epoch 1 | Mean Loss 1.265289068222046\n",
      "Fold 2 | Epoch 2 | Mean Loss 1.3083184957504272\n",
      "Fold 2 | Epoch 3 | Mean Loss 1.3209633231163025\n",
      "Fold 2 | Epoch 4 | Mean Loss 1.1630864143371582\n",
      "Fold 2 | Epoch 5 | Mean Loss 1.151260793209076\n",
      "Fold 2 | Epoch 6 | Mean Loss 1.1548876762390137\n",
      "Fold 2 | Epoch 7 | Mean Loss 1.115315318107605\n",
      "Fold 2 | Epoch 8 | Mean Loss 1.0791245102882385\n",
      "Fold 2 | Epoch 9 | Mean Loss 1.171662986278534\n",
      "Fold 2 | Epoch 10 | Mean Loss 1.0780606865882874\n",
      "Fold 2 | Epoch 11 | Mean Loss 1.1264833807945251\n",
      "Fold 2 | Epoch 12 | Mean Loss 0.9995611310005188\n",
      "Fold 2 | Epoch 13 | Mean Loss 1.044580101966858\n",
      "Fold 2 | Epoch 14 | Mean Loss 1.0232775807380676\n",
      "Fold 2 | Epoch 15 | Mean Loss 0.9799979031085968\n",
      "Fold 2 | Epoch 16 | Mean Loss 0.8929974734783173\n",
      "Fold 2 | Epoch 17 | Mean Loss 0.9446456134319305\n",
      "Fold 2 | Epoch 18 | Mean Loss 0.9102462530136108\n",
      "Fold 2 | Epoch 19 | Mean Loss 0.9364311695098877\n",
      "Fold 2 | Epoch 20 | Mean Loss 0.9045665264129639\n",
      "Fold 2 | Epoch 21 | Mean Loss 0.9028378129005432\n",
      "Fold 2 | Epoch 22 | Mean Loss 0.8830238580703735\n",
      "Fold 2 | Epoch 23 | Mean Loss 0.9586275815963745\n",
      "Fold 2 | Epoch 24 | Mean Loss 0.8577547967433929\n",
      "Fold 2 | Epoch 25 | Mean Loss 0.8679808378219604\n",
      "Fold 2 | Epoch 26 | Mean Loss 0.7579422891139984\n",
      "Fold 2 | Epoch 27 | Mean Loss 0.8582653999328613\n",
      "Fold 2 | Epoch 28 | Mean Loss 0.8490002155303955\n",
      "Fold 2 | Epoch 29 | Mean Loss 0.7902257442474365\n",
      "Fold 2 | Epoch 30 | Mean Loss 0.7801544964313507\n",
      "Fold 2 | Epoch 31 | Mean Loss 0.7499220073223114\n",
      "Fold 2 | Epoch 32 | Mean Loss 0.7414874732494354\n",
      "Fold 2 | Epoch 33 | Mean Loss 0.6879934072494507\n",
      "Fold 2 | Epoch 34 | Mean Loss 0.7094788551330566\n",
      "Fold 2 | Epoch 35 | Mean Loss 0.7647528052330017\n",
      "Fold 2 | Epoch 36 | Mean Loss 0.7262334227561951\n",
      "Fold 2 | Epoch 37 | Mean Loss 0.7031905055046082\n",
      "Fold 2 | Epoch 38 | Mean Loss 0.7135193943977356\n",
      "Fold 2 | Epoch 39 | Mean Loss 0.6982750296592712\n",
      "Fold 2 | Epoch 40 | Mean Loss 0.6334850192070007\n",
      "Fold 2 | Epoch 41 | Mean Loss 0.7220450937747955\n",
      "Fold 2 | Epoch 42 | Mean Loss 0.6172355115413666\n",
      "Fold 2 | Epoch 43 | Mean Loss 0.6654566824436188\n",
      "Fold 2 | Epoch 44 | Mean Loss 0.6967120468616486\n",
      "Fold 2 | Epoch 45 | Mean Loss 0.6969867646694183\n",
      "Fold 2 | Epoch 46 | Mean Loss 0.7046272158622742\n",
      "Fold 2 | Epoch 47 | Mean Loss 0.656824380159378\n",
      "Fold 2 | Epoch 48 | Mean Loss 0.6204065084457397\n",
      "Fold 2 | Epoch 49 | Mean Loss 0.5984886884689331\n",
      "Fold 2 | Epoch 50 | Mean Loss 0.6754325032234192\n",
      "Fold 2 | Epoch 51 | Mean Loss 0.6413798928260803\n",
      "Fold 2 | Epoch 52 | Mean Loss 0.6525451242923737\n",
      "Fold 2 | Epoch 53 | Mean Loss 0.6140277683734894\n",
      "Fold 2 | Epoch 54 | Mean Loss 0.6310243904590607\n",
      "Fold 2 | Epoch 55 | Mean Loss 0.6554109454154968\n",
      "Fold 2 | Epoch 56 | Mean Loss 0.6106261909008026\n",
      "Fold 2 | Epoch 57 | Mean Loss 0.5894729495048523\n",
      "Fold 2 | Epoch 58 | Mean Loss 0.610897034406662\n",
      "Fold 2 | Epoch 59 | Mean Loss 0.5651550889015198\n",
      "Fold 2 | Epoch 60 | Mean Loss 0.6085788011550903\n",
      "Fold 2 | Epoch 61 | Mean Loss 0.5561325252056122\n",
      "Fold 2 | Epoch 62 | Mean Loss 0.6402606964111328\n",
      "Fold 2 | Epoch 63 | Mean Loss 0.5683847665786743\n",
      "Fold 2 | Epoch 64 | Mean Loss 0.5346924662590027\n",
      "Fold 2 | Epoch 65 | Mean Loss 0.5327398180961609\n",
      "Fold 2 | Epoch 66 | Mean Loss 0.5640316903591156\n",
      "Fold 2 | Epoch 67 | Mean Loss 0.5676381587982178\n",
      "Fold 2 | Epoch 68 | Mean Loss 0.5369456112384796\n",
      "Fold 2 | Epoch 69 | Mean Loss 0.5347990691661835\n",
      "Fold 2 | Epoch 70 | Mean Loss 0.5747596025466919\n",
      "Fold 2 | Epoch 71 | Mean Loss 0.5589386522769928\n",
      "Fold 2 | Epoch 72 | Mean Loss 0.594633162021637\n",
      "Fold 2 | Epoch 73 | Mean Loss 0.6298176646232605\n",
      "Fold 2 | Epoch 74 | Mean Loss 0.5496121942996979\n",
      "Fold 2 | Epoch 75 | Mean Loss 0.5066322088241577\n",
      "Fold 2 | Epoch 76 | Mean Loss 0.5502072274684906\n",
      "Fold 2 | Epoch 77 | Mean Loss 0.5179651081562042\n",
      "Fold 2 | Epoch 78 | Mean Loss 0.5534821152687073\n",
      "Fold 2 | Epoch 79 | Mean Loss 0.5082902610301971\n",
      "Fold 2 | Epoch 80 | Mean Loss 0.5796618461608887\n",
      "Fold 2 | Epoch 81 | Mean Loss 0.5047510862350464\n",
      "Fold 2 | Epoch 82 | Mean Loss 0.5349624752998352\n",
      "Fold 2 | Epoch 83 | Mean Loss 0.4980829656124115\n",
      "Fold 2 | Epoch 84 | Mean Loss 0.521644651889801\n",
      "Fold 2 | Epoch 85 | Mean Loss 0.5395250916481018\n",
      "Fold 2 | Epoch 86 | Mean Loss 0.5381517112255096\n",
      "Fold 2 | Epoch 87 | Mean Loss 0.5349999666213989\n",
      "Fold 2 | Epoch 88 | Mean Loss 0.5197250247001648\n",
      "Fold 2 | Epoch 89 | Mean Loss 0.5035990476608276\n",
      "Fold 2 | Epoch 90 | Mean Loss 0.5281341075897217\n",
      "Fold 2 | Epoch 91 | Mean Loss 0.526723176240921\n",
      "Fold 2 | Epoch 92 | Mean Loss 0.5370321273803711\n",
      "Fold 2 | Epoch 93 | Mean Loss 0.6136181056499481\n",
      "Fold 2 | Epoch 94 | Mean Loss 0.5139140188694\n",
      "Fold 2 | Epoch 95 | Mean Loss 0.5279877185821533\n",
      "Fold 2 | Epoch 96 | Mean Loss 0.52158984541893\n",
      "Fold 2 | Epoch 97 | Mean Loss 0.5157656520605087\n",
      "Fold 2 | Epoch 98 | Mean Loss 0.5078509747982025\n",
      "Fold 2 | Epoch 99 | Mean Loss 0.48279234766960144\n",
      "Training target estimator\n",
      "Training target estimator\n",
      "Starting fold 3\n",
      "torch.Size([94, 499500])\n",
      "torch.Size([94, 59])\n",
      "torch.Size([23, 499500])\n",
      "torch.Size([23, 59])\n",
      "Fold 3 | Epoch 0 | Mean Loss 1.4810560941696167\n",
      "Fold 3 | Epoch 1 | Mean Loss 1.357876718044281\n",
      "Fold 3 | Epoch 2 | Mean Loss 1.3344507813453674\n",
      "Fold 3 | Epoch 3 | Mean Loss 1.2614758610725403\n",
      "Fold 3 | Epoch 4 | Mean Loss 1.3007158041000366\n",
      "Fold 3 | Epoch 5 | Mean Loss 1.153307318687439\n",
      "Fold 3 | Epoch 6 | Mean Loss 1.2980902791023254\n",
      "Fold 3 | Epoch 7 | Mean Loss 1.2280043363571167\n",
      "Fold 3 | Epoch 8 | Mean Loss 1.1126006841659546\n",
      "Fold 3 | Epoch 9 | Mean Loss 1.1841859817504883\n",
      "Fold 3 | Epoch 10 | Mean Loss 1.1367747783660889\n",
      "Fold 3 | Epoch 11 | Mean Loss 1.1446188688278198\n",
      "Fold 3 | Epoch 12 | Mean Loss 1.1765381693840027\n",
      "Fold 3 | Epoch 13 | Mean Loss 1.08896404504776\n",
      "Fold 3 | Epoch 14 | Mean Loss 1.1063419580459595\n",
      "Fold 3 | Epoch 15 | Mean Loss 1.1735278964042664\n",
      "Fold 3 | Epoch 16 | Mean Loss 1.1065846681594849\n",
      "Fold 3 | Epoch 17 | Mean Loss 1.047504037618637\n",
      "Fold 3 | Epoch 18 | Mean Loss 1.0897698402404785\n",
      "Fold 3 | Epoch 19 | Mean Loss 1.0271643996238708\n",
      "Fold 3 | Epoch 20 | Mean Loss 0.978243499994278\n",
      "Fold 3 | Epoch 21 | Mean Loss 1.0136575102806091\n",
      "Fold 3 | Epoch 22 | Mean Loss 1.056678295135498\n",
      "Fold 3 | Epoch 23 | Mean Loss 1.032654345035553\n",
      "Fold 3 | Epoch 24 | Mean Loss 0.9173040986061096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 | Epoch 25 | Mean Loss 1.0339378118515015\n",
      "Fold 3 | Epoch 26 | Mean Loss 0.9622008204460144\n",
      "Fold 3 | Epoch 27 | Mean Loss 0.9686800241470337\n",
      "Fold 3 | Epoch 28 | Mean Loss 0.9060491025447845\n",
      "Fold 3 | Epoch 29 | Mean Loss 0.8375467956066132\n",
      "Fold 3 | Epoch 30 | Mean Loss 0.9658018946647644\n",
      "Fold 3 | Epoch 31 | Mean Loss 0.8265327215194702\n",
      "Fold 3 | Epoch 32 | Mean Loss 0.9274781942367554\n",
      "Fold 3 | Epoch 33 | Mean Loss 0.9230959117412567\n",
      "Fold 3 | Epoch 34 | Mean Loss 0.9277197122573853\n",
      "Fold 3 | Epoch 35 | Mean Loss 0.8417080044746399\n",
      "Fold 3 | Epoch 36 | Mean Loss 0.804212212562561\n",
      "Fold 3 | Epoch 37 | Mean Loss 0.8313669264316559\n",
      "Fold 3 | Epoch 38 | Mean Loss 0.8382694721221924\n",
      "Fold 3 | Epoch 39 | Mean Loss 0.8359838128089905\n",
      "Fold 3 | Epoch 40 | Mean Loss 0.7431919872760773\n",
      "Fold 3 | Epoch 41 | Mean Loss 0.8167958557605743\n",
      "Fold 3 | Epoch 42 | Mean Loss 0.8271759748458862\n",
      "Fold 3 | Epoch 43 | Mean Loss 0.800975114107132\n",
      "Fold 3 | Epoch 44 | Mean Loss 0.7241008579730988\n",
      "Fold 3 | Epoch 45 | Mean Loss 0.7235001921653748\n",
      "Fold 3 | Epoch 46 | Mean Loss 0.7536554038524628\n",
      "Fold 3 | Epoch 47 | Mean Loss 0.7591231763362885\n",
      "Fold 3 | Epoch 48 | Mean Loss 0.6731193959712982\n",
      "Fold 3 | Epoch 49 | Mean Loss 0.7742750942707062\n",
      "Fold 3 | Epoch 50 | Mean Loss 0.7103292346000671\n",
      "Fold 3 | Epoch 51 | Mean Loss 0.7182390987873077\n",
      "Fold 3 | Epoch 52 | Mean Loss 0.7443913221359253\n",
      "Fold 3 | Epoch 53 | Mean Loss 0.7053064107894897\n",
      "Fold 3 | Epoch 54 | Mean Loss 0.7497320175170898\n",
      "Fold 3 | Epoch 55 | Mean Loss 0.6894697546958923\n",
      "Fold 3 | Epoch 56 | Mean Loss 0.7349608540534973\n",
      "Fold 3 | Epoch 57 | Mean Loss 0.7071654796600342\n",
      "Fold 3 | Epoch 58 | Mean Loss 0.7491812109947205\n",
      "Fold 3 | Epoch 59 | Mean Loss 0.6567018032073975\n",
      "Fold 3 | Epoch 60 | Mean Loss 0.6841656267642975\n",
      "Fold 3 | Epoch 61 | Mean Loss 0.7361569702625275\n",
      "Fold 3 | Epoch 62 | Mean Loss 0.6995801627635956\n",
      "Fold 3 | Epoch 63 | Mean Loss 0.6797067224979401\n",
      "Fold 3 | Epoch 64 | Mean Loss 0.683777779340744\n",
      "Fold 3 | Epoch 65 | Mean Loss 0.6145573258399963\n",
      "Fold 3 | Epoch 66 | Mean Loss 0.66712886095047\n",
      "Fold 3 | Epoch 67 | Mean Loss 0.6991776525974274\n",
      "Fold 3 | Epoch 68 | Mean Loss 0.7548456788063049\n",
      "Fold 3 | Epoch 69 | Mean Loss 0.7032422423362732\n",
      "Fold 3 | Epoch 70 | Mean Loss 0.6760104596614838\n",
      "Fold 3 | Epoch 71 | Mean Loss 0.6530595421791077\n",
      "Fold 3 | Epoch 72 | Mean Loss 0.6970117092132568\n",
      "Fold 3 | Epoch 73 | Mean Loss 0.6056566834449768\n",
      "Fold 3 | Epoch 74 | Mean Loss 0.6543736159801483\n",
      "Fold 3 | Epoch 75 | Mean Loss 0.6509556472301483\n",
      "Fold 3 | Epoch 76 | Mean Loss 0.6130011975765228\n",
      "Fold 3 | Epoch 77 | Mean Loss 0.5769471526145935\n",
      "Fold 3 | Epoch 78 | Mean Loss 0.5867884755134583\n",
      "Fold 3 | Epoch 79 | Mean Loss 0.6044346988201141\n",
      "Fold 3 | Epoch 80 | Mean Loss 0.6118528842926025\n",
      "Fold 3 | Epoch 81 | Mean Loss 0.6031804978847504\n",
      "Fold 3 | Epoch 82 | Mean Loss 0.546845093369484\n",
      "Fold 3 | Epoch 83 | Mean Loss 0.5423679947853088\n",
      "Fold 3 | Epoch 84 | Mean Loss 0.6727970540523529\n",
      "Fold 3 | Epoch 85 | Mean Loss 0.5845758616924286\n",
      "Fold 3 | Epoch 86 | Mean Loss 0.5504854619503021\n",
      "Fold 3 | Epoch 87 | Mean Loss 0.557066410779953\n",
      "Fold 3 | Epoch 88 | Mean Loss 0.6536439955234528\n",
      "Fold 3 | Epoch 89 | Mean Loss 0.5774643123149872\n",
      "Fold 3 | Epoch 90 | Mean Loss 0.5238026976585388\n",
      "Fold 3 | Epoch 91 | Mean Loss 0.5473616123199463\n",
      "Fold 3 | Epoch 92 | Mean Loss 0.5797955393791199\n",
      "Fold 3 | Epoch 93 | Mean Loss 0.5826299786567688\n",
      "Fold 3 | Epoch 94 | Mean Loss 0.5308258831501007\n",
      "Fold 3 | Epoch 95 | Mean Loss 0.5328421592712402\n",
      "Fold 3 | Epoch 96 | Mean Loss 0.5838179886341095\n",
      "Fold 3 | Epoch 97 | Mean Loss 0.5583034157752991\n",
      "Fold 3 | Epoch 98 | Mean Loss 0.5353480577468872\n",
      "Fold 3 | Epoch 99 | Mean Loss 0.5799125730991364\n",
      "Training target estimator\n",
      "Training target estimator\n",
      "Starting fold 4\n",
      "torch.Size([94, 499500])\n",
      "torch.Size([94, 59])\n",
      "torch.Size([23, 499500])\n",
      "torch.Size([23, 59])\n",
      "Fold 4 | Epoch 0 | Mean Loss 1.5375993251800537\n",
      "Fold 4 | Epoch 1 | Mean Loss 1.2719013690948486\n",
      "Fold 4 | Epoch 2 | Mean Loss 1.2526139616966248\n",
      "Fold 4 | Epoch 3 | Mean Loss 1.088612973690033\n",
      "Fold 4 | Epoch 4 | Mean Loss 1.1666690111160278\n",
      "Fold 4 | Epoch 5 | Mean Loss 0.985477089881897\n",
      "Fold 4 | Epoch 6 | Mean Loss 1.0807991027832031\n",
      "Fold 4 | Epoch 7 | Mean Loss 1.0397571325302124\n",
      "Fold 4 | Epoch 8 | Mean Loss 0.9892049133777618\n",
      "Fold 4 | Epoch 9 | Mean Loss 0.9595511257648468\n",
      "Fold 4 | Epoch 10 | Mean Loss 1.0099762976169586\n",
      "Fold 4 | Epoch 11 | Mean Loss 0.8935989141464233\n",
      "Fold 4 | Epoch 12 | Mean Loss 0.9121057689189911\n",
      "Fold 4 | Epoch 13 | Mean Loss 0.869163304567337\n",
      "Fold 4 | Epoch 14 | Mean Loss 0.9147399067878723\n",
      "Fold 4 | Epoch 15 | Mean Loss 0.779141366481781\n",
      "Fold 4 | Epoch 16 | Mean Loss 0.8775426149368286\n",
      "Fold 4 | Epoch 17 | Mean Loss 0.8069565296173096\n",
      "Fold 4 | Epoch 18 | Mean Loss 0.8742944896221161\n",
      "Fold 4 | Epoch 19 | Mean Loss 0.7314052283763885\n",
      "Fold 4 | Epoch 20 | Mean Loss 0.8149831295013428\n",
      "Fold 4 | Epoch 21 | Mean Loss 0.7944808006286621\n",
      "Fold 4 | Epoch 22 | Mean Loss 0.80866539478302\n",
      "Fold 4 | Epoch 23 | Mean Loss 0.7620839774608612\n",
      "Fold 4 | Epoch 24 | Mean Loss 0.799983561038971\n",
      "Fold 4 | Epoch 25 | Mean Loss 0.718543142080307\n",
      "Fold 4 | Epoch 26 | Mean Loss 0.7797005772590637\n",
      "Fold 4 | Epoch 27 | Mean Loss 0.7258078455924988\n",
      "Fold 4 | Epoch 28 | Mean Loss 0.7551884353160858\n",
      "Fold 4 | Epoch 29 | Mean Loss 0.7599318027496338\n",
      "Fold 4 | Epoch 30 | Mean Loss 0.6913554072380066\n",
      "Fold 4 | Epoch 31 | Mean Loss 0.678716242313385\n",
      "Fold 4 | Epoch 32 | Mean Loss 0.7590228617191315\n",
      "Fold 4 | Epoch 33 | Mean Loss 0.8075553774833679\n",
      "Fold 4 | Epoch 34 | Mean Loss 0.7188479900360107\n",
      "Fold 4 | Epoch 35 | Mean Loss 0.730238139629364\n",
      "Fold 4 | Epoch 36 | Mean Loss 0.689374178647995\n",
      "Fold 4 | Epoch 37 | Mean Loss 0.6783828437328339\n",
      "Fold 4 | Epoch 38 | Mean Loss 0.6853784322738647\n",
      "Fold 4 | Epoch 39 | Mean Loss 0.6402447819709778\n",
      "Fold 4 | Epoch 40 | Mean Loss 0.7055928111076355\n",
      "Fold 4 | Epoch 41 | Mean Loss 0.715835452079773\n",
      "Fold 4 | Epoch 42 | Mean Loss 0.6749572157859802\n",
      "Fold 4 | Epoch 43 | Mean Loss 0.612951934337616\n",
      "Fold 4 | Epoch 44 | Mean Loss 0.634575605392456\n",
      "Fold 4 | Epoch 45 | Mean Loss 0.6223788857460022\n",
      "Fold 4 | Epoch 46 | Mean Loss 0.6172952055931091\n",
      "Fold 4 | Epoch 47 | Mean Loss 0.6190504133701324\n",
      "Fold 4 | Epoch 48 | Mean Loss 0.6030177772045135\n",
      "Fold 4 | Epoch 49 | Mean Loss 0.6215671300888062\n",
      "Fold 4 | Epoch 50 | Mean Loss 0.619208574295044\n",
      "Fold 4 | Epoch 51 | Mean Loss 0.6759540736675262\n",
      "Fold 4 | Epoch 52 | Mean Loss 0.6303649246692657\n",
      "Fold 4 | Epoch 53 | Mean Loss 0.6254309713840485\n",
      "Fold 4 | Epoch 54 | Mean Loss 0.5894629061222076\n",
      "Fold 4 | Epoch 55 | Mean Loss 0.5649676620960236\n",
      "Fold 4 | Epoch 56 | Mean Loss 0.5895813405513763\n",
      "Fold 4 | Epoch 57 | Mean Loss 0.6042667627334595\n",
      "Fold 4 | Epoch 58 | Mean Loss 0.5821724832057953\n",
      "Fold 4 | Epoch 59 | Mean Loss 0.5869105458259583\n",
      "Fold 4 | Epoch 60 | Mean Loss 0.6397946774959564\n",
      "Fold 4 | Epoch 61 | Mean Loss 0.5194027721881866\n",
      "Fold 4 | Epoch 62 | Mean Loss 0.5118566155433655\n",
      "Fold 4 | Epoch 63 | Mean Loss 0.584590494632721\n",
      "Fold 4 | Epoch 64 | Mean Loss 0.5854115784168243\n",
      "Fold 4 | Epoch 65 | Mean Loss 0.6087243556976318\n",
      "Fold 4 | Epoch 66 | Mean Loss 0.566026508808136\n",
      "Fold 4 | Epoch 67 | Mean Loss 0.5960076153278351\n",
      "Fold 4 | Epoch 68 | Mean Loss 0.5528619885444641\n",
      "Fold 4 | Epoch 69 | Mean Loss 0.6047505438327789\n",
      "Fold 4 | Epoch 70 | Mean Loss 0.5594887733459473\n",
      "Fold 4 | Epoch 71 | Mean Loss 0.5593753159046173\n",
      "Fold 4 | Epoch 72 | Mean Loss 0.5177760720252991\n",
      "Fold 4 | Epoch 73 | Mean Loss 0.5357298254966736\n",
      "Fold 4 | Epoch 74 | Mean Loss 0.5834010243415833\n",
      "Fold 4 | Epoch 75 | Mean Loss 0.5149390995502472\n",
      "Fold 4 | Epoch 76 | Mean Loss 0.5335328429937363\n",
      "Fold 4 | Epoch 77 | Mean Loss 0.5778534710407257\n",
      "Fold 4 | Epoch 78 | Mean Loss 0.4850175678730011\n",
      "Fold 4 | Epoch 79 | Mean Loss 0.5475886464118958\n",
      "Fold 4 | Epoch 80 | Mean Loss 0.5548228621482849\n",
      "Fold 4 | Epoch 81 | Mean Loss 0.5796393156051636\n",
      "Fold 4 | Epoch 82 | Mean Loss 0.500994086265564\n",
      "Fold 4 | Epoch 83 | Mean Loss 0.47901207208633423\n",
      "Fold 4 | Epoch 84 | Mean Loss 0.5400722622871399\n",
      "Fold 4 | Epoch 85 | Mean Loss 0.4947662353515625\n",
      "Fold 4 | Epoch 86 | Mean Loss 0.5302371084690094\n",
      "Fold 4 | Epoch 87 | Mean Loss 0.5352151989936829\n",
      "Fold 4 | Epoch 88 | Mean Loss 0.5367763340473175\n",
      "Fold 4 | Epoch 89 | Mean Loss 0.5289797484874725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 | Epoch 90 | Mean Loss 0.5284376740455627\n",
      "Fold 4 | Epoch 91 | Mean Loss 0.5352140665054321\n",
      "Fold 4 | Epoch 92 | Mean Loss 0.5524659156799316\n",
      "Fold 4 | Epoch 93 | Mean Loss 0.5071049928665161\n",
      "Fold 4 | Epoch 94 | Mean Loss 0.5346395373344421\n",
      "Fold 4 | Epoch 95 | Mean Loss 0.4921553432941437\n",
      "Fold 4 | Epoch 96 | Mean Loss 0.4661918878555298\n",
      "Fold 4 | Epoch 97 | Mean Loss 0.5108111947774887\n",
      "Fold 4 | Epoch 98 | Mean Loss 0.5113405883312225\n",
      "Fold 4 | Epoch 99 | Mean Loss 0.5560811161994934\n",
      "Training target estimator\n",
      "Training target estimator\n"
     ]
    }
   ],
   "source": [
    "results_cv = []\n",
    "best_mape = np.inf\n",
    "best_r2 = -np.inf\n",
    "best_average_loss = np.inf\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset)):\n",
    "    \n",
    "    print(f\"Starting fold {fold}\")\n",
    "    \n",
    "    train_subset = Subset(train_dataset, train_idx)\n",
    "    val_subset = Subset(train_dataset, val_idx)\n",
    "\n",
    "    std_train_subset = standardize_dataset(train_subset)\n",
    "    std_val_subset = standardize_dataset(val_subset)\n",
    "\n",
    "    # Now, 'standardized_train_subset' and 'standardized_val_subset' are ready to be used for training and validation\n",
    "    # Example: Convert them to DataLoader if needed\n",
    "    train_loader = DataLoader(std_train_subset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(std_val_subset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # Initialize your model, criterion, and optimizer here\n",
    "    model = MLP(input_dim_feat, input_dim_target, hidden_dim_feat, hidden_dim_target, output_dim).to(device)\n",
    "    criterion = CustomContrastiveLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        batch_losses = []\n",
    "        for batch_num, (features, targets) in enumerate(train_loader):\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out_feat, out_target = model(features, targets)\n",
    "            loss = criterion(out_feat, out_target)\n",
    "            loss.backward()\n",
    "            batch_losses.append(loss.item())\n",
    "            optimizer.step()\n",
    "        print(f'Fold {fold} | Epoch {epoch} | Mean Loss {sum(batch_losses)/len(batch_losses)}')\n",
    "            \n",
    "    val_losses = []\n",
    "    model.eval() \n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "        for features, targets in val_loader:\n",
    "            features = features.to(device).float()\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            out_feat, out_target = model(features, targets)\n",
    "            loss = criterion(out_feat, out_target)\n",
    "            val_losses.append(loss.item())\n",
    "            total_loss += loss.item() * features.size(0)\n",
    "            total_samples += features.size(0)\n",
    "        val_losses =np.array(val_losses)\n",
    "        average_loss = total_loss / total_samples\n",
    "        if best_average_loss > average_loss:\n",
    "            best_average_loss = average_loss\n",
    "            save_model(model, fold, optimizer, f\"best_model_hopkins_cv.pt\")\n",
    "    mape_train, mape_val = compute_target_score(model, train_loader, val_loader, device, 'mape')\n",
    "    r2_train, r2_val = compute_target_score(model, train_loader, val_loader, device, 'r2')\n",
    "#     if mape_train < best_mae and r2_train > best_r2: # saving the one which has the best train metrics\n",
    "#         best_mape = mape_train\n",
    "#         best_r2 = r2_train\n",
    "#         save_model(model, fold, optimizer, f\"best_model_hopkins_cv.pt\")\n",
    "    results_cv.append([fold, mape_train, r2_train, mape_val, r2_val])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Mean Loss 1.3616798400878907\n",
      "Epoch 1 | Mean Loss 1.354862356185913\n",
      "Epoch 2 | Mean Loss 1.3148438692092896\n",
      "Epoch 3 | Mean Loss 1.3571008920669556\n",
      "Epoch 4 | Mean Loss 1.3193791151046752\n",
      "Epoch 5 | Mean Loss 1.3327240228652955\n",
      "Epoch 6 | Mean Loss 1.3277281761169433\n",
      "Epoch 7 | Mean Loss 1.3289444208145142\n",
      "Epoch 8 | Mean Loss 1.3378843069076538\n",
      "Epoch 9 | Mean Loss 1.317392683029175\n",
      "Epoch 10 | Mean Loss 1.3506352663040162\n",
      "Epoch 11 | Mean Loss 1.3399664878845214\n",
      "Epoch 12 | Mean Loss 1.323616361618042\n",
      "Epoch 13 | Mean Loss 1.3325619459152223\n",
      "Epoch 14 | Mean Loss 1.3311208009719848\n",
      "Epoch 15 | Mean Loss 1.3405274629592896\n",
      "Epoch 16 | Mean Loss 1.3558753728866577\n",
      "Epoch 17 | Mean Loss 1.306177568435669\n",
      "Epoch 18 | Mean Loss 1.3226912498474122\n",
      "Epoch 19 | Mean Loss 1.3237359762191772\n",
      "Epoch 20 | Mean Loss 1.3234088182449342\n",
      "Epoch 21 | Mean Loss 1.3177572250366212\n",
      "Epoch 22 | Mean Loss 1.2950052976608277\n",
      "Epoch 23 | Mean Loss 1.3069126844406127\n",
      "Epoch 24 | Mean Loss 1.3221591711044312\n",
      "Epoch 25 | Mean Loss 1.3015563249588014\n",
      "Epoch 26 | Mean Loss 1.3144814252853394\n",
      "Epoch 27 | Mean Loss 1.3036230325698852\n",
      "Epoch 28 | Mean Loss 1.3113651752471924\n",
      "Epoch 29 | Mean Loss 1.3181921005249024\n",
      "Epoch 30 | Mean Loss 1.305506157875061\n",
      "Epoch 31 | Mean Loss 1.3100131034851075\n",
      "Epoch 32 | Mean Loss 1.277081298828125\n",
      "Epoch 33 | Mean Loss 1.3079005241394044\n",
      "Epoch 34 | Mean Loss 1.3086828947067262\n",
      "Epoch 35 | Mean Loss 1.2874795198440552\n",
      "Epoch 36 | Mean Loss 1.2901381731033326\n",
      "Epoch 37 | Mean Loss 1.3000576496124268\n",
      "Epoch 38 | Mean Loss 1.261929738521576\n",
      "Epoch 39 | Mean Loss 1.295145082473755\n",
      "Epoch 40 | Mean Loss 1.2995064020156861\n",
      "Epoch 41 | Mean Loss 1.2668640851974486\n",
      "Epoch 42 | Mean Loss 1.3032036423683167\n",
      "Epoch 43 | Mean Loss 1.2570732355117797\n",
      "Epoch 44 | Mean Loss 1.3116403341293335\n",
      "Epoch 45 | Mean Loss 1.307491421699524\n",
      "Epoch 46 | Mean Loss 1.3113565921783448\n",
      "Epoch 47 | Mean Loss 1.2771279335021972\n",
      "Epoch 48 | Mean Loss 1.2869860410690308\n",
      "Epoch 49 | Mean Loss 1.2709926247596741\n",
      "Epoch 50 | Mean Loss 1.2437978029251098\n",
      "Epoch 51 | Mean Loss 1.2517965316772461\n",
      "Epoch 52 | Mean Loss 1.2815052270889282\n",
      "Epoch 53 | Mean Loss 1.3024378061294555\n",
      "Epoch 54 | Mean Loss 1.2828913927078247\n",
      "Epoch 55 | Mean Loss 1.2666652917861938\n",
      "Epoch 56 | Mean Loss 1.2755560874938965\n",
      "Epoch 57 | Mean Loss 1.2697094440460206\n",
      "Epoch 58 | Mean Loss 1.3036588191986085\n",
      "Epoch 59 | Mean Loss 1.257839560508728\n",
      "Epoch 60 | Mean Loss 1.2547600269317627\n",
      "Epoch 61 | Mean Loss 1.266171145439148\n",
      "Epoch 62 | Mean Loss 1.3031732797622682\n",
      "Epoch 63 | Mean Loss 1.254186177253723\n",
      "Epoch 64 | Mean Loss 1.2714958667755127\n",
      "Epoch 65 | Mean Loss 1.2548492908477784\n",
      "Epoch 66 | Mean Loss 1.2648542165756225\n",
      "Epoch 67 | Mean Loss 1.2783268451690675\n",
      "Epoch 68 | Mean Loss 1.290133285522461\n",
      "Epoch 69 | Mean Loss 1.238720679283142\n",
      "Epoch 70 | Mean Loss 1.286055088043213\n",
      "Epoch 71 | Mean Loss 1.2856832504272462\n",
      "Epoch 72 | Mean Loss 1.242089080810547\n",
      "Epoch 73 | Mean Loss 1.2443912148475647\n",
      "Epoch 74 | Mean Loss 1.270961332321167\n",
      "Epoch 75 | Mean Loss 1.2611941576004029\n",
      "Epoch 76 | Mean Loss 1.2535736560821533\n",
      "Epoch 77 | Mean Loss 1.2685456991195678\n",
      "Epoch 78 | Mean Loss 1.2926441431045532\n",
      "Epoch 79 | Mean Loss 1.2725313186645508\n",
      "Epoch 80 | Mean Loss 1.2668569087982178\n",
      "Epoch 81 | Mean Loss 1.2531986951828002\n",
      "Epoch 82 | Mean Loss 1.257200837135315\n",
      "Epoch 83 | Mean Loss 1.255935835838318\n",
      "Epoch 84 | Mean Loss 1.2631729602813722\n",
      "Epoch 85 | Mean Loss 1.2796663045883179\n",
      "Epoch 86 | Mean Loss 1.3051225185394286\n",
      "Epoch 87 | Mean Loss 1.2544986963272096\n",
      "Epoch 88 | Mean Loss 1.2517704725265504\n",
      "Epoch 89 | Mean Loss 1.2590282917022706\n",
      "Epoch 90 | Mean Loss 1.2656902551651001\n",
      "Epoch 91 | Mean Loss 1.286866807937622\n",
      "Epoch 92 | Mean Loss 1.2641520023345947\n",
      "Epoch 93 | Mean Loss 1.288973879814148\n",
      "Epoch 94 | Mean Loss 1.2981722354888916\n",
      "Epoch 95 | Mean Loss 1.2913925647735596\n",
      "Epoch 96 | Mean Loss 1.286992597579956\n",
      "Epoch 97 | Mean Loss 1.26587655544281\n",
      "Epoch 98 | Mean Loss 1.25666286945343\n",
      "Epoch 99 | Mean Loss 1.273196005821228\n",
      "Epoch 100 | Mean Loss 1.2675832271575929\n",
      "Epoch 101 | Mean Loss 1.2431056261062623\n",
      "Epoch 102 | Mean Loss 1.2908056735992433\n",
      "Epoch 103 | Mean Loss 1.2607433795928955\n",
      "Epoch 104 | Mean Loss 1.2317323207855224\n",
      "Epoch 105 | Mean Loss 1.255794930458069\n",
      "Epoch 106 | Mean Loss 1.259373164176941\n",
      "Epoch 107 | Mean Loss 1.2824343681335448\n",
      "Epoch 108 | Mean Loss 1.2715481281280518\n",
      "Epoch 109 | Mean Loss 1.273136043548584\n",
      "Epoch 110 | Mean Loss 1.2664782285690308\n",
      "Epoch 111 | Mean Loss 1.2791525721549988\n",
      "Epoch 112 | Mean Loss 1.283699870109558\n",
      "Epoch 113 | Mean Loss 1.275446605682373\n",
      "Epoch 114 | Mean Loss 1.2800092220306396\n",
      "Epoch 115 | Mean Loss 1.291558313369751\n",
      "Epoch 116 | Mean Loss 1.2853018522262574\n",
      "Epoch 117 | Mean Loss 1.2931725025177\n",
      "Epoch 118 | Mean Loss 1.26771479845047\n",
      "Epoch 119 | Mean Loss 1.281328558921814\n",
      "Epoch 120 | Mean Loss 1.2886467456817627\n",
      "Epoch 121 | Mean Loss 1.2450973510742187\n",
      "Epoch 122 | Mean Loss 1.287753129005432\n",
      "Epoch 123 | Mean Loss 1.2344759941101073\n",
      "Epoch 124 | Mean Loss 1.268097162246704\n",
      "Epoch 125 | Mean Loss 1.2531549453735351\n",
      "Epoch 126 | Mean Loss 1.2477967500686646\n",
      "Epoch 127 | Mean Loss 1.271013069152832\n",
      "Epoch 128 | Mean Loss 1.2592582941055297\n",
      "Epoch 129 | Mean Loss 1.2638569593429565\n",
      "Epoch 130 | Mean Loss 1.2779855966567992\n",
      "Epoch 131 | Mean Loss 1.2565778970718384\n",
      "Epoch 132 | Mean Loss 1.2466009855270386\n",
      "Epoch 133 | Mean Loss 1.283855128288269\n",
      "Epoch 134 | Mean Loss 1.2834382057189941\n",
      "Epoch 135 | Mean Loss 1.2534646153450013\n",
      "Epoch 136 | Mean Loss 1.2795872688293457\n",
      "Epoch 137 | Mean Loss 1.2736038208007812\n",
      "Epoch 138 | Mean Loss 1.2287510633468628\n",
      "Epoch 139 | Mean Loss 1.2974157333374023\n",
      "Epoch 140 | Mean Loss 1.260144305229187\n",
      "Epoch 141 | Mean Loss 1.299348521232605\n",
      "Epoch 142 | Mean Loss 1.2330759644508362\n",
      "Epoch 143 | Mean Loss 1.277054524421692\n",
      "Epoch 144 | Mean Loss 1.2251687526702881\n",
      "Epoch 145 | Mean Loss 1.2810074806213378\n",
      "Epoch 146 | Mean Loss 1.2779569149017334\n",
      "Epoch 147 | Mean Loss 1.2611778736114503\n",
      "Epoch 148 | Mean Loss 1.2753532648086547\n",
      "Epoch 149 | Mean Loss 1.2667917490005494\n",
      "Epoch 150 | Mean Loss 1.25498046875\n",
      "Epoch 151 | Mean Loss 1.2624102592468263\n",
      "Epoch 152 | Mean Loss 1.2646117687225342\n",
      "Epoch 153 | Mean Loss 1.2590441942214965\n",
      "Epoch 154 | Mean Loss 1.271394395828247\n",
      "Epoch 155 | Mean Loss 1.2606436729431152\n",
      "Epoch 156 | Mean Loss 1.2822399377822875\n",
      "Epoch 157 | Mean Loss 1.2605172157287599\n",
      "Epoch 158 | Mean Loss 1.268745768070221\n",
      "Epoch 159 | Mean Loss 1.2565418481826782\n",
      "Epoch 160 | Mean Loss 1.2391554832458496\n",
      "Epoch 161 | Mean Loss 1.2275268197059632\n",
      "Epoch 162 | Mean Loss 1.2548217535018922\n",
      "Epoch 163 | Mean Loss 1.224606418609619\n",
      "Epoch 164 | Mean Loss 1.2472297430038453\n",
      "Epoch 165 | Mean Loss 1.2512254238128662\n",
      "Epoch 166 | Mean Loss 1.2452194452285767\n",
      "Epoch 167 | Mean Loss 1.2457794189453124\n",
      "Epoch 168 | Mean Loss 1.2631105184555054\n",
      "Epoch 169 | Mean Loss 1.2803951263427735\n",
      "Epoch 170 | Mean Loss 1.284026575088501\n",
      "Epoch 171 | Mean Loss 1.2547027587890625\n",
      "Epoch 172 | Mean Loss 1.2534020662307739\n",
      "Epoch 173 | Mean Loss 1.224260926246643\n",
      "Epoch 174 | Mean Loss 1.2458527088165283\n",
      "Epoch 175 | Mean Loss 1.2467692375183106\n",
      "Epoch 176 | Mean Loss 1.2463196516036987\n",
      "Epoch 177 | Mean Loss 1.2585095405578612\n",
      "Epoch 178 | Mean Loss 1.3059456825256348\n",
      "Epoch 179 | Mean Loss 1.2756341576576233\n",
      "Epoch 180 | Mean Loss 1.23634192943573\n",
      "Epoch 181 | Mean Loss 1.274592638015747\n",
      "Epoch 182 | Mean Loss 1.2545395135879516\n",
      "Epoch 183 | Mean Loss 1.2382383346557617\n",
      "Epoch 184 | Mean Loss 1.2997291326522826\n",
      "Epoch 185 | Mean Loss 1.2790321111679077\n",
      "Epoch 186 | Mean Loss 1.2530779838562012\n",
      "Epoch 187 | Mean Loss 1.2866851568222046\n",
      "Epoch 188 | Mean Loss 1.2682566165924072\n",
      "Epoch 189 | Mean Loss 1.2515932083129884\n",
      "Epoch 190 | Mean Loss 1.281809687614441\n",
      "Epoch 191 | Mean Loss 1.2479368925094605\n",
      "Epoch 192 | Mean Loss 1.300815463066101\n",
      "Epoch 193 | Mean Loss 1.2661065101623534\n",
      "Epoch 194 | Mean Loss 1.257132649421692\n",
      "Epoch 195 | Mean Loss 1.2647897720336914\n",
      "Epoch 196 | Mean Loss 1.2440188884735108\n",
      "Epoch 197 | Mean Loss 1.2389255881309509\n",
      "Epoch 198 | Mean Loss 1.247544240951538\n",
      "Epoch 199 | Mean Loss 1.2562196493148803\n",
      "Epoch 200 | Mean Loss 1.276671552658081\n",
      "Epoch 201 | Mean Loss 1.3109883546829224\n",
      "Epoch 202 | Mean Loss 1.2754772663116456\n",
      "Epoch 203 | Mean Loss 1.2725712776184082\n",
      "Epoch 204 | Mean Loss 1.2687154293060303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 205 | Mean Loss 1.289695715904236\n",
      "Epoch 206 | Mean Loss 1.2730433940887451\n",
      "Epoch 207 | Mean Loss 1.2641244411468506\n",
      "Epoch 208 | Mean Loss 1.2439054489135741\n",
      "Epoch 209 | Mean Loss 1.2545250177383422\n",
      "Epoch 210 | Mean Loss 1.242627167701721\n",
      "Epoch 211 | Mean Loss 1.2575950860977172\n",
      "Epoch 212 | Mean Loss 1.2542659282684325\n",
      "Epoch 213 | Mean Loss 1.2726576805114747\n",
      "Epoch 214 | Mean Loss 1.259055495262146\n",
      "Epoch 215 | Mean Loss 1.2450913190841675\n",
      "Epoch 216 | Mean Loss 1.241499388217926\n",
      "Epoch 217 | Mean Loss 1.2389806985855103\n",
      "Epoch 218 | Mean Loss 1.2585210800170898\n",
      "Epoch 219 | Mean Loss 1.263112473487854\n",
      "Epoch 220 | Mean Loss 1.2921778440475464\n",
      "Epoch 221 | Mean Loss 1.2624018907546997\n",
      "Epoch 222 | Mean Loss 1.235888409614563\n",
      "Epoch 223 | Mean Loss 1.2543436527252196\n",
      "Epoch 224 | Mean Loss 1.2759108066558837\n",
      "Epoch 225 | Mean Loss 1.2488525629043579\n",
      "Epoch 226 | Mean Loss 1.2913944244384765\n",
      "Epoch 227 | Mean Loss 1.2371384620666503\n",
      "Epoch 228 | Mean Loss 1.2196063995361328\n",
      "Epoch 229 | Mean Loss 1.3075538635253907\n",
      "Epoch 230 | Mean Loss 1.2574240446090699\n",
      "Epoch 231 | Mean Loss 1.2250495195388793\n",
      "Epoch 232 | Mean Loss 1.2788536071777343\n",
      "Epoch 233 | Mean Loss 1.2758148193359375\n",
      "Epoch 234 | Mean Loss 1.2482722520828247\n",
      "Epoch 235 | Mean Loss 1.2722909688949584\n",
      "Epoch 236 | Mean Loss 1.2333901166915893\n",
      "Epoch 237 | Mean Loss 1.2958729147911072\n",
      "Epoch 238 | Mean Loss 1.2449973821640015\n",
      "Epoch 239 | Mean Loss 1.2665101647377015\n",
      "Epoch 240 | Mean Loss 1.246783971786499\n",
      "Epoch 241 | Mean Loss 1.2378690004348756\n",
      "Epoch 242 | Mean Loss 1.2747658729553222\n",
      "Epoch 243 | Mean Loss 1.253846001625061\n",
      "Epoch 244 | Mean Loss 1.2411675691604613\n",
      "Epoch 245 | Mean Loss 1.2673136711120605\n",
      "Epoch 246 | Mean Loss 1.2774961471557618\n",
      "Epoch 247 | Mean Loss 1.2661288261413575\n",
      "Epoch 248 | Mean Loss 1.280919599533081\n",
      "Epoch 249 | Mean Loss 1.221618616580963\n",
      "Epoch 250 | Mean Loss 1.2626620888710023\n",
      "Epoch 251 | Mean Loss 1.288058590888977\n",
      "Epoch 252 | Mean Loss 1.2355776309967041\n",
      "Epoch 253 | Mean Loss 1.2368419170379639\n",
      "Epoch 254 | Mean Loss 1.234874176979065\n",
      "Epoch 255 | Mean Loss 1.2840551853179931\n",
      "Epoch 256 | Mean Loss 1.2903701782226562\n",
      "Epoch 257 | Mean Loss 1.2634246349334717\n",
      "Epoch 258 | Mean Loss 1.263693642616272\n",
      "Epoch 259 | Mean Loss 1.2365254163742065\n",
      "Epoch 260 | Mean Loss 1.2589141845703125\n",
      "Epoch 261 | Mean Loss 1.2380139589309693\n",
      "Epoch 262 | Mean Loss 1.2414924144744872\n",
      "Epoch 263 | Mean Loss 1.2440157651901245\n",
      "Epoch 264 | Mean Loss 1.2818878412246704\n",
      "Epoch 265 | Mean Loss 1.2639957666397095\n",
      "Epoch 266 | Mean Loss 1.2841443300247193\n",
      "Epoch 267 | Mean Loss 1.258590078353882\n",
      "Epoch 268 | Mean Loss 1.2484917998313905\n",
      "Epoch 269 | Mean Loss 1.267982053756714\n",
      "Epoch 270 | Mean Loss 1.2508461475372314\n",
      "Epoch 271 | Mean Loss 1.2214298486709594\n",
      "Epoch 272 | Mean Loss 1.237247681617737\n",
      "Epoch 273 | Mean Loss 1.2351423382759095\n",
      "Epoch 274 | Mean Loss 1.2583086967468262\n",
      "Epoch 275 | Mean Loss 1.2642549991607666\n",
      "Epoch 276 | Mean Loss 1.2565115213394165\n",
      "Epoch 277 | Mean Loss 1.2488847255706788\n",
      "Epoch 278 | Mean Loss 1.2569992065429687\n",
      "Epoch 279 | Mean Loss 1.2417877912521362\n",
      "Epoch 280 | Mean Loss 1.238730788230896\n",
      "Epoch 281 | Mean Loss 1.2932911396026612\n",
      "Epoch 282 | Mean Loss 1.2357964754104613\n",
      "Epoch 283 | Mean Loss 1.2607996225357057\n",
      "Epoch 284 | Mean Loss 1.2381780624389649\n",
      "Epoch 285 | Mean Loss 1.2568532466888427\n",
      "Epoch 286 | Mean Loss 1.2621874809265137\n",
      "Epoch 287 | Mean Loss 1.265764629840851\n",
      "Epoch 288 | Mean Loss 1.2360754013061523\n",
      "Epoch 289 | Mean Loss 1.2562368392944336\n",
      "Epoch 290 | Mean Loss 1.2355026960372926\n",
      "Epoch 291 | Mean Loss 1.2208491206169128\n",
      "Epoch 292 | Mean Loss 1.2528013229370116\n",
      "Epoch 293 | Mean Loss 1.2972977876663208\n",
      "Epoch 294 | Mean Loss 1.307868194580078\n",
      "Epoch 295 | Mean Loss 1.2975836753845216\n",
      "Epoch 296 | Mean Loss 1.220695972442627\n",
      "Epoch 297 | Mean Loss 1.237462592124939\n",
      "Epoch 298 | Mean Loss 1.2361947059631349\n",
      "Epoch 299 | Mean Loss 1.2757087469100952\n",
      "Epoch 300 | Mean Loss 1.2477152824401856\n",
      "Epoch 301 | Mean Loss 1.2373769521713256\n",
      "Epoch 302 | Mean Loss 1.233969020843506\n",
      "Epoch 303 | Mean Loss 1.279087209701538\n",
      "Epoch 304 | Mean Loss 1.2372164964675902\n",
      "Epoch 305 | Mean Loss 1.2506710052490235\n",
      "Epoch 306 | Mean Loss 1.2511465787887572\n",
      "Epoch 307 | Mean Loss 1.2770564794540404\n",
      "Epoch 308 | Mean Loss 1.2198687314987182\n",
      "Epoch 309 | Mean Loss 1.2340351462364196\n",
      "Epoch 310 | Mean Loss 1.272900152206421\n",
      "Epoch 311 | Mean Loss 1.2525938153266907\n",
      "Epoch 312 | Mean Loss 1.285325288772583\n",
      "Epoch 313 | Mean Loss 1.234466564655304\n",
      "Epoch 314 | Mean Loss 1.2367893218994142\n",
      "Epoch 315 | Mean Loss 1.2548589944839477\n",
      "Epoch 316 | Mean Loss 1.2191943407058716\n",
      "Epoch 317 | Mean Loss 1.2360372304916383\n",
      "Epoch 318 | Mean Loss 1.2493648290634156\n",
      "Epoch 319 | Mean Loss 1.2643819808959962\n",
      "Epoch 320 | Mean Loss 1.234552526473999\n",
      "Epoch 321 | Mean Loss 1.2916861534118653\n",
      "Epoch 322 | Mean Loss 1.2346803069114685\n",
      "Epoch 323 | Mean Loss 1.2599732160568238\n",
      "Epoch 324 | Mean Loss 1.2348727226257323\n",
      "Epoch 325 | Mean Loss 1.2367679476737976\n",
      "Epoch 326 | Mean Loss 1.2344979524612427\n",
      "Epoch 327 | Mean Loss 1.2303904175758362\n",
      "Epoch 328 | Mean Loss 1.2430358290672303\n",
      "Epoch 329 | Mean Loss 1.2387117147445679\n",
      "Epoch 330 | Mean Loss 1.259627342224121\n",
      "Epoch 331 | Mean Loss 1.2357231616973876\n",
      "Epoch 332 | Mean Loss 1.2655160903930665\n",
      "Epoch 333 | Mean Loss 1.2631484985351562\n",
      "Epoch 334 | Mean Loss 1.2226229548454284\n",
      "Epoch 335 | Mean Loss 1.2341258764266967\n",
      "Epoch 336 | Mean Loss 1.2423243522644043\n",
      "Epoch 337 | Mean Loss 1.2665891885757445\n",
      "Epoch 338 | Mean Loss 1.2653340816497802\n",
      "Epoch 339 | Mean Loss 1.2798197507858275\n",
      "Epoch 340 | Mean Loss 1.2393763661384583\n",
      "Epoch 341 | Mean Loss 1.2566165685653687\n",
      "Epoch 342 | Mean Loss 1.219148051738739\n",
      "Epoch 343 | Mean Loss 1.2406484723091125\n",
      "Epoch 344 | Mean Loss 1.2521984219551086\n",
      "Epoch 345 | Mean Loss 1.2574399709701538\n",
      "Epoch 346 | Mean Loss 1.2503827571868897\n",
      "Epoch 347 | Mean Loss 1.2447486877441407\n",
      "Epoch 348 | Mean Loss 1.246194052696228\n",
      "Epoch 349 | Mean Loss 1.2804361820220946\n",
      "Epoch 350 | Mean Loss 1.2320024013519286\n",
      "Epoch 351 | Mean Loss 1.2487519264221192\n",
      "Epoch 352 | Mean Loss 1.240947461128235\n",
      "Epoch 353 | Mean Loss 1.2760915160179138\n",
      "Epoch 354 | Mean Loss 1.2858750343322753\n",
      "Epoch 355 | Mean Loss 1.2235005974769593\n",
      "Epoch 356 | Mean Loss 1.2343920946121216\n",
      "Epoch 357 | Mean Loss 1.2650163412094115\n",
      "Epoch 358 | Mean Loss 1.2376001596450805\n",
      "Epoch 359 | Mean Loss 1.2367158889770509\n",
      "Epoch 360 | Mean Loss 1.2332707405090333\n",
      "Epoch 361 | Mean Loss 1.257156002521515\n",
      "Epoch 362 | Mean Loss 1.2662583827972411\n",
      "Epoch 363 | Mean Loss 1.2549701929092407\n",
      "Epoch 364 | Mean Loss 1.2651095151901246\n",
      "Epoch 365 | Mean Loss 1.223826789855957\n",
      "Epoch 366 | Mean Loss 1.288556694984436\n",
      "Epoch 367 | Mean Loss 1.2660143971443176\n",
      "Epoch 368 | Mean Loss 1.248021125793457\n",
      "Epoch 369 | Mean Loss 1.250397539138794\n",
      "Epoch 370 | Mean Loss 1.247180199623108\n",
      "Epoch 371 | Mean Loss 1.2492544174194335\n",
      "Epoch 372 | Mean Loss 1.2208568811416627\n",
      "Epoch 373 | Mean Loss 1.2349855303764343\n",
      "Epoch 374 | Mean Loss 1.2446281671524049\n",
      "Epoch 375 | Mean Loss 1.2276948809623718\n",
      "Epoch 376 | Mean Loss 1.2576955318450929\n",
      "Epoch 377 | Mean Loss 1.2677209377288818\n",
      "Epoch 378 | Mean Loss 1.2625534415245057\n",
      "Epoch 379 | Mean Loss 1.2428918600082397\n",
      "Epoch 380 | Mean Loss 1.2218831062316895\n",
      "Epoch 381 | Mean Loss 1.2533513069152833\n",
      "Epoch 382 | Mean Loss 1.2763656854629517\n",
      "Epoch 383 | Mean Loss 1.2517870903015136\n",
      "Epoch 384 | Mean Loss 1.3020515203475953\n",
      "Epoch 385 | Mean Loss 1.2702213764190673\n",
      "Epoch 386 | Mean Loss 1.2219001531600953\n",
      "Epoch 387 | Mean Loss 1.2470890522003173\n",
      "Epoch 388 | Mean Loss 1.2579486608505248\n",
      "Epoch 389 | Mean Loss 1.2553229928016663\n",
      "Epoch 390 | Mean Loss 1.247071099281311\n",
      "Epoch 391 | Mean Loss 1.296934676170349\n",
      "Epoch 392 | Mean Loss 1.267319416999817\n",
      "Epoch 393 | Mean Loss 1.272232723236084\n",
      "Epoch 394 | Mean Loss 1.2576610803604127\n",
      "Epoch 395 | Mean Loss 1.2336727142333985\n",
      "Epoch 396 | Mean Loss 1.2552112102508546\n",
      "Epoch 397 | Mean Loss 1.2398763656616212\n",
      "Epoch 398 | Mean Loss 1.2720963478088378\n",
      "Epoch 399 | Mean Loss 1.25855792760849\n",
      "Epoch 400 | Mean Loss 1.2240551352500915\n",
      "Epoch 401 | Mean Loss 1.2845878839492797\n",
      "Epoch 402 | Mean Loss 1.2696381568908692\n",
      "Epoch 403 | Mean Loss 1.245229172706604\n",
      "Epoch 404 | Mean Loss 1.2269197583198548\n",
      "Epoch 405 | Mean Loss 1.2477020025253296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 406 | Mean Loss 1.2602952718734741\n",
      "Epoch 407 | Mean Loss 1.2499021291732788\n",
      "Epoch 408 | Mean Loss 1.2796754360198974\n",
      "Epoch 409 | Mean Loss 1.2254726886749268\n",
      "Epoch 410 | Mean Loss 1.2535171031951904\n",
      "Epoch 411 | Mean Loss 1.2895574808120727\n",
      "Epoch 412 | Mean Loss 1.2739389419555665\n",
      "Epoch 413 | Mean Loss 1.2879810094833375\n",
      "Epoch 414 | Mean Loss 1.2194210648536683\n",
      "Epoch 415 | Mean Loss 1.2210783123970033\n",
      "Epoch 416 | Mean Loss 1.2848376989364625\n",
      "Epoch 417 | Mean Loss 1.2577597141265868\n",
      "Epoch 418 | Mean Loss 1.251668667793274\n",
      "Epoch 419 | Mean Loss 1.2406530618667602\n",
      "Epoch 420 | Mean Loss 1.2352956056594848\n",
      "Epoch 421 | Mean Loss 1.2772870540618897\n",
      "Epoch 422 | Mean Loss 1.2732063055038452\n",
      "Epoch 423 | Mean Loss 1.2258206963539124\n",
      "Epoch 424 | Mean Loss 1.262274479866028\n",
      "Epoch 425 | Mean Loss 1.2974668860435485\n",
      "Epoch 426 | Mean Loss 1.2218557715415954\n",
      "Epoch 427 | Mean Loss 1.2518522977828979\n",
      "Epoch 428 | Mean Loss 1.2849309682846068\n",
      "Epoch 429 | Mean Loss 1.252336287498474\n",
      "Epoch 430 | Mean Loss 1.2352094173431396\n",
      "Epoch 431 | Mean Loss 1.2465606212615967\n",
      "Epoch 432 | Mean Loss 1.277989435195923\n",
      "Epoch 433 | Mean Loss 1.2514554738998414\n",
      "Epoch 434 | Mean Loss 1.264150893688202\n",
      "Epoch 435 | Mean Loss 1.263778519630432\n",
      "Epoch 436 | Mean Loss 1.2593072652816772\n",
      "Epoch 437 | Mean Loss 1.2647414207458496\n",
      "Epoch 438 | Mean Loss 1.2541335344314575\n",
      "Epoch 439 | Mean Loss 1.3000254154205322\n",
      "Epoch 440 | Mean Loss 1.3195948123931884\n",
      "Epoch 441 | Mean Loss 1.2679417371749877\n",
      "Epoch 442 | Mean Loss 1.3062171936035156\n",
      "Epoch 443 | Mean Loss 1.2701191663742066\n",
      "Epoch 444 | Mean Loss 1.238972580432892\n",
      "Epoch 445 | Mean Loss 1.3248893976211549\n",
      "Epoch 446 | Mean Loss 1.3015415906906127\n",
      "Epoch 447 | Mean Loss 1.2797460556030273\n",
      "Epoch 448 | Mean Loss 1.3054242610931397\n",
      "Epoch 449 | Mean Loss 1.2866239547729492\n",
      "Epoch 450 | Mean Loss 1.278745436668396\n",
      "Epoch 451 | Mean Loss 1.279741632938385\n",
      "Epoch 452 | Mean Loss 1.2824952602386475\n",
      "Epoch 453 | Mean Loss 1.304914164543152\n",
      "Epoch 454 | Mean Loss 1.2851342678070068\n",
      "Epoch 455 | Mean Loss 1.276189374923706\n",
      "Epoch 456 | Mean Loss 1.2504505157470702\n",
      "Epoch 457 | Mean Loss 1.2636940717697143\n",
      "Epoch 458 | Mean Loss 1.263441228866577\n",
      "Epoch 459 | Mean Loss 1.2664902687072754\n",
      "Epoch 460 | Mean Loss 1.2718948364257812\n",
      "Epoch 461 | Mean Loss 1.3055624961853027\n",
      "Epoch 462 | Mean Loss 1.246623706817627\n",
      "Epoch 463 | Mean Loss 1.2763753652572631\n",
      "Epoch 464 | Mean Loss 1.280178427696228\n",
      "Epoch 465 | Mean Loss 1.262821650505066\n",
      "Epoch 466 | Mean Loss 1.263533067703247\n",
      "Epoch 467 | Mean Loss 1.2827429294586181\n",
      "Epoch 468 | Mean Loss 1.2727058172225951\n",
      "Epoch 469 | Mean Loss 1.2345537304878236\n",
      "Epoch 470 | Mean Loss 1.270644187927246\n",
      "Epoch 471 | Mean Loss 1.277033019065857\n",
      "Epoch 472 | Mean Loss 1.288588571548462\n",
      "Epoch 473 | Mean Loss 1.2630862474441529\n",
      "Epoch 474 | Mean Loss 1.2462811470031738\n",
      "Epoch 475 | Mean Loss 1.2412962913513184\n",
      "Epoch 476 | Mean Loss 1.2912550687789917\n",
      "Epoch 477 | Mean Loss 1.249442458152771\n",
      "Epoch 478 | Mean Loss 1.2351267099380494\n",
      "Epoch 479 | Mean Loss 1.2412501335144044\n",
      "Epoch 480 | Mean Loss 1.274021577835083\n",
      "Epoch 481 | Mean Loss 1.2786579132080078\n",
      "Epoch 482 | Mean Loss 1.2530078172683716\n",
      "Epoch 483 | Mean Loss 1.293638491630554\n",
      "Epoch 484 | Mean Loss 1.2701195240020753\n",
      "Epoch 485 | Mean Loss 1.2545957803726195\n",
      "Epoch 486 | Mean Loss 1.2465858459472656\n",
      "Epoch 487 | Mean Loss 1.2594165802001953\n",
      "Epoch 488 | Mean Loss 1.2904855728149414\n",
      "Epoch 489 | Mean Loss 1.2810568809509277\n",
      "Epoch 490 | Mean Loss 1.243946623802185\n",
      "Epoch 491 | Mean Loss 1.2399986028671264\n",
      "Epoch 492 | Mean Loss 1.2997676372528075\n",
      "Epoch 493 | Mean Loss 1.256281304359436\n",
      "Epoch 494 | Mean Loss 1.2566711187362671\n",
      "Epoch 495 | Mean Loss 1.2626651406288147\n",
      "Epoch 496 | Mean Loss 1.2686652898788453\n",
      "Epoch 497 | Mean Loss 1.2219900488853455\n",
      "Epoch 498 | Mean Loss 1.2417896270751954\n",
      "Epoch 499 | Mean Loss 1.2547466516494752\n",
      "Epoch 500 | Mean Loss 1.2471970319747925\n",
      "Epoch 501 | Mean Loss 1.2452514529228211\n",
      "Epoch 502 | Mean Loss 1.3001174211502076\n",
      "Epoch 503 | Mean Loss 1.2572611570358276\n",
      "Epoch 504 | Mean Loss 1.2362321376800538\n",
      "Epoch 505 | Mean Loss 1.2522650957107544\n",
      "Epoch 506 | Mean Loss 1.2505432724952699\n",
      "Epoch 507 | Mean Loss 1.2749480962753297\n",
      "Epoch 508 | Mean Loss 1.255925965309143\n",
      "Epoch 509 | Mean Loss 1.3008900880813599\n",
      "Epoch 510 | Mean Loss 1.2240603923797608\n",
      "Epoch 511 | Mean Loss 1.2671344518661498\n",
      "Epoch 512 | Mean Loss 1.246399188041687\n",
      "Epoch 513 | Mean Loss 1.2718180656433105\n",
      "Epoch 514 | Mean Loss 1.2375202417373656\n",
      "Epoch 515 | Mean Loss 1.2361709117889403\n",
      "Epoch 516 | Mean Loss 1.2537119269371033\n",
      "Epoch 517 | Mean Loss 1.2569443464279175\n",
      "Epoch 518 | Mean Loss 1.2642505884170532\n",
      "Epoch 519 | Mean Loss 1.2832884550094605\n",
      "Epoch 520 | Mean Loss 1.2537748217582703\n",
      "Epoch 521 | Mean Loss 1.2514824628829957\n",
      "Epoch 522 | Mean Loss 1.250913953781128\n",
      "Epoch 523 | Mean Loss 1.2668183088302611\n",
      "Epoch 524 | Mean Loss 1.227346408367157\n",
      "Epoch 525 | Mean Loss 1.2752227306365966\n",
      "Epoch 526 | Mean Loss 1.2741846084594726\n",
      "Epoch 527 | Mean Loss 1.2473340511322022\n",
      "Epoch 528 | Mean Loss 1.2881238460540771\n",
      "Epoch 529 | Mean Loss 1.2583059191703796\n",
      "Epoch 530 | Mean Loss 1.258061146736145\n",
      "Epoch 531 | Mean Loss 1.2606632709503174\n",
      "Epoch 532 | Mean Loss 1.2709286689758301\n",
      "Epoch 533 | Mean Loss 1.250839853286743\n",
      "Epoch 534 | Mean Loss 1.2658868074417113\n",
      "Epoch 535 | Mean Loss 1.267855131626129\n",
      "Epoch 536 | Mean Loss 1.3096908807754517\n",
      "Epoch 537 | Mean Loss 1.2686914205551147\n",
      "Epoch 538 | Mean Loss 1.2704896926879883\n",
      "Epoch 539 | Mean Loss 1.312984824180603\n",
      "Epoch 540 | Mean Loss 1.2500338912010194\n",
      "Epoch 541 | Mean Loss 1.2505788326263427\n",
      "Epoch 542 | Mean Loss 1.2546266555786132\n",
      "Epoch 543 | Mean Loss 1.2493067264556885\n",
      "Epoch 544 | Mean Loss 1.223640263080597\n",
      "Epoch 545 | Mean Loss 1.2808927774429322\n",
      "Epoch 546 | Mean Loss 1.2185911417007447\n",
      "Epoch 547 | Mean Loss 1.2347732305526733\n",
      "Epoch 548 | Mean Loss 1.3072062730789185\n",
      "Epoch 549 | Mean Loss 1.2192150831222535\n",
      "Epoch 550 | Mean Loss 1.21941180229187\n",
      "Epoch 551 | Mean Loss 1.2495936632156373\n",
      "Epoch 552 | Mean Loss 1.2366528749465941\n",
      "Epoch 553 | Mean Loss 1.2795597553253173\n",
      "Epoch 554 | Mean Loss 1.282085156440735\n",
      "Epoch 555 | Mean Loss 1.2528900623321533\n",
      "Epoch 556 | Mean Loss 1.2190704822540284\n",
      "Epoch 557 | Mean Loss 1.25586941242218\n",
      "Epoch 558 | Mean Loss 1.234683918952942\n",
      "Epoch 559 | Mean Loss 1.2473496913909912\n",
      "Epoch 560 | Mean Loss 1.3004952907562255\n",
      "Epoch 561 | Mean Loss 1.2330349206924438\n",
      "Epoch 562 | Mean Loss 1.2684264183044434\n",
      "Epoch 563 | Mean Loss 1.2602817058563232\n",
      "Epoch 564 | Mean Loss 1.2739424467086793\n",
      "Epoch 565 | Mean Loss 1.2627782344818115\n",
      "Epoch 566 | Mean Loss 1.235767912864685\n",
      "Epoch 567 | Mean Loss 1.2625502824783326\n",
      "Epoch 568 | Mean Loss 1.286506462097168\n",
      "Epoch 569 | Mean Loss 1.2632818222045898\n",
      "Epoch 570 | Mean Loss 1.2606523990631104\n",
      "Epoch 571 | Mean Loss 1.2773969411849975\n",
      "Epoch 572 | Mean Loss 1.245018756389618\n",
      "Epoch 573 | Mean Loss 1.2738542795181274\n",
      "Epoch 574 | Mean Loss 1.293730628490448\n",
      "Epoch 575 | Mean Loss 1.2694889307022095\n",
      "Epoch 576 | Mean Loss 1.2392960071563721\n",
      "Epoch 577 | Mean Loss 1.2786993265151978\n",
      "Epoch 578 | Mean Loss 1.2225687026977539\n",
      "Epoch 579 | Mean Loss 1.2859562635421753\n",
      "Epoch 580 | Mean Loss 1.2883873224258422\n",
      "Epoch 581 | Mean Loss 1.2812031745910644\n",
      "Epoch 582 | Mean Loss 1.2826372385025024\n",
      "Epoch 583 | Mean Loss 1.246648406982422\n",
      "Epoch 584 | Mean Loss 1.2736487865447998\n",
      "Epoch 585 | Mean Loss 1.2573399305343629\n",
      "Epoch 586 | Mean Loss 1.2585739135742187\n",
      "Epoch 587 | Mean Loss 1.239527440071106\n",
      "Epoch 588 | Mean Loss 1.2579670190811156\n",
      "Epoch 589 | Mean Loss 1.2866007089614868\n",
      "Epoch 590 | Mean Loss 1.2421290040016175\n",
      "Epoch 591 | Mean Loss 1.2325302600860595\n",
      "Epoch 592 | Mean Loss 1.2661113262176513\n",
      "Epoch 593 | Mean Loss 1.2355029463768006\n",
      "Epoch 594 | Mean Loss 1.2598654747009277\n",
      "Epoch 595 | Mean Loss 1.244766926765442\n",
      "Epoch 596 | Mean Loss 1.2661267518997192\n",
      "Epoch 597 | Mean Loss 1.2311002016067505\n",
      "Epoch 598 | Mean Loss 1.2423476457595826\n",
      "Epoch 599 | Mean Loss 1.2663867235183717\n",
      "Epoch 600 | Mean Loss 1.2527820587158203\n",
      "Epoch 601 | Mean Loss 1.2401856660842896\n",
      "Epoch 602 | Mean Loss 1.301284623146057\n",
      "Epoch 603 | Mean Loss 1.2202410459518434\n",
      "Epoch 604 | Mean Loss 1.2295023322105407\n",
      "Epoch 605 | Mean Loss 1.2407598853111268\n",
      "Epoch 606 | Mean Loss 1.2554888486862184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 607 | Mean Loss 1.2485331058502198\n",
      "Epoch 608 | Mean Loss 1.2234508991241455\n",
      "Epoch 609 | Mean Loss 1.2510621070861816\n",
      "Epoch 610 | Mean Loss 1.2642542839050293\n",
      "Epoch 611 | Mean Loss 1.2663268566131591\n",
      "Epoch 612 | Mean Loss 1.2992246389389037\n",
      "Epoch 613 | Mean Loss 1.2398028135299684\n",
      "Epoch 614 | Mean Loss 1.2767603874206543\n",
      "Epoch 615 | Mean Loss 1.234957218170166\n",
      "Epoch 616 | Mean Loss 1.2388031721115111\n",
      "Epoch 617 | Mean Loss 1.2660581827163697\n",
      "Epoch 618 | Mean Loss 1.2493893265724183\n",
      "Epoch 619 | Mean Loss 1.2627076864242555\n",
      "Epoch 620 | Mean Loss 1.2656526803970336\n",
      "Epoch 621 | Mean Loss 1.2453335404396058\n",
      "Epoch 622 | Mean Loss 1.2784750938415528\n",
      "Epoch 623 | Mean Loss 1.250542116165161\n",
      "Epoch 624 | Mean Loss 1.2590900897979735\n",
      "Epoch 625 | Mean Loss 1.2893893003463746\n",
      "Epoch 626 | Mean Loss 1.2458521962165832\n",
      "Epoch 627 | Mean Loss 1.2222703695297241\n",
      "Epoch 628 | Mean Loss 1.2633907794952393\n",
      "Epoch 629 | Mean Loss 1.2358529329299928\n",
      "Epoch 630 | Mean Loss 1.2256526470184326\n",
      "Epoch 631 | Mean Loss 1.2192004323005676\n",
      "Epoch 632 | Mean Loss 1.2846133708953857\n",
      "Epoch 633 | Mean Loss 1.2206244349479676\n",
      "Epoch 634 | Mean Loss 1.2491647720336914\n",
      "Epoch 635 | Mean Loss 1.2628706216812133\n",
      "Epoch 636 | Mean Loss 1.2223802566528321\n",
      "Epoch 637 | Mean Loss 1.2589204788208008\n",
      "Epoch 638 | Mean Loss 1.2355366826057435\n",
      "Epoch 639 | Mean Loss 1.2568871974945068\n",
      "Epoch 640 | Mean Loss 1.2523796081542968\n",
      "Epoch 641 | Mean Loss 1.2745690107345582\n",
      "Epoch 642 | Mean Loss 1.255162787437439\n",
      "Epoch 643 | Mean Loss 1.2310378551483154\n",
      "Epoch 644 | Mean Loss 1.2195449113845824\n",
      "Epoch 645 | Mean Loss 1.2655601739883422\n",
      "Epoch 646 | Mean Loss 1.2199679851531982\n",
      "Epoch 647 | Mean Loss 1.2926177740097047\n",
      "Epoch 648 | Mean Loss 1.2571365594863892\n",
      "Epoch 649 | Mean Loss 1.2235716700553894\n",
      "Epoch 650 | Mean Loss 1.2350056886672973\n",
      "Epoch 651 | Mean Loss 1.2203260660171509\n",
      "Epoch 652 | Mean Loss 1.2381964802742005\n",
      "Epoch 653 | Mean Loss 1.2824145793914794\n",
      "Epoch 654 | Mean Loss 1.2424915075302123\n",
      "Epoch 655 | Mean Loss 1.2503875494003296\n",
      "Epoch 656 | Mean Loss 1.287448263168335\n",
      "Epoch 657 | Mean Loss 1.2187093853950501\n",
      "Epoch 658 | Mean Loss 1.276441717147827\n",
      "Epoch 659 | Mean Loss 1.2261576294898986\n",
      "Epoch 660 | Mean Loss 1.2846624374389648\n",
      "Epoch 661 | Mean Loss 1.2619018316268922\n",
      "Epoch 662 | Mean Loss 1.2860968828201294\n",
      "Epoch 663 | Mean Loss 1.2373540163040162\n",
      "Epoch 664 | Mean Loss 1.2835547208786011\n",
      "Epoch 665 | Mean Loss 1.2713194847106934\n",
      "Epoch 666 | Mean Loss 1.223192858695984\n",
      "Epoch 667 | Mean Loss 1.2680680274963378\n",
      "Epoch 668 | Mean Loss 1.2243382930755615\n",
      "Epoch 669 | Mean Loss 1.241915488243103\n",
      "Epoch 670 | Mean Loss 1.2358707785606384\n",
      "Epoch 671 | Mean Loss 1.2377376317977906\n",
      "Epoch 672 | Mean Loss 1.2781180143356323\n",
      "Epoch 673 | Mean Loss 1.2624992728233337\n",
      "Epoch 674 | Mean Loss 1.2386106252670288\n",
      "Epoch 675 | Mean Loss 1.2506167411804199\n",
      "Epoch 676 | Mean Loss 1.2525158405303956\n",
      "Epoch 677 | Mean Loss 1.2260646581649781\n",
      "Epoch 678 | Mean Loss 1.2198639631271362\n",
      "Epoch 679 | Mean Loss 1.2589955449104309\n",
      "Epoch 680 | Mean Loss 1.2550373554229737\n",
      "Epoch 681 | Mean Loss 1.233132576942444\n",
      "Epoch 682 | Mean Loss 1.2340760946273803\n",
      "Epoch 683 | Mean Loss 1.2674877285957336\n",
      "Epoch 684 | Mean Loss 1.253816056251526\n",
      "Epoch 685 | Mean Loss 1.250835919380188\n",
      "Epoch 686 | Mean Loss 1.219178318977356\n",
      "Epoch 687 | Mean Loss 1.2319474458694457\n",
      "Epoch 688 | Mean Loss 1.2451808452606201\n",
      "Epoch 689 | Mean Loss 1.2710466146469117\n",
      "Epoch 690 | Mean Loss 1.2829272270202636\n",
      "Epoch 691 | Mean Loss 1.2716549277305602\n",
      "Epoch 692 | Mean Loss 1.2394668102264403\n",
      "Epoch 693 | Mean Loss 1.2580782294273376\n",
      "Epoch 694 | Mean Loss 1.2696391344070435\n",
      "Epoch 695 | Mean Loss 1.2956854581832886\n",
      "Epoch 696 | Mean Loss 1.2826850891113282\n",
      "Epoch 697 | Mean Loss 1.2387082815170287\n",
      "Epoch 698 | Mean Loss 1.2459362745285034\n",
      "Epoch 699 | Mean Loss 1.274380362033844\n",
      "Epoch 700 | Mean Loss 1.251425862312317\n",
      "Epoch 701 | Mean Loss 1.2599751949310303\n",
      "Epoch 702 | Mean Loss 1.3128145456314086\n",
      "Epoch 703 | Mean Loss 1.257103443145752\n",
      "Epoch 704 | Mean Loss 1.2620800018310547\n",
      "Epoch 705 | Mean Loss 1.2651082754135132\n",
      "Epoch 706 | Mean Loss 1.2737517833709717\n",
      "Epoch 707 | Mean Loss 1.2824568271636962\n",
      "Epoch 708 | Mean Loss 1.274064302444458\n",
      "Epoch 709 | Mean Loss 1.2964168071746827\n",
      "Epoch 710 | Mean Loss 1.2577234983444214\n",
      "Epoch 711 | Mean Loss 1.2620158672332764\n",
      "Epoch 712 | Mean Loss 1.2541889190673827\n",
      "Epoch 713 | Mean Loss 1.2465603351593018\n",
      "Epoch 714 | Mean Loss 1.2939237594604491\n",
      "Epoch 715 | Mean Loss 1.2645499229431152\n",
      "Epoch 716 | Mean Loss 1.250879979133606\n",
      "Epoch 717 | Mean Loss 1.2575771808624268\n",
      "Epoch 718 | Mean Loss 1.2628288984298706\n",
      "Epoch 719 | Mean Loss 1.2440030097961425\n",
      "Epoch 720 | Mean Loss 1.2678755044937133\n",
      "Epoch 721 | Mean Loss 1.2369080305099487\n",
      "Epoch 722 | Mean Loss 1.2680335760116577\n",
      "Epoch 723 | Mean Loss 1.235547935962677\n",
      "Epoch 724 | Mean Loss 1.281732153892517\n",
      "Epoch 725 | Mean Loss 1.2995662212371826\n",
      "Epoch 726 | Mean Loss 1.2262855529785157\n",
      "Epoch 727 | Mean Loss 1.2901496410369873\n",
      "Epoch 728 | Mean Loss 1.2745463132858277\n",
      "Epoch 729 | Mean Loss 1.2198030710220338\n",
      "Epoch 730 | Mean Loss 1.2490985631942748\n",
      "Epoch 731 | Mean Loss 1.225403332710266\n",
      "Epoch 732 | Mean Loss 1.2608773708343506\n",
      "Epoch 733 | Mean Loss 1.2869476079940796\n",
      "Epoch 734 | Mean Loss 1.2581003665924073\n",
      "Epoch 735 | Mean Loss 1.2603101015090943\n",
      "Epoch 736 | Mean Loss 1.2222968578338622\n",
      "Epoch 737 | Mean Loss 1.2479174137115479\n",
      "Epoch 738 | Mean Loss 1.287483286857605\n",
      "Epoch 739 | Mean Loss 1.2612385988235473\n",
      "Epoch 740 | Mean Loss 1.3103924512863159\n",
      "Epoch 741 | Mean Loss 1.2768141269683837\n",
      "Epoch 742 | Mean Loss 1.2618596792221068\n",
      "Epoch 743 | Mean Loss 1.2610139846801758\n",
      "Epoch 744 | Mean Loss 1.2398869395256042\n",
      "Epoch 745 | Mean Loss 1.2379907846450806\n",
      "Epoch 746 | Mean Loss 1.259199619293213\n",
      "Epoch 747 | Mean Loss 1.3070423364639283\n",
      "Epoch 748 | Mean Loss 1.2369843959808349\n",
      "Epoch 749 | Mean Loss 1.2488492012023926\n",
      "Epoch 750 | Mean Loss 1.2646118998527527\n",
      "Epoch 751 | Mean Loss 1.2384905815124512\n",
      "Epoch 752 | Mean Loss 1.3036739110946656\n",
      "Epoch 753 | Mean Loss 1.2263306856155396\n",
      "Epoch 754 | Mean Loss 1.3004443883895873\n",
      "Epoch 755 | Mean Loss 1.2715521693229674\n",
      "Epoch 756 | Mean Loss 1.22239146232605\n",
      "Epoch 757 | Mean Loss 1.2483086824417113\n",
      "Epoch 758 | Mean Loss 1.2769410848617553\n",
      "Epoch 759 | Mean Loss 1.2860769510269165\n",
      "Epoch 760 | Mean Loss 1.2733799457550048\n",
      "Epoch 761 | Mean Loss 1.2678402423858643\n",
      "Epoch 762 | Mean Loss 1.2925131559371947\n",
      "Epoch 763 | Mean Loss 1.2646769523620605\n",
      "Epoch 764 | Mean Loss 1.2607077598571776\n",
      "Epoch 765 | Mean Loss 1.244481635093689\n",
      "Epoch 766 | Mean Loss 1.2437412023544312\n",
      "Epoch 767 | Mean Loss 1.2773457527160645\n",
      "Epoch 768 | Mean Loss 1.26336110830307\n",
      "Epoch 769 | Mean Loss 1.2428136110305785\n",
      "Epoch 770 | Mean Loss 1.2478523254394531\n",
      "Epoch 771 | Mean Loss 1.283492875099182\n",
      "Epoch 772 | Mean Loss 1.2680195808410644\n",
      "Epoch 773 | Mean Loss 1.2604109287261962\n",
      "Epoch 774 | Mean Loss 1.3010407209396362\n",
      "Epoch 775 | Mean Loss 1.2727549076080322\n",
      "Epoch 776 | Mean Loss 1.2193762898445129\n",
      "Epoch 777 | Mean Loss 1.2356146812438964\n",
      "Epoch 778 | Mean Loss 1.2457401633262635\n",
      "Epoch 779 | Mean Loss 1.2537070274353028\n",
      "Epoch 780 | Mean Loss 1.2379552602767945\n",
      "Epoch 781 | Mean Loss 1.2732357263565064\n",
      "Epoch 782 | Mean Loss 1.2699854850769043\n",
      "Epoch 783 | Mean Loss 1.2464914798736573\n",
      "Epoch 784 | Mean Loss 1.2942144870758057\n",
      "Epoch 785 | Mean Loss 1.244109582901001\n",
      "Epoch 786 | Mean Loss 1.2661657571792602\n",
      "Epoch 787 | Mean Loss 1.264620018005371\n",
      "Epoch 788 | Mean Loss 1.24891037940979\n",
      "Epoch 789 | Mean Loss 1.258038067817688\n",
      "Epoch 790 | Mean Loss 1.2451921463012696\n",
      "Epoch 791 | Mean Loss 1.254625916481018\n",
      "Epoch 792 | Mean Loss 1.2342111349105835\n",
      "Epoch 793 | Mean Loss 1.2889792442321777\n",
      "Epoch 794 | Mean Loss 1.2529422283172607\n",
      "Epoch 795 | Mean Loss 1.233671736717224\n",
      "Epoch 796 | Mean Loss 1.2190497040748596\n",
      "Epoch 797 | Mean Loss 1.238298773765564\n",
      "Epoch 798 | Mean Loss 1.2584053993225097\n",
      "Epoch 799 | Mean Loss 1.3044858694076538\n",
      "Epoch 800 | Mean Loss 1.3007774829864502\n",
      "Epoch 801 | Mean Loss 1.2198024272918702\n",
      "Epoch 802 | Mean Loss 1.2560901641845703\n",
      "Epoch 803 | Mean Loss 1.236690592765808\n",
      "Epoch 804 | Mean Loss 1.235603404045105\n",
      "Epoch 805 | Mean Loss 1.248491644859314\n",
      "Epoch 806 | Mean Loss 1.2487135648727417\n",
      "Epoch 807 | Mean Loss 1.2654099822044373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 808 | Mean Loss 1.2406450510025024\n",
      "Epoch 809 | Mean Loss 1.2191683053970337\n",
      "Epoch 810 | Mean Loss 1.24475257396698\n",
      "Epoch 811 | Mean Loss 1.2357393026351928\n",
      "Epoch 812 | Mean Loss 1.224582552909851\n",
      "Epoch 813 | Mean Loss 1.2397825121879578\n",
      "Epoch 814 | Mean Loss 1.2519257068634033\n",
      "Epoch 815 | Mean Loss 1.2754154920578002\n",
      "Epoch 816 | Mean Loss 1.2453060507774354\n",
      "Epoch 817 | Mean Loss 1.2873723745346068\n",
      "Epoch 818 | Mean Loss 1.2583105087280273\n",
      "Epoch 819 | Mean Loss 1.243376660346985\n",
      "Epoch 820 | Mean Loss 1.2402193427085877\n",
      "Epoch 821 | Mean Loss 1.2912909030914306\n",
      "Epoch 822 | Mean Loss 1.2340953826904297\n",
      "Epoch 823 | Mean Loss 1.2270352482795714\n",
      "Epoch 824 | Mean Loss 1.2364802837371827\n",
      "Epoch 825 | Mean Loss 1.2188760042190552\n",
      "Epoch 826 | Mean Loss 1.272071075439453\n",
      "Epoch 827 | Mean Loss 1.2605307817459106\n",
      "Epoch 828 | Mean Loss 1.2429760694503784\n",
      "Epoch 829 | Mean Loss 1.2351041793823243\n",
      "Epoch 830 | Mean Loss 1.2414917588233947\n",
      "Epoch 831 | Mean Loss 1.292389154434204\n",
      "Epoch 832 | Mean Loss 1.2204845309257508\n",
      "Epoch 833 | Mean Loss 1.2198094844818115\n",
      "Epoch 834 | Mean Loss 1.240108323097229\n",
      "Epoch 835 | Mean Loss 1.2212806820869446\n",
      "Epoch 836 | Mean Loss 1.2342151761054994\n",
      "Epoch 837 | Mean Loss 1.237670612335205\n",
      "Epoch 838 | Mean Loss 1.242613959312439\n",
      "Epoch 839 | Mean Loss 1.2528782844543458\n",
      "Epoch 840 | Mean Loss 1.2675496697425843\n",
      "Epoch 841 | Mean Loss 1.2589638471603393\n",
      "Epoch 842 | Mean Loss 1.2340654611587525\n",
      "Epoch 843 | Mean Loss 1.2723946213722228\n",
      "Epoch 844 | Mean Loss 1.2818764686584472\n",
      "Epoch 845 | Mean Loss 1.2189578652381896\n",
      "Epoch 846 | Mean Loss 1.2348525285720826\n",
      "Epoch 847 | Mean Loss 1.2778944492340087\n",
      "Epoch 848 | Mean Loss 1.2204939007759095\n",
      "Epoch 849 | Mean Loss 1.274313712120056\n",
      "Epoch 850 | Mean Loss 1.2454871654510498\n",
      "Epoch 851 | Mean Loss 1.2195196390151977\n",
      "Epoch 852 | Mean Loss 1.2839233160018921\n",
      "Epoch 853 | Mean Loss 1.2478720903396607\n",
      "Epoch 854 | Mean Loss 1.2460472106933593\n",
      "Epoch 855 | Mean Loss 1.2422995090484619\n",
      "Epoch 856 | Mean Loss 1.260041642189026\n",
      "Epoch 857 | Mean Loss 1.2185191631317138\n",
      "Epoch 858 | Mean Loss 1.2208900213241578\n",
      "Epoch 859 | Mean Loss 1.2543401718139648\n",
      "Epoch 860 | Mean Loss 1.2632984399795533\n",
      "Epoch 861 | Mean Loss 1.2444286346435547\n",
      "Epoch 862 | Mean Loss 1.2733699321746825\n",
      "Epoch 863 | Mean Loss 1.2631026029586792\n",
      "Epoch 864 | Mean Loss 1.24178546667099\n",
      "Epoch 865 | Mean Loss 1.265661931037903\n",
      "Epoch 866 | Mean Loss 1.2445181369781495\n",
      "Epoch 867 | Mean Loss 1.2939590692520142\n",
      "Epoch 868 | Mean Loss 1.219770324230194\n",
      "Epoch 869 | Mean Loss 1.219335150718689\n",
      "Epoch 870 | Mean Loss 1.2509339332580567\n",
      "Epoch 871 | Mean Loss 1.2198241353034973\n",
      "Epoch 872 | Mean Loss 1.2203937411308288\n",
      "Epoch 873 | Mean Loss 1.2334300041198731\n",
      "Epoch 874 | Mean Loss 1.2363828897476197\n",
      "Epoch 875 | Mean Loss 1.2748711585998536\n",
      "Epoch 876 | Mean Loss 1.2437919139862061\n",
      "Epoch 877 | Mean Loss 1.233441948890686\n",
      "Epoch 878 | Mean Loss 1.2344192743301392\n",
      "Epoch 879 | Mean Loss 1.2408275365829469\n",
      "Epoch 880 | Mean Loss 1.2531880617141724\n",
      "Epoch 881 | Mean Loss 1.2187545776367188\n",
      "Epoch 882 | Mean Loss 1.25791654586792\n",
      "Epoch 883 | Mean Loss 1.2239293456077576\n",
      "Epoch 884 | Mean Loss 1.2706966161727906\n",
      "Epoch 885 | Mean Loss 1.237199592590332\n",
      "Epoch 886 | Mean Loss 1.2315608024597169\n",
      "Epoch 887 | Mean Loss 1.2568421840667725\n",
      "Epoch 888 | Mean Loss 1.256415557861328\n",
      "Epoch 889 | Mean Loss 1.2199211597442627\n",
      "Epoch 890 | Mean Loss 1.2601624488830567\n",
      "Epoch 891 | Mean Loss 1.2637931823730468\n",
      "Epoch 892 | Mean Loss 1.2285375952720643\n",
      "Epoch 893 | Mean Loss 1.2334610104560852\n",
      "Epoch 894 | Mean Loss 1.2594102144241333\n",
      "Epoch 895 | Mean Loss 1.2631804943084717\n",
      "Epoch 896 | Mean Loss 1.2586299419403075\n",
      "Epoch 897 | Mean Loss 1.219172763824463\n",
      "Epoch 898 | Mean Loss 1.2478673934936524\n",
      "Epoch 899 | Mean Loss 1.2962013721466064\n",
      "Epoch 900 | Mean Loss 1.2188004016876222\n",
      "Epoch 901 | Mean Loss 1.2526456594467164\n",
      "Epoch 902 | Mean Loss 1.248039674758911\n",
      "Epoch 903 | Mean Loss 1.282007384300232\n",
      "Epoch 904 | Mean Loss 1.2357786893844604\n",
      "Epoch 905 | Mean Loss 1.2675512552261352\n",
      "Epoch 906 | Mean Loss 1.2361062049865723\n",
      "Epoch 907 | Mean Loss 1.2635586738586426\n",
      "Epoch 908 | Mean Loss 1.2655837535858154\n",
      "Epoch 909 | Mean Loss 1.2540256977081299\n",
      "Epoch 910 | Mean Loss 1.2348997831344604\n",
      "Epoch 911 | Mean Loss 1.2620162487030029\n",
      "Epoch 912 | Mean Loss 1.265426015853882\n",
      "Epoch 913 | Mean Loss 1.2341671705245971\n",
      "Epoch 914 | Mean Loss 1.2363404512405396\n",
      "Epoch 915 | Mean Loss 1.2633317470550538\n",
      "Epoch 916 | Mean Loss 1.2194374799728394\n",
      "Epoch 917 | Mean Loss 1.2409267187118531\n",
      "Epoch 918 | Mean Loss 1.2546010494232178\n",
      "Epoch 919 | Mean Loss 1.2590672492980957\n",
      "Epoch 920 | Mean Loss 1.2185625433921814\n",
      "Epoch 921 | Mean Loss 1.2340046644210816\n",
      "Epoch 922 | Mean Loss 1.2606726288795471\n",
      "Epoch 923 | Mean Loss 1.2528708696365356\n",
      "Epoch 924 | Mean Loss 1.220159912109375\n",
      "Epoch 925 | Mean Loss 1.280230736732483\n",
      "Epoch 926 | Mean Loss 1.2445317029953002\n",
      "Epoch 927 | Mean Loss 1.2371352434158325\n",
      "Epoch 928 | Mean Loss 1.2611545324325562\n",
      "Epoch 929 | Mean Loss 1.2812521696090697\n",
      "Epoch 930 | Mean Loss 1.235724425315857\n",
      "Epoch 931 | Mean Loss 1.238693940639496\n",
      "Epoch 932 | Mean Loss 1.2605080366134644\n",
      "Epoch 933 | Mean Loss 1.236187708377838\n",
      "Epoch 934 | Mean Loss 1.2712692022323608\n",
      "Epoch 935 | Mean Loss 1.2721254587173463\n",
      "Epoch 936 | Mean Loss 1.2557843923568726\n",
      "Epoch 937 | Mean Loss 1.2891305446624757\n",
      "Epoch 938 | Mean Loss 1.258445143699646\n",
      "Epoch 939 | Mean Loss 1.2481652975082398\n",
      "Epoch 940 | Mean Loss 1.273381280899048\n",
      "Epoch 941 | Mean Loss 1.262728214263916\n",
      "Epoch 942 | Mean Loss 1.2797316551208495\n",
      "Epoch 943 | Mean Loss 1.241841197013855\n",
      "Epoch 944 | Mean Loss 1.2772319316864014\n",
      "Epoch 945 | Mean Loss 1.265073299407959\n",
      "Epoch 946 | Mean Loss 1.2581250190734863\n",
      "Epoch 947 | Mean Loss 1.2337809562683106\n",
      "Epoch 948 | Mean Loss 1.2382139921188355\n",
      "Epoch 949 | Mean Loss 1.2634222507476807\n",
      "Epoch 950 | Mean Loss 1.2487369060516358\n",
      "Epoch 951 | Mean Loss 1.2192484855651855\n",
      "Epoch 952 | Mean Loss 1.251896572113037\n",
      "Epoch 953 | Mean Loss 1.250470757484436\n",
      "Epoch 954 | Mean Loss 1.2503726482391357\n",
      "Epoch 955 | Mean Loss 1.254481554031372\n",
      "Epoch 956 | Mean Loss 1.2421154856681824\n",
      "Epoch 957 | Mean Loss 1.2574779272079468\n",
      "Epoch 958 | Mean Loss 1.233219861984253\n",
      "Epoch 959 | Mean Loss 1.2191774129867554\n",
      "Epoch 960 | Mean Loss 1.2480294346809386\n",
      "Epoch 961 | Mean Loss 1.2182383775711059\n",
      "Epoch 962 | Mean Loss 1.2895828485488892\n",
      "Epoch 963 | Mean Loss 1.2593222975730896\n",
      "Epoch 964 | Mean Loss 1.2327328205108643\n",
      "Epoch 965 | Mean Loss 1.2333086252212524\n",
      "Epoch 966 | Mean Loss 1.241846227645874\n",
      "Epoch 967 | Mean Loss 1.2927939176559449\n",
      "Epoch 968 | Mean Loss 1.233453345298767\n",
      "Epoch 969 | Mean Loss 1.2333108186721802\n",
      "Epoch 970 | Mean Loss 1.278290605545044\n",
      "Epoch 971 | Mean Loss 1.2853799343109131\n",
      "Epoch 972 | Mean Loss 1.240171980857849\n",
      "Epoch 973 | Mean Loss 1.276891279220581\n",
      "Epoch 974 | Mean Loss 1.2340506792068482\n",
      "Epoch 975 | Mean Loss 1.2478451371192931\n",
      "Epoch 976 | Mean Loss 1.2335487842559814\n",
      "Epoch 977 | Mean Loss 1.2505202412605285\n",
      "Epoch 978 | Mean Loss 1.2371376633644104\n",
      "Epoch 979 | Mean Loss 1.2661766290664673\n",
      "Epoch 980 | Mean Loss 1.2796940565109254\n",
      "Epoch 981 | Mean Loss 1.2985381364822388\n",
      "Epoch 982 | Mean Loss 1.2793123722076416\n",
      "Epoch 983 | Mean Loss 1.260415291786194\n",
      "Epoch 984 | Mean Loss 1.2559219121932983\n",
      "Epoch 985 | Mean Loss 1.2834483861923218\n",
      "Epoch 986 | Mean Loss 1.2490260124206543\n",
      "Epoch 987 | Mean Loss 1.2608273386955262\n",
      "Epoch 988 | Mean Loss 1.2367747664451598\n",
      "Epoch 989 | Mean Loss 1.2793195962905883\n",
      "Epoch 990 | Mean Loss 1.244987678527832\n",
      "Epoch 991 | Mean Loss 1.2232932448387146\n",
      "Epoch 992 | Mean Loss 1.2552448749542235\n",
      "Epoch 993 | Mean Loss 1.2797111511230468\n",
      "Epoch 994 | Mean Loss 1.2678606986999512\n",
      "Epoch 995 | Mean Loss 1.236589002609253\n",
      "Epoch 996 | Mean Loss 1.2213061332702637\n",
      "Epoch 997 | Mean Loss 1.2586493015289306\n",
      "Epoch 998 | Mean Loss 1.2604229927062989\n",
      "Epoch 999 | Mean Loss 1.2345916867256164\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a predefined split or a separate validation dataset\n",
    "# If not, you should split your train_dataset before standardizing\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Compute global mean and std for the entire training dataset\n",
    "features = torch.vstack([train_dataset[i][0] for i in range(len(train_loader))])\n",
    "targets = torch.vstack([train_dataset[i][1] for i in range(len(train_loader))])\n",
    "\n",
    "features_mean, features_std, targets_mean, targets_std = compute_global_stats(train_dataset)\n",
    "\n",
    "# Apply standardization to the entire dataset\n",
    "standardized_features = (features - features_mean) / features_std\n",
    "standardized_targets = (targets - targets_mean) / targets_std\n",
    "\n",
    "# Create TensorDataset for the standardized dataset\n",
    "standardized_train_dataset = TensorDataset(standardized_features, standardized_targets)\n",
    "\n",
    "# Split into training and validation datasets if needed\n",
    "# train_dataset, val_dataset = torch.utils.data.random_split(standardized_train_dataset, [train_size, val_size])\n",
    "# Assuming you have a separate validation dataset or have already split the data\n",
    "\n",
    "train_loader = DataLoader(standardized_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Ensure val_loader is defined, possibly using a separate validation set\n",
    "\n",
    "# Initialize your model, criterion, and optimizer\n",
    "model = MLP(input_dim_feat, input_dim_target, hidden_dim_feat, hidden_dim_target, output_dim).to(device)\n",
    "criterion = KernelizedSupCon(method='expw', temperature=temperature, base_temperature=base_temperature, kernel=kernel)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    batch_losses = []\n",
    "    for batch_num, (features, targets) in enumerate(train_loader):\n",
    "        features, targets = features.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out_feat, out_target = model(features, targets)\n",
    "        loss = criterion(out_feat, out_target)\n",
    "        loss.backward()\n",
    "        batch_losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch} | Mean Loss {sum(batch_losses)/len(batch_losses)}')\n",
    "\n",
    "# Validation loop\n",
    "# val_losses = []\n",
    "# model.eval() \n",
    "# with torch.no_grad():\n",
    "#     total_loss = 0\n",
    "#     total_samples = 0\n",
    "#     for features, targets in val_loader:\n",
    "#         features = features.to(device).float()\n",
    "#         targets = targets.to(device)\n",
    "\n",
    "#         out_feat, out_target = model(features, targets)\n",
    "#         loss = criterion(out_feat, out_target)\n",
    "#         val_losses.append(loss.item())\n",
    "#         total_loss += loss.item() * features.size(0)\n",
    "#         total_samples += features.size(0)\n",
    "#     val_losses = np.array(val_losses)\n",
    "#     average_loss = total_loss / total_samples\n",
    "#     print('Validation Mean Loss: %6.2f' % (average_loss))\n",
    "\n",
    "# Compute and print evaluation metrics if necessary\n",
    "# mape_train, mape_val = compute_target_score(model, train_loader, val_loader, device, 'mape')\n",
    "# r2_train, r2_val = compute_target_score(model, train_loader, val_loader, device, 'r2')\n",
    "# results_cv.append(['Overall', mape_train, r2_train, mape_val, r2_val])\n",
    "# print(results_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_cv, columns=['Fold', 'Train_MAPE', 'Train_R2', 'Val_MAPE', 'Val_R2'])\n",
    "results_df.to_csv('cv_results_hopkins.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold</th>\n",
       "      <th>Train_MAPE</th>\n",
       "      <th>Train_R2</th>\n",
       "      <th>Val_MAPE</th>\n",
       "      <th>Val_R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.739708</td>\n",
       "      <td>0.650901</td>\n",
       "      <td>1.740213</td>\n",
       "      <td>-1.514190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.814204</td>\n",
       "      <td>0.685313</td>\n",
       "      <td>2.286090</td>\n",
       "      <td>-0.881625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.109953</td>\n",
       "      <td>0.616215</td>\n",
       "      <td>3.290102</td>\n",
       "      <td>-0.354272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.385824</td>\n",
       "      <td>0.508040</td>\n",
       "      <td>2.310466</td>\n",
       "      <td>-0.470970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.262301</td>\n",
       "      <td>0.524027</td>\n",
       "      <td>1.605523</td>\n",
       "      <td>-0.505183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fold  Train_MAPE  Train_R2  Val_MAPE    Val_R2\n",
       "0     0    0.739708  0.650901  1.740213 -1.514190\n",
       "1     1    1.814204  0.685313  2.286090 -0.881625\n",
       "2     2    1.109953  0.616215  3.290102 -0.354272\n",
       "3     3    1.385824  0.508040  2.310466 -0.470970\n",
       "4     4    1.262301  0.524027  1.605523 -0.505183"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print evaluation metrics if necessary\n",
    "mape_train, mape_val = compute_target_score(model, train_loader, val_loader, device, 'mape')\n",
    "r2_train, r2_val = compute_target_score(model, train_loader, val_loader, device, 'r2')\n",
    "results_cv.append(['Overall', mape_train, r2_train, mape_val, r2_val])\n",
    "print(results_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 499500])\n",
      "torch.Size([30, 59])\n"
     ]
    }
   ],
   "source": [
    "standardized_test_dataset = standardize_dataset(test_dataset)\n",
    "std_test_loader = DataLoader(standardized_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(input_dim_feat, input_dim_target, hidden_dim_feat, hidden_dim_target, output_dim)\n",
    "model.load_state_dict(torch.load('best_model_hopkins_cv.pt')[\"model\"])\n",
    "criterion = CustomContrastiveLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "optimizer.load_state_dict(torch.load('best_model_hopkins_cv.pt')[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load('best_model_hopkins_cv.pt')[\"cv_fold\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (feat_mlp): Sequential(\n",
       "    (0): Linear(in_features=499500, out_features=1000, bias=True)\n",
       "    (1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): Linear(in_features=1000, out_features=2, bias=True)\n",
       "  )\n",
       "  (target_mlp): Sequential(\n",
       "    (0): Linear(in_features=59, out_features=24, bias=True)\n",
       "    (1): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): Linear(in_features=24, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Loss:   1.01\n"
     ]
    }
   ],
   "source": [
    "test_losses = []\n",
    "model.eval()\n",
    "emb_features = [] # saving the embedded features for each batch\n",
    "emb_targets = []\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    for batch_num, (features, targets) in enumerate(std_test_loader):\n",
    "        features = features.to(device).float()\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        out_feat, out_target = model(features, targets)\n",
    "        emb_features.append(out_feat.cpu())\n",
    "        emb_targets.append(out_target.cpu())\n",
    "        loss = criterion(out_feat, out_target)\n",
    "        test_losses.append(loss.item())\n",
    "        total_loss += loss.item() * features.size(0)\n",
    "        total_samples += features.size(0)\n",
    "        \n",
    "    test_losses =np.array(test_losses)\n",
    "    average_loss = total_loss / total_samples\n",
    "    print('Mean Test Loss: %6.2f' % (average_loss))\n",
    "    #np.save(f\"losses/test_losses_batch{batch_num}.npy\", test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_features = np.row_stack(emb_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_targets = np.row_stack(emb_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_features = pd.DataFrame(emb_features,columns = [\"Dim_1\", \"Dim_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_targets = pd.DataFrame(emb_targets,columns = [\"Dim_1\", \"Dim_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_features[\"sub\"] = np.arange(1, len(emb_features) +1)\n",
    "emb_targets[\"sub\"] = np.arange(1, len(emb_targets) +1)\n",
    "emb_features[\"Type\"] = 'Features'\n",
    "emb_targets[\"Type\"] = 'Targets'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs3/well/margulies/users/cpy397/.local/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/gpfs3/well/margulies/users/cpy397/.local/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/gpfs3/well/margulies/users/cpy397/.local/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/gpfs3/well/margulies/users/cpy397/.local/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/gpfs3/well/margulies/users/cpy397/.local/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/gpfs3/well/margulies/users/cpy397/.local/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f8cc0098f40>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.lines as mlines\n",
    "\n",
    "sns.scatterplot(emb_features, x = 'Dim_1', y = 'Dim_2', s = 100, alpha = 0.8, hue = 'sub', palette = 'nipy_spectral', label='Features')\n",
    "sns.scatterplot(emb_targets, x = 'Dim_1', marker = 'v', y = 'Dim_2', s = 100, alpha = 0.8, hue = 'sub', palette = 'nipy_spectral', label='Targets')\n",
    "plt.xlim(-1.2, 1.2)\n",
    "plt.ylim(-1.2, 1.2)\n",
    "\n",
    "feature_handle = mlines.Line2D([], [], color='black', marker='o', linestyle='None', markersize=10, label='Features')\n",
    "target_handle = mlines.Line2D([], [], color='black', marker='v', linestyle='None', markersize=10, label='Targets')\n",
    "\n",
    "plt.legend(handles=[feature_handle, target_handle])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dim_1</th>\n",
       "      <th>Dim_2</th>\n",
       "      <th>sub</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.965320</td>\n",
       "      <td>-0.261070</td>\n",
       "      <td>1</td>\n",
       "      <td>Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.710620</td>\n",
       "      <td>-0.703576</td>\n",
       "      <td>2</td>\n",
       "      <td>Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.954792</td>\n",
       "      <td>-0.297276</td>\n",
       "      <td>3</td>\n",
       "      <td>Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.984910</td>\n",
       "      <td>-0.173069</td>\n",
       "      <td>4</td>\n",
       "      <td>Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.943903</td>\n",
       "      <td>-0.330222</td>\n",
       "      <td>5</td>\n",
       "      <td>Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.922472</td>\n",
       "      <td>-0.386064</td>\n",
       "      <td>113</td>\n",
       "      <td>Targets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>-0.851817</td>\n",
       "      <td>0.523839</td>\n",
       "      <td>114</td>\n",
       "      <td>Targets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.986375</td>\n",
       "      <td>-0.164511</td>\n",
       "      <td>115</td>\n",
       "      <td>Targets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.998986</td>\n",
       "      <td>0.045019</td>\n",
       "      <td>116</td>\n",
       "      <td>Targets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>-0.739074</td>\n",
       "      <td>0.673624</td>\n",
       "      <td>117</td>\n",
       "      <td>Targets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>234 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dim_1     Dim_2  sub      Type\n",
       "0    0.965320 -0.261070    1  Features\n",
       "1   -0.710620 -0.703576    2  Features\n",
       "2   -0.954792 -0.297276    3  Features\n",
       "3   -0.984910 -0.173069    4  Features\n",
       "4   -0.943903 -0.330222    5  Features\n",
       "..        ...       ...  ...       ...\n",
       "112  0.922472 -0.386064  113   Targets\n",
       "113 -0.851817  0.523839  114   Targets\n",
       "114  0.986375 -0.164511  115   Targets\n",
       "115  0.998986  0.045019  116   Targets\n",
       "116 -0.739074  0.673624  117   Targets\n",
       "\n",
       "[234 rows x 4 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dim_1</th>\n",
       "      <th>Dim_2</th>\n",
       "      <th>sub</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.954792</td>\n",
       "      <td>-0.297276</td>\n",
       "      <td>3</td>\n",
       "      <td>Features</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Dim_1     Dim_2  sub      Type\n",
       "2 -0.954792 -0.297276    3  Features"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[(embeddings['sub'] == 3) & (embeddings['Type'] == \"Features\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dim_1</th>\n",
       "      <th>Dim_2</th>\n",
       "      <th>sub</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.997188</td>\n",
       "      <td>0.074936</td>\n",
       "      <td>3</td>\n",
       "      <td>Targets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Dim_1     Dim_2  sub     Type\n",
       "2  0.997188  0.074936    3  Targets"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[(embeddings['sub'] == 3) & (embeddings['Type'] == \"Targets\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training target estimator\n",
      "Training target estimator\n",
      "Train MAPE: 1.5097272396087646, Test MAPE: 1.603095293045044.\n",
      "Train R2: 6.000279517137663e-07, Test R2: -0.048680421194799206.\n"
     ]
    }
   ],
   "source": [
    "mae_train, mae_test = compute_target_score(model, train_loader, test_loader, device, 'mape')\n",
    "r2_train, r2_test = compute_target_score(model, train_loader, test_loader, device, 'r2')\n",
    "\n",
    "print(f\"Train MAPE: {mae_train}, Test MAPE: {mae_test}.\")\n",
    "print(f\"Train R2: {r2_train}, Test R2: {r2_test}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from estimators import TargetEstimator\n",
    "from utils_v import gather_feats_targets\n",
    "\n",
    "X_train, y_train = gather_feats_targets(model, train_loader, device)\n",
    "X_test, y_test = gather_feats_targets(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.row_stack((X_train, X_test))\n",
    "y = np.concatenate((y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TargetEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, scoring = 'r2') #neg_mean_absolute_percentage_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display = LearningCurveDisplay(train_sizes=train_sizes, train_scores=train_scores, test_scores=test_scores, score_name=\"r2\")\n",
    "display.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
