{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from nilearn.connectome import sym_matrix_to_vec\n",
    "from cmath import isinf\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import math\n",
    "from utils_v import compute_target_score, estimate_target, save_model, standardize_dataset\n",
    "from cmath import isinf\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split, KFold, LearningCurveDisplay, learning_curve\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_percentage_error, r2_score\n",
    "from helper_classes import MatData, MLP\n",
    "from dev_losses import cauchy, rbf, gaussian_kernel, CustomSupCon, CustomContrastiveLoss\n",
    "from losses import KernelizedSupCon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim_feat, input_dim_target, hidden_dim_feat, output_dim, dropout_rate):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # Xavier initialization for feature MLP\n",
    "        self.feat_mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim_feat, hidden_dim_feat),\n",
    "            nn.BatchNorm1d(hidden_dim_feat),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(hidden_dim_feat, output_dim)\n",
    "        )\n",
    "        self.init_weights(self.feat_mlp)\n",
    "\n",
    "        # Xavier initialization for target MLP\n",
    "        self.target_mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim_target, output_dim)\n",
    "        )\n",
    "        self.init_weights(self.target_mlp)\n",
    "        \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        features = self.feat_mlp(x)\n",
    "        targets = self.target_mlp(y)\n",
    "        features = nn.functional.normalize(features, p=2, dim=1)\n",
    "        targets = nn.functional.normalize(targets, p=2, dim=1)\n",
    "        return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mirror_conn(matrix, region_indices, vecrorize_mat = True):\n",
    "    \"\"\"\n",
    "    Flips the connectivity of specified regions within their hemispheres. Handles both single indices and lists of indices.\n",
    "    \n",
    "    :param matrix: The connectivity matrix.\n",
    "    :param region_indices: A single index or a list of indices for the regions.\n",
    "    :return: A 3D array of modified connectivity matrices. Each \"slice\" corresponds to the matrix after flipping each specified region.\n",
    "    \"\"\"\n",
    "    if not isinstance(region_indices, list):\n",
    "        region_indices = [region_indices]\n",
    "    \n",
    "    flipped_matrices = []\n",
    "    \n",
    "    for index in region_indices:\n",
    "        new_matrix = matrix.copy()\n",
    "        hemisphere_size = matrix.shape[0] // 2\n",
    "        is_left_hemisphere = index < hemisphere_size\n",
    "        opposite_index = index + (-1 if is_left_hemisphere else 1) * hemisphere_size\n",
    "\n",
    "        # Flip connectivity for the specified region within its hemisphere\n",
    "        if is_left_hemisphere:\n",
    "            new_matrix[index, :hemisphere_size], new_matrix[opposite_index, :hemisphere_size] = \\\n",
    "                new_matrix[opposite_index, :hemisphere_size].copy(), new_matrix[index, :hemisphere_size].copy()\n",
    "        else:\n",
    "            new_matrix[index, hemisphere_size:], new_matrix[opposite_index, hemisphere_size:] = \\\n",
    "                new_matrix[opposite_index, hemisphere_size:].copy(), new_matrix[index, hemisphere_size:].copy()\n",
    "            \n",
    "        if vecrorize_mat:\n",
    "            new_matrix = sym_matrix_to_vec(new_matrix, discard_diagonal = True)\n",
    "        flipped_matrices.append(new_matrix)\n",
    "        \n",
    "    return np.array(flipped_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatData(Dataset):\n",
    "    def __init__(self, path_feat, path_target, target_name, transform = None, train=True, train_size = 0.8, test_size=None, region_indices = None, random_state=42):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with the capability to handle training and testing splits, \n",
    "        including multiple views for augmented data.\n",
    "        \n",
    "        Args:\n",
    "            path_feat (str): Path to the features file.\n",
    "            path_target (str): Path to the target file.\n",
    "            transform (callable): A transformation function to apply for augmentation.\n",
    "            train (bool): Whether the dataset is used for training. False will load the test set.\n",
    "            test_size (float): Proportion of the dataset to include in the test split.\n",
    "            random_state (int): Random state for reproducible train-test splits.\n",
    "        \"\"\"\n",
    "        # Load the entire dataset\n",
    "        features = np.load(path_feat)\n",
    "        targets = np.expand_dims(pd.read_csv(path_target)[target_name].values, axis = 1)        \n",
    "\n",
    "        # Split the dataset into training and test sets\n",
    "        train_indices, test_indices = train_test_split(np.arange(len(features)), \n",
    "                                                       train_size = train_size,\n",
    "                                                       test_size=test_size,                \n",
    "                                                       random_state=random_state)\n",
    "        \n",
    "        if train:\n",
    "            selected_indices = train_indices\n",
    "        else:\n",
    "            selected_indices = test_indices\n",
    "        \n",
    "        # Select the subset of data for the current mode (train/test)\n",
    "        features = features[selected_indices]\n",
    "        targets = targets[selected_indices]\n",
    "        \n",
    "\n",
    "        self.n_sub = len(features)\n",
    "        self.n_views = 1\n",
    "        self.transform = transform\n",
    "        self.targets = targets\n",
    "        \n",
    "        vectorized_feat = np.array([sym_matrix_to_vec(mat, discard_diagonal=True) for mat in features])\n",
    "        self.n_features = vectorized_feat.shape[-1]\n",
    "        \n",
    "        if train:\n",
    "            # augmentation only in training mode\n",
    "            augmented_features = np.array([self.transform(sample, region_indices = region_indices) for sample in features])\n",
    "\n",
    "            self.n_views = self.n_views + augmented_features.shape[1]\n",
    "            self.features = np.zeros((self.n_sub, self.n_views, self.n_features))\n",
    "            for sub in range(self.n_sub):\n",
    "                self.features[sub, 0, :] = vectorized_feat[sub]\n",
    "                self.features[sub, 1:, :] = augmented_features[sub]\n",
    "        else:\n",
    "            self.features = vectorized_feat\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.n_sub\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.features[idx]\n",
    "        targets = self.targets[idx]\n",
    "        features = torch.from_numpy(features).float()\n",
    "        targets = torch.from_numpy(targets).float()\n",
    "        \n",
    "        return features, targets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_dataset(dataset):\n",
    "    all_features_flat = torch.cat([dataset[i][0].view(-1, dataset[i][0].shape[-1]) for i in range(len(dataset))], dim=0)\n",
    "    all_targets = torch.cat([dataset[i][1].unsqueeze(0) for i in range(len(dataset))], dim=0)\n",
    "    \n",
    "    features_mean = all_features_flat.mean(dim=0)\n",
    "    features_std = all_features_flat.std(dim=0)\n",
    "    \n",
    "    features_std[features_std == 0] = 1\n",
    "    standardized_features_list = []\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        features = dataset[i][0].view(-1, dataset[i][0].shape[-1])\n",
    "        standardized_features = (features - features_mean) / features_std\n",
    "        standardized_features_list.append(standardized_features.view(dataset[i][0].shape))\n",
    "    \n",
    "    standardized_features = torch.stack(standardized_features_list)\n",
    "    \n",
    "    standardized_dataset = TensorDataset(standardized_features, all_targets)\n",
    "    \n",
    "    return standardized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x, krnl_sigma):\n",
    "    x = x - x.T\n",
    "    return torch.exp(-(x**2) / (2*(krnl_sigma**2))) / (math.sqrt(2*torch.pi)*krnl_sigma)\n",
    "\n",
    "def cauchy(x, krnl_sigma):\n",
    "        x = x - x.T\n",
    "        return  1. / (krnl_sigma*(x**2) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs3/well/margulies/users/cpy397/contrastive-learning\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss from: https://github.com/EIDOSLAB/contrastive-brain-age-prediction/blob/master/src/losses.py\n",
    "# modified to accept input shape [bsz, n_feats]. In the age paper: [bsz, n_views, n_feats].\n",
    "class KernelizedSupCon(nn.Module):\n",
    "    \"\"\"Supervised contrastive loss: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\n",
    "    Based on: https://github.com/HobbitLong/SupContrast\"\"\"\n",
    "    def __init__(self, method: str, temperature: float=0.07, contrast_mode: str='all',\n",
    "                 base_temperature: float=0.07, krnl_sigma: float = 1., kernel: callable=None, delta_reduction: str='sum'):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "        self.method = method\n",
    "        self.kernel = kernel\n",
    "        self.krnl_sigma = krnl_sigma\n",
    "        self.delta_reduction = delta_reduction\n",
    "\n",
    "        if kernel is not None and method == 'supcon':\n",
    "            raise ValueError('Kernel must be none if method=supcon')\n",
    "        \n",
    "        if kernel is None and method != 'supcon':\n",
    "            raise ValueError('Kernel must not be none if method != supcon')\n",
    "\n",
    "        if delta_reduction not in ['mean', 'sum']:\n",
    "            raise ValueError(f\"Invalid reduction {delta_reduction}\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__} ' \\\n",
    "               f'(t={self.temperature}, ' \\\n",
    "               f'method={self.method}, ' \\\n",
    "               f'kernel={self.kernel is not None}, ' \\\n",
    "               f'delta_reduction={self.delta_reduction})'\n",
    "\n",
    "    def forward(self, features, labels=None):\n",
    "        \"\"\"Compute loss for model. If `labels` is None, \n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, n_features]. \n",
    "                input has to be rearranged to [bsz, n_views, n_features] and labels [bsz],\n",
    "            labels: ground truth of shape [bsz].\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "        device = features.device\n",
    "\n",
    "        if len(features.shape) != 3:\n",
    "            raise ValueError('`features` needs to be [bsz, n_views, n_feats],'\n",
    "                             '3 dimensions are required')\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        n_views = features.shape[1]\n",
    "\n",
    "        if labels is None:\n",
    "            mask = torch.eye(batch_size, device=device)\n",
    "        \n",
    "        else:\n",
    "            # labels = labels.view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError('Num of labels does not match num of features')\n",
    "            \n",
    "            if self.kernel is None:\n",
    "                mask = torch.eq(labels, labels.T)\n",
    "            else:\n",
    "                mask = self.kernel(labels, krnl_sigma = self.krnl_sigma)\n",
    "            \n",
    "        view_count = features.shape[1]\n",
    "        features = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == 'one':\n",
    "            features = features[:, 0]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == 'all':\n",
    "            features = features\n",
    "            anchor_count = view_count\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
    "\n",
    "        # Tile mask\n",
    "        mask = mask.repeat(anchor_count, view_count)\n",
    "\n",
    "        # Inverse of torch-eye to remove self-contrast (diagonal)\n",
    "        inv_diagonal = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size*n_views, device=device).view(-1, 1),\n",
    "            0\n",
    "        )\n",
    "\n",
    "        # compute similarity\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(features, features.T),\n",
    "            self.temperature\n",
    "        )\n",
    "\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        alignment = logits \n",
    "\n",
    "        # base case is:\n",
    "        # - supcon if kernel = none \n",
    "        # - y-aware is kernel != none\n",
    "        uniformity = torch.exp(logits) * inv_diagonal \n",
    "\n",
    "        if self.method == 'threshold':\n",
    "            repeated = mask.unsqueeze(-1).repeat(1, 1, mask.shape[0]) # repeat kernel mask\n",
    "\n",
    "            delta = (mask[:, None].T - repeated.T).transpose(1, 2) # compute the difference w_k - w_j for every k,j\n",
    "            delta = (delta > 0.).float()\n",
    "\n",
    "            # for each z_i, repel only samples j s.t. K(z_i, z_j) < K(z_i, z_k)\n",
    "            uniformity = uniformity.unsqueeze(-1).repeat(1, 1, mask.shape[0])\n",
    "\n",
    "            if self.delta_reduction == 'mean':\n",
    "                uniformity = (uniformity * delta).mean(-1)\n",
    "            else:\n",
    "                uniformity = (uniformity * delta).sum(-1)\n",
    "    \n",
    "        elif self.method == 'expw':\n",
    "            # exp weight e^(s_j(1-w_j))\n",
    "            uniformity = torch.exp(logits * (1 - mask)) * inv_diagonal\n",
    "\n",
    "        uniformity = torch.log(uniformity.sum(1, keepdim=True))\n",
    "\n",
    "\n",
    "        # positive mask contains the anchor-positive pairs\n",
    "        # excluding <self,self> on the diagonal\n",
    "        positive_mask = mask * inv_diagonal\n",
    "\n",
    "        log_prob = alignment - uniformity # log(alignment/uniformity) = log(alignment) - log(uniformity)\n",
    "        log_prob = (positive_mask * log_prob).sum(1) / positive_mask.sum(1) # compute mean of log-likelihood over positive\n",
    " \n",
    "        # loss\n",
    "        loss = - (self.temperature / self.base_temperature) * log_prob\n",
    "        return loss.mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dim_feat = 499500 # vectorized mat, diagonal discarded\n",
    "# input_dim_target = 59\n",
    "# # the rest is arbitrary\n",
    "# hidden_dim_feat_1 = 1024\n",
    "# hidden_dim_feat_2 = 512\n",
    "# hidden_dim_target_1 = 24\n",
    "# hidden_dim_target_2 = 8\n",
    "# output_dim = 2\n",
    "# num_epochs = 1000\n",
    "\n",
    "input_dim_feat = 499500 # vectorized mat, diagonal discarded\n",
    "# the rest is arbitrary\n",
    "hidden_dim_feat = 1000\n",
    "input_dim_target = 1\n",
    "output_dim = 2\n",
    "num_epochs = 1000\n",
    "\n",
    "lr = 0.001 # too low values return nan loss\n",
    "kernel = cauchy\n",
    "batch_size = 10 # too low values return nan loss\n",
    "dropout_rate = 0\n",
    "weight_decay = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MatData(\"matrices.npy\", \"participants.csv\", \"age\", transform = mirror_conn, region_indices = [100], train_size = 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MatData(\"matrices.npy\", \"participants.csv\", \"age\", train=False, test_size = 47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_train_dataset = standardize_dataset(train_dataset)\n",
    "std_train_loader = DataLoader(standardized_train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_test_dataset = standardize_dataset(test_dataset)\n",
    "std_test_loader = DataLoader(standardized_test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Mean Loss 16.469335269927978\n",
      "Epoch 1 | Mean Loss 10.930840492248535\n",
      "Epoch 2 | Mean Loss 7.3519645690917965\n",
      "Epoch 3 | Mean Loss 6.585882997512817\n",
      "Epoch 4 | Mean Loss 6.322476482391357\n",
      "Epoch 5 | Mean Loss 6.207472658157348\n",
      "Epoch 6 | Mean Loss 6.063387680053711\n",
      "Epoch 7 | Mean Loss 5.951940059661865\n",
      "Epoch 8 | Mean Loss 5.965478467941284\n",
      "Epoch 9 | Mean Loss 5.9922338962554935\n",
      "Epoch 10 | Mean Loss 5.928503227233887\n",
      "Epoch 11 | Mean Loss 5.940913534164428\n",
      "Epoch 12 | Mean Loss 5.92286229133606\n",
      "Epoch 13 | Mean Loss 5.9295234203338625\n",
      "Epoch 14 | Mean Loss 5.9124500274658205\n",
      "Epoch 15 | Mean Loss 5.900765371322632\n",
      "Epoch 16 | Mean Loss 5.883850860595703\n",
      "Epoch 17 | Mean Loss 5.88899598121643\n",
      "Epoch 18 | Mean Loss 5.885649108886719\n",
      "Epoch 19 | Mean Loss 5.875869274139404\n",
      "Epoch 20 | Mean Loss 5.870638227462768\n",
      "Epoch 21 | Mean Loss 5.869984579086304\n",
      "Epoch 22 | Mean Loss 5.851471281051635\n",
      "Epoch 23 | Mean Loss 5.84382472038269\n",
      "Epoch 24 | Mean Loss 5.8206117153167725\n",
      "Epoch 25 | Mean Loss 5.771181535720825\n",
      "Epoch 26 | Mean Loss 5.672056198120117\n",
      "Epoch 27 | Mean Loss 5.423141574859619\n",
      "Epoch 28 | Mean Loss 5.173206424713134\n",
      "Epoch 29 | Mean Loss 5.189647388458252\n",
      "Epoch 30 | Mean Loss 5.198751497268677\n",
      "Epoch 31 | Mean Loss 5.1499580383300785\n",
      "Epoch 32 | Mean Loss 5.1110491275787355\n",
      "Epoch 33 | Mean Loss 5.153929662704468\n",
      "Epoch 34 | Mean Loss 5.141857862472534\n",
      "Epoch 35 | Mean Loss 5.133369970321655\n",
      "Epoch 36 | Mean Loss 5.10091986656189\n",
      "Epoch 37 | Mean Loss 5.187199306488037\n",
      "Epoch 38 | Mean Loss 5.1284730434417725\n",
      "Epoch 39 | Mean Loss 5.125211429595947\n",
      "Epoch 40 | Mean Loss 5.140296554565429\n",
      "Epoch 41 | Mean Loss 5.199913597106933\n",
      "Epoch 42 | Mean Loss 5.097866153717041\n",
      "Epoch 43 | Mean Loss 5.098955869674683\n",
      "Epoch 44 | Mean Loss 5.153605890274048\n",
      "Epoch 45 | Mean Loss 5.109753227233886\n",
      "Epoch 46 | Mean Loss 5.1439292430877686\n",
      "Epoch 47 | Mean Loss 5.098072195053101\n",
      "Epoch 48 | Mean Loss 5.096111392974853\n",
      "Epoch 49 | Mean Loss 5.123910284042358\n",
      "Epoch 50 | Mean Loss 5.112642478942871\n",
      "Epoch 51 | Mean Loss 5.1100554943084715\n",
      "Epoch 52 | Mean Loss 5.105150890350342\n",
      "Epoch 53 | Mean Loss 5.069851875305176\n",
      "Epoch 54 | Mean Loss 5.073601293563843\n",
      "Epoch 55 | Mean Loss 5.067547178268432\n",
      "Epoch 56 | Mean Loss 5.086072587966919\n",
      "Epoch 57 | Mean Loss 5.15512170791626\n",
      "Epoch 58 | Mean Loss 5.097381734848023\n",
      "Epoch 59 | Mean Loss 5.108060359954834\n",
      "Epoch 60 | Mean Loss 5.076542806625366\n",
      "Epoch 61 | Mean Loss 5.118492507934571\n",
      "Epoch 62 | Mean Loss 5.0492781639099125\n",
      "Epoch 63 | Mean Loss 5.075332593917847\n",
      "Epoch 64 | Mean Loss 5.102091073989868\n",
      "Epoch 65 | Mean Loss 5.05787992477417\n",
      "Epoch 66 | Mean Loss 5.031580543518066\n",
      "Epoch 67 | Mean Loss 5.028226375579834\n",
      "Epoch 68 | Mean Loss 5.07071099281311\n",
      "Epoch 69 | Mean Loss 5.055879354476929\n",
      "Epoch 70 | Mean Loss 5.0623101711273195\n",
      "Epoch 71 | Mean Loss 5.103039026260376\n",
      "Epoch 72 | Mean Loss 5.061061239242553\n",
      "Epoch 73 | Mean Loss 5.148400068283081\n",
      "Epoch 74 | Mean Loss 5.093188285827637\n",
      "Epoch 75 | Mean Loss 5.0620200634002686\n",
      "Epoch 76 | Mean Loss 5.072778367996216\n",
      "Epoch 77 | Mean Loss 5.03771448135376\n",
      "Epoch 78 | Mean Loss 5.084246492385864\n",
      "Epoch 79 | Mean Loss 5.09012279510498\n",
      "Epoch 80 | Mean Loss 5.0518406391143795\n",
      "Epoch 81 | Mean Loss 5.043679046630859\n",
      "Epoch 82 | Mean Loss 5.040081596374511\n",
      "Epoch 83 | Mean Loss 5.086857557296753\n",
      "Epoch 84 | Mean Loss 5.10343656539917\n",
      "Epoch 85 | Mean Loss 5.092522668838501\n",
      "Epoch 86 | Mean Loss 5.09910569190979\n",
      "Epoch 87 | Mean Loss 5.153229713439941\n",
      "Epoch 88 | Mean Loss 5.106672906875611\n",
      "Epoch 89 | Mean Loss 5.063117551803589\n",
      "Epoch 90 | Mean Loss 5.04495587348938\n",
      "Epoch 91 | Mean Loss 5.043129920959473\n",
      "Epoch 92 | Mean Loss 5.076097011566162\n",
      "Epoch 93 | Mean Loss 5.112110328674317\n",
      "Epoch 94 | Mean Loss 5.114070034027099\n",
      "Epoch 95 | Mean Loss 5.099896478652954\n",
      "Epoch 96 | Mean Loss 5.058935260772705\n",
      "Epoch 97 | Mean Loss 5.045878744125366\n",
      "Epoch 98 | Mean Loss 5.121572351455688\n",
      "Epoch 99 | Mean Loss 5.067602682113647\n",
      "Epoch 100 | Mean Loss 5.049476099014282\n",
      "Epoch 101 | Mean Loss 5.103144788742066\n",
      "Epoch 102 | Mean Loss 5.05714373588562\n",
      "Epoch 103 | Mean Loss 5.103469324111939\n",
      "Epoch 104 | Mean Loss 5.085994911193848\n",
      "Epoch 105 | Mean Loss 5.075458288192749\n",
      "Epoch 106 | Mean Loss 5.146514081954956\n",
      "Epoch 107 | Mean Loss 5.10811219215393\n",
      "Epoch 108 | Mean Loss 5.109109783172608\n",
      "Epoch 109 | Mean Loss 5.136930465698242\n",
      "Epoch 110 | Mean Loss 5.065485715866089\n",
      "Epoch 111 | Mean Loss 5.120239496231079\n",
      "Epoch 112 | Mean Loss 5.101009702682495\n",
      "Epoch 113 | Mean Loss 5.05359001159668\n",
      "Epoch 114 | Mean Loss 5.076394510269165\n",
      "Epoch 115 | Mean Loss 5.052104520797729\n",
      "Epoch 116 | Mean Loss 5.1152183532714846\n",
      "Epoch 117 | Mean Loss 5.0729468822479244\n",
      "Epoch 118 | Mean Loss 5.105400276184082\n",
      "Epoch 119 | Mean Loss 5.11380672454834\n",
      "Epoch 120 | Mean Loss 5.075859403610229\n",
      "Epoch 121 | Mean Loss 5.103553390502929\n",
      "Epoch 122 | Mean Loss 5.109814739227295\n",
      "Epoch 123 | Mean Loss 5.058514595031738\n",
      "Epoch 124 | Mean Loss 5.042933225631714\n",
      "Epoch 125 | Mean Loss 5.098037004470825\n",
      "Epoch 126 | Mean Loss 5.052261257171631\n",
      "Epoch 127 | Mean Loss 5.059518957138062\n",
      "Epoch 128 | Mean Loss 5.088401174545288\n",
      "Epoch 129 | Mean Loss 5.087032794952393\n",
      "Epoch 130 | Mean Loss 5.097438526153565\n",
      "Epoch 131 | Mean Loss 5.087891149520874\n",
      "Epoch 132 | Mean Loss 5.068062400817871\n",
      "Epoch 133 | Mean Loss 5.106884956359863\n",
      "Epoch 134 | Mean Loss 5.143810224533081\n",
      "Epoch 135 | Mean Loss 5.052289533615112\n",
      "Epoch 136 | Mean Loss 5.115612268447876\n",
      "Epoch 137 | Mean Loss 5.1061910629272464\n",
      "Epoch 138 | Mean Loss 5.04897780418396\n",
      "Epoch 139 | Mean Loss 5.143630313873291\n",
      "Epoch 140 | Mean Loss 5.107522773742676\n",
      "Epoch 141 | Mean Loss 5.080289220809936\n",
      "Epoch 142 | Mean Loss 5.092882966995239\n",
      "Epoch 143 | Mean Loss 5.026574563980103\n",
      "Epoch 144 | Mean Loss 5.0816590785980225\n",
      "Epoch 145 | Mean Loss 5.06634292602539\n",
      "Epoch 146 | Mean Loss 5.146637725830078\n",
      "Epoch 147 | Mean Loss 5.141680097579956\n",
      "Epoch 148 | Mean Loss 5.066340827941895\n",
      "Epoch 149 | Mean Loss 5.067420196533203\n",
      "Epoch 150 | Mean Loss 5.096422576904297\n",
      "Epoch 151 | Mean Loss 5.0601881980896\n",
      "Epoch 152 | Mean Loss 5.113525867462158\n",
      "Epoch 153 | Mean Loss 5.06974687576294\n",
      "Epoch 154 | Mean Loss 5.059264707565307\n",
      "Epoch 155 | Mean Loss 5.071429920196533\n",
      "Epoch 156 | Mean Loss 5.072606801986694\n",
      "Epoch 157 | Mean Loss 5.069974565505982\n",
      "Epoch 158 | Mean Loss 5.056121110916138\n",
      "Epoch 159 | Mean Loss 5.10676794052124\n",
      "Epoch 160 | Mean Loss 5.10494818687439\n",
      "Epoch 161 | Mean Loss 5.05855975151062\n",
      "Epoch 162 | Mean Loss 5.042768955230713\n",
      "Epoch 163 | Mean Loss 5.138413286209106\n",
      "Epoch 164 | Mean Loss 5.1509558200836185\n",
      "Epoch 165 | Mean Loss 5.013661336898804\n",
      "Epoch 166 | Mean Loss 5.08655948638916\n",
      "Epoch 167 | Mean Loss 5.056109619140625\n",
      "Epoch 168 | Mean Loss 5.060759544372559\n",
      "Epoch 169 | Mean Loss 5.072999668121338\n",
      "Epoch 170 | Mean Loss 5.064026880264282\n",
      "Epoch 171 | Mean Loss 5.097761964797973\n",
      "Epoch 172 | Mean Loss 5.050675296783448\n",
      "Epoch 173 | Mean Loss 5.126067876815796\n",
      "Epoch 174 | Mean Loss 5.150299453735352\n",
      "Epoch 175 | Mean Loss 5.1177978515625\n",
      "Epoch 176 | Mean Loss 5.105583715438843\n",
      "Epoch 177 | Mean Loss 5.0887962818145756\n",
      "Epoch 178 | Mean Loss 5.081454467773438\n",
      "Epoch 179 | Mean Loss 5.1153148174285885\n",
      "Epoch 180 | Mean Loss 5.16154465675354\n",
      "Epoch 181 | Mean Loss 5.057136487960816\n",
      "Epoch 182 | Mean Loss 5.121069145202637\n",
      "Epoch 183 | Mean Loss 5.071831130981446\n",
      "Epoch 184 | Mean Loss 5.048206186294555\n",
      "Epoch 185 | Mean Loss 5.1174867153167725\n",
      "Epoch 186 | Mean Loss 5.069396448135376\n",
      "Epoch 187 | Mean Loss 5.0614248752594\n",
      "Epoch 188 | Mean Loss 5.0899101257324215\n",
      "Epoch 189 | Mean Loss 5.072478294372559\n",
      "Epoch 190 | Mean Loss 5.061345386505127\n",
      "Epoch 191 | Mean Loss 5.087455415725708\n",
      "Epoch 192 | Mean Loss 5.068904066085816\n",
      "Epoch 193 | Mean Loss 5.073888683319092\n",
      "Epoch 194 | Mean Loss 5.036002206802368\n",
      "Epoch 195 | Mean Loss 5.080448484420776\n",
      "Epoch 196 | Mean Loss 5.056641530990601\n",
      "Epoch 197 | Mean Loss 5.084046125411987\n",
      "Epoch 198 | Mean Loss 5.107343339920044\n",
      "Epoch 199 | Mean Loss 5.128579759597779\n",
      "Epoch 200 | Mean Loss 5.04560546875\n",
      "Epoch 201 | Mean Loss 5.0516551494598385\n",
      "Epoch 202 | Mean Loss 5.109159660339356\n",
      "Epoch 203 | Mean Loss 5.0775422096252445\n",
      "Epoch 204 | Mean Loss 5.111068344116211\n",
      "Epoch 205 | Mean Loss 5.1242584705352785\n",
      "Epoch 206 | Mean Loss 5.078505563735962\n",
      "Epoch 207 | Mean Loss 5.160143947601318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 208 | Mean Loss 5.042608833312988\n",
      "Epoch 209 | Mean Loss 5.089464807510376\n",
      "Epoch 210 | Mean Loss 5.060083723068237\n",
      "Epoch 211 | Mean Loss 5.0359166145324705\n",
      "Epoch 212 | Mean Loss 5.06659083366394\n",
      "Epoch 213 | Mean Loss 5.100061655044556\n",
      "Epoch 214 | Mean Loss 5.102928638458252\n",
      "Epoch 215 | Mean Loss 5.0709113597869875\n",
      "Epoch 216 | Mean Loss 5.054097270965576\n",
      "Epoch 217 | Mean Loss 5.07418303489685\n",
      "Epoch 218 | Mean Loss 5.076726341247559\n",
      "Epoch 219 | Mean Loss 5.055976057052613\n",
      "Epoch 220 | Mean Loss 5.032170963287354\n",
      "Epoch 221 | Mean Loss 5.077888298034668\n",
      "Epoch 222 | Mean Loss 5.0819660186767575\n",
      "Epoch 223 | Mean Loss 5.078001880645752\n",
      "Epoch 224 | Mean Loss 5.10282940864563\n",
      "Epoch 225 | Mean Loss 5.0788350105285645\n",
      "Epoch 226 | Mean Loss 5.082896375656128\n",
      "Epoch 227 | Mean Loss 5.092334461212158\n",
      "Epoch 228 | Mean Loss 5.063675737380981\n",
      "Epoch 229 | Mean Loss 5.079302978515625\n",
      "Epoch 230 | Mean Loss 5.152772712707519\n",
      "Epoch 231 | Mean Loss 5.089501953125\n",
      "Epoch 232 | Mean Loss 5.050706481933593\n",
      "Epoch 233 | Mean Loss 5.067931699752807\n",
      "Epoch 234 | Mean Loss 5.109429407119751\n",
      "Epoch 235 | Mean Loss 5.06136302947998\n",
      "Epoch 236 | Mean Loss 5.0738341331481935\n",
      "Epoch 237 | Mean Loss 5.077195930480957\n",
      "Epoch 238 | Mean Loss 5.09662013053894\n",
      "Epoch 239 | Mean Loss 5.095576333999634\n",
      "Epoch 240 | Mean Loss 5.102524089813232\n",
      "Epoch 241 | Mean Loss 5.166043138504028\n",
      "Epoch 242 | Mean Loss 5.084732580184936\n",
      "Epoch 243 | Mean Loss 5.157538509368896\n",
      "Epoch 244 | Mean Loss 5.12043867111206\n",
      "Epoch 245 | Mean Loss 5.134956979751587\n",
      "Epoch 246 | Mean Loss 5.107555532455445\n",
      "Epoch 247 | Mean Loss 5.05700364112854\n",
      "Epoch 248 | Mean Loss 5.091198968887329\n",
      "Epoch 249 | Mean Loss 5.066926765441894\n",
      "Epoch 250 | Mean Loss 5.068442535400391\n",
      "Epoch 251 | Mean Loss 5.09448618888855\n",
      "Epoch 252 | Mean Loss 5.122210597991943\n",
      "Epoch 253 | Mean Loss 5.097329711914062\n",
      "Epoch 254 | Mean Loss 5.10974817276001\n",
      "Epoch 255 | Mean Loss 5.080395793914795\n",
      "Epoch 256 | Mean Loss 5.0758514404296875\n",
      "Epoch 257 | Mean Loss 5.043723011016846\n",
      "Epoch 258 | Mean Loss 5.113333225250244\n",
      "Epoch 259 | Mean Loss 5.120897674560547\n",
      "Epoch 260 | Mean Loss 5.072770690917968\n",
      "Epoch 261 | Mean Loss 5.116309690475464\n",
      "Epoch 262 | Mean Loss 5.0641967296600345\n",
      "Epoch 263 | Mean Loss 5.084217500686646\n",
      "Epoch 264 | Mean Loss 5.038431167602539\n",
      "Epoch 265 | Mean Loss 5.060476684570313\n",
      "Epoch 266 | Mean Loss 5.090884971618652\n",
      "Epoch 267 | Mean Loss 5.081736946105957\n",
      "Epoch 268 | Mean Loss 5.098278665542603\n",
      "Epoch 269 | Mean Loss 5.100394344329834\n",
      "Epoch 270 | Mean Loss 5.121987962722779\n",
      "Epoch 271 | Mean Loss 5.181944465637207\n",
      "Epoch 272 | Mean Loss 5.150595474243164\n",
      "Epoch 273 | Mean Loss 5.071270322799682\n",
      "Epoch 274 | Mean Loss 5.119491004943848\n",
      "Epoch 275 | Mean Loss 5.055572557449341\n",
      "Epoch 276 | Mean Loss 5.198105096817017\n",
      "Epoch 277 | Mean Loss 5.022878837585449\n",
      "Epoch 278 | Mean Loss 5.120935726165771\n",
      "Epoch 279 | Mean Loss 5.097812557220459\n",
      "Epoch 280 | Mean Loss 5.1119140625\n",
      "Epoch 281 | Mean Loss 5.089358139038086\n",
      "Epoch 282 | Mean Loss 5.066110038757325\n",
      "Epoch 283 | Mean Loss 5.150768184661866\n",
      "Epoch 284 | Mean Loss 5.045116758346557\n",
      "Epoch 285 | Mean Loss 5.065252065658569\n",
      "Epoch 286 | Mean Loss 5.087194490432739\n",
      "Epoch 287 | Mean Loss 5.110408210754395\n",
      "Epoch 288 | Mean Loss 5.138854169845581\n",
      "Epoch 289 | Mean Loss 5.088751935958863\n",
      "Epoch 290 | Mean Loss 5.1191887855529785\n",
      "Epoch 291 | Mean Loss 5.166073226928711\n",
      "Epoch 292 | Mean Loss 5.056309175491333\n",
      "Epoch 293 | Mean Loss 5.0966897964477536\n",
      "Epoch 294 | Mean Loss 5.079460668563843\n",
      "Epoch 295 | Mean Loss 5.057161426544189\n",
      "Epoch 296 | Mean Loss 5.058696031570435\n",
      "Epoch 297 | Mean Loss 5.05173659324646\n",
      "Epoch 298 | Mean Loss 5.028782892227173\n",
      "Epoch 299 | Mean Loss 5.044957447052002\n",
      "Epoch 300 | Mean Loss 5.054666328430176\n",
      "Epoch 301 | Mean Loss 5.04745888710022\n",
      "Epoch 302 | Mean Loss 5.139901542663575\n",
      "Epoch 303 | Mean Loss 5.0873250484466555\n",
      "Epoch 304 | Mean Loss 5.058742332458496\n",
      "Epoch 305 | Mean Loss 5.11916036605835\n",
      "Epoch 306 | Mean Loss 5.171126508712769\n",
      "Epoch 307 | Mean Loss 5.124804830551147\n",
      "Epoch 308 | Mean Loss 5.127845811843872\n",
      "Epoch 309 | Mean Loss 5.119486474990845\n",
      "Epoch 310 | Mean Loss 5.07111873626709\n",
      "Epoch 311 | Mean Loss 5.07043490409851\n",
      "Epoch 312 | Mean Loss 5.049756574630737\n",
      "Epoch 313 | Mean Loss 5.075760269165039\n",
      "Epoch 314 | Mean Loss 5.097641134262085\n",
      "Epoch 315 | Mean Loss 5.035954761505127\n",
      "Epoch 316 | Mean Loss 5.054873895645142\n",
      "Epoch 317 | Mean Loss 5.072160053253174\n",
      "Epoch 318 | Mean Loss 5.0334789752960205\n",
      "Epoch 319 | Mean Loss 5.059849834442138\n",
      "Epoch 320 | Mean Loss 5.048392391204834\n",
      "Epoch 321 | Mean Loss 5.116069602966308\n",
      "Epoch 322 | Mean Loss 5.092081594467163\n",
      "Epoch 323 | Mean Loss 5.107346439361573\n",
      "Epoch 324 | Mean Loss 5.073619556427002\n",
      "Epoch 325 | Mean Loss 5.04665355682373\n",
      "Epoch 326 | Mean Loss 5.095759773254395\n",
      "Epoch 327 | Mean Loss 5.054970836639404\n",
      "Epoch 328 | Mean Loss 5.07622799873352\n",
      "Epoch 329 | Mean Loss 5.100194692611694\n",
      "Epoch 330 | Mean Loss 5.0935088157653805\n",
      "Epoch 331 | Mean Loss 5.0894452095031735\n",
      "Epoch 332 | Mean Loss 5.0678917407989506\n",
      "Epoch 333 | Mean Loss 5.077432775497437\n",
      "Epoch 334 | Mean Loss 5.086490058898926\n",
      "Epoch 335 | Mean Loss 5.075891780853271\n",
      "Epoch 336 | Mean Loss 5.076038217544555\n",
      "Epoch 337 | Mean Loss 5.105804443359375\n",
      "Epoch 338 | Mean Loss 5.036549234390259\n",
      "Epoch 339 | Mean Loss 5.066376638412476\n",
      "Epoch 340 | Mean Loss 5.076802921295166\n",
      "Epoch 341 | Mean Loss 5.08079161643982\n",
      "Epoch 342 | Mean Loss 5.104663467407226\n",
      "Epoch 343 | Mean Loss 5.102300071716309\n",
      "Epoch 344 | Mean Loss 5.07504506111145\n",
      "Epoch 345 | Mean Loss 5.138310050964355\n",
      "Epoch 346 | Mean Loss 5.041102743148803\n",
      "Epoch 347 | Mean Loss 5.063880443572998\n",
      "Epoch 348 | Mean Loss 5.145499515533447\n",
      "Epoch 349 | Mean Loss 5.0396811962127686\n",
      "Epoch 350 | Mean Loss 5.093306207656861\n",
      "Epoch 351 | Mean Loss 5.0250246047973635\n",
      "Epoch 352 | Mean Loss 5.0526453971862795\n",
      "Epoch 353 | Mean Loss 5.0642444610595705\n",
      "Epoch 354 | Mean Loss 5.071082353591919\n",
      "Epoch 355 | Mean Loss 5.1202489852905275\n",
      "Epoch 356 | Mean Loss 5.073330068588257\n",
      "Epoch 357 | Mean Loss 5.087583684921265\n",
      "Epoch 358 | Mean Loss 5.125952053070068\n",
      "Epoch 359 | Mean Loss 5.05594162940979\n",
      "Epoch 360 | Mean Loss 5.069041967391968\n",
      "Epoch 361 | Mean Loss 5.051281452178955\n",
      "Epoch 362 | Mean Loss 5.102067375183106\n",
      "Epoch 363 | Mean Loss 5.116560745239258\n",
      "Epoch 364 | Mean Loss 5.056176996231079\n",
      "Epoch 365 | Mean Loss 5.0940076351165775\n",
      "Epoch 366 | Mean Loss 5.045258808135986\n",
      "Epoch 367 | Mean Loss 5.136140918731689\n",
      "Epoch 368 | Mean Loss 5.126203155517578\n",
      "Epoch 369 | Mean Loss 5.053218698501587\n",
      "Epoch 370 | Mean Loss 5.056907033920288\n",
      "Epoch 371 | Mean Loss 5.051876735687256\n",
      "Epoch 372 | Mean Loss 5.1544252872467045\n",
      "Epoch 373 | Mean Loss 5.037004470825195\n",
      "Epoch 374 | Mean Loss 5.100510740280152\n",
      "Epoch 375 | Mean Loss 5.064977025985717\n",
      "Epoch 376 | Mean Loss 5.093647813796997\n",
      "Epoch 377 | Mean Loss 5.083244657516479\n",
      "Epoch 378 | Mean Loss 5.115407466888428\n",
      "Epoch 379 | Mean Loss 5.149029874801636\n",
      "Epoch 380 | Mean Loss 5.056379270553589\n",
      "Epoch 381 | Mean Loss 5.071370363235474\n",
      "Epoch 382 | Mean Loss 5.053470945358276\n",
      "Epoch 383 | Mean Loss 5.135174989700317\n",
      "Epoch 384 | Mean Loss 5.059877252578735\n",
      "Epoch 385 | Mean Loss 5.084058380126953\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m n_feat \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     18\u001b[0m features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m n_views, n_feat) \u001b[38;5;66;03m# [bsz*2, 499500]\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m features, targets \u001b[38;5;241m=\u001b[39m \u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, targets\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# [bsz, 2, 499500], [bsz, 1]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     22\u001b[0m out_feat, out_target \u001b[38;5;241m=\u001b[39m model(features, torch\u001b[38;5;241m.\u001b[39mcat(n_views\u001b[38;5;241m*\u001b[39m[targets], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;66;03m# ([bsz*5, 1], [bsz*5, 1])\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "model = MLP(input_dim_feat, input_dim_target, hidden_dim_feat, output_dim, dropout_rate).to(device)\n",
    "criterion_pft = KernelizedSupCon(method='expw', temperature=0.07, base_temperature=0.07, kernel=cauchy, krnl_sigma = 1)\n",
    "criterion_ptt = KernelizedSupCon(method='expw', temperature=0.07, base_temperature=0.07, kernel=cauchy, krnl_sigma = 1)\n",
    "\n",
    "# criterion = CustomKernelizedSupCon(temperature = temperature, base_temperature = base_temperature, kernel = kernel)\n",
    "# criterion = CustomSupCon('exp',temperature = temperature, base_temperature = base_temperature, kernel = kernel)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    batch_losses = []\n",
    "    for batch_num, (features, targets) in enumerate(std_train_loader):\n",
    "        bsz = targets.shape[0]\n",
    "        n_views = features.shape[1]\n",
    "        n_feat = features.shape[-1]\n",
    "        \n",
    "        features = features.view(bsz * n_views, n_feat) # [bsz*2, 499500]\n",
    "        features, targets = features.to(device), targets.to(device) # [bsz, 2, 499500], [bsz, 1]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out_feat, out_target = model(features, torch.cat(n_views*[targets], dim=0)) # ([bsz*5, 1], [bsz*5, 1])\n",
    "        \n",
    "        out_feat = torch.split(out_feat, [bsz]*n_views, dim=0)\n",
    "        out_feat = torch.cat([f.unsqueeze(1) for f in out_feat], dim=1) # [bsz, 5, 2]\n",
    "        \n",
    "        loss = criterion_pft(out_feat, targets) # ([bsz, 5, 2], [bsz, 1])\n",
    "        \n",
    "        out_target = torch.split(out_target, [bsz]*n_views, dim=0)\n",
    "        out_target = torch.cat([f.unsqueeze(1) for f in out_target], dim=1) # [bsz, 2, 2]\n",
    "        \n",
    "        loss += criterion_ptt(out_target, targets) # ([bsz, 5, 2], [bsz, 1])\n",
    "        loss += torch.nn.functional.mse_loss(out_feat.view(bsz * n_views, 2), out_target.view(bsz * n_views, 2)) # mse_loss([bsz*2, 2], [bsz*2, 2])\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        batch_losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch} | Mean Loss {sum(batch_losses)/len(batch_losses)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training target estimator\n",
      "Training target estimator\n",
      "1.3394337 3.079791734417192e-06\n"
     ]
    }
   ],
   "source": [
    "mape_train, _ = compute_target_score(model, std_train_loader, std_test_loader, device, 'mape')\n",
    "r2_train, _ = compute_target_score(model, std_train_loader, std_test_loader, device, 'r2')\n",
    "# results_cv.append(['Overall', mape_train, r2_train, mape_val, r2_val])\n",
    "print(mape_train, r2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = pd.DataFrame(results_cv, columns=['Fold', 'Train_MAPE', 'Train_R2', 'Val_MAPE', 'Val_R2'])\n",
    "# results_df.to_csv('cv_results_hopkins.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "# features = torch.vstack([test_dataset[i][0] for i in range(len(test_loader))])\n",
    "# targets = torch.vstack([test_dataset[i][1] for i in range(len(test_loader))])\n",
    "# features_mean, features_std, targets_mean, targets_std = compute_global_stats(test_dataset)\n",
    "# standardized_features = (features - features_mean) / features_std\n",
    "# standardized_targets = (targets - targets_mean) / targets_std\n",
    "# standardized_test_dataset = TensorDataset(standardized_features, standardized_targets)\n",
    "# test_loader = DataLoader(standardized_test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Loss:   1.37\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_losses = []\n",
    "emb_features = [] # saving the embedded features for each batch\n",
    "emb_targets = []\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    for batch_num, (features, targets) in enumerate(std_test_loader):\n",
    "        bsz = targets.shape[0]\n",
    "        n_views = 1\n",
    "        n_feat = features.shape[-1]\n",
    "        \n",
    "        if len(features.shape) > 2:\n",
    "            n_views = features.shape[1]\n",
    "            features = features.view(bsz * n_views, n_feat) # [bsz*2, 499500]\n",
    "        features, targets = features.to(device), targets.to(device) # [bsz, 2, 499500], [bsz, 1]\n",
    "\n",
    "        out_feat, out_target = model(features, torch.cat(n_views*[targets], dim=0))   \n",
    "        \n",
    "        out_feat = torch.split(out_feat, [bsz]*n_views, dim=0)\n",
    "        out_feat = torch.cat([f.unsqueeze(1) for f in out_feat], dim=1) # [bsz, 5, 2]\n",
    "        \n",
    "        loss = criterion_pft(out_feat, targets) # ([bsz, 5, 2], [bsz, 1])\n",
    "        \n",
    "        out_target = torch.split(out_target, [bsz]*n_views, dim=0)\n",
    "        out_target = torch.cat([f.unsqueeze(1) for f in out_target], dim=1) # [bsz, 2, 2]\n",
    "        \n",
    "        loss += criterion_ptt(out_target, targets) # ([bsz, 5, 2], [bsz, 1])\n",
    "        loss += torch.nn.functional.mse_loss(out_feat.view(bsz * n_views, 2), out_target.view(bsz * n_views, 2)) # mse_loss([bsz*2, 2], [bsz*2, 2])\n",
    "        \n",
    "        emb_features.append(out_feat[:, 0, :])\n",
    "        emb_targets.append(out_target[:, 0, :])\n",
    "        \n",
    "        test_losses.append(loss.item())\n",
    "        total_loss += loss.item() * features.size(0)\n",
    "        total_samples += features.size(0)\n",
    "        \n",
    "    test_losses =np.array(test_losses)\n",
    "    average_loss = total_loss / total_samples\n",
    "    print('Mean Test Loss: %6.2f' % (average_loss))\n",
    "    #np.save(f\"losses/test_losses_batch{batch_num}.npy\", test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_features = torch.row_stack(emb_features).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_targets = torch.row_stack(emb_targets).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_features = pd.DataFrame(emb_features,columns = [\"Dim_1\", \"Dim_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_targets = pd.DataFrame(emb_targets,columns = [\"Dim_1\", \"Dim_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_features[\"sub\"] = np.arange(1, len(emb_features) +1)\n",
    "emb_targets[\"sub\"] = np.arange(1, len(emb_targets) +1)\n",
    "emb_features[\"Type\"] = 'Features'\n",
    "emb_targets[\"Type\"] = 'Targets'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs3/well/margulies/users/cpy397/.local/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/gpfs3/well/margulies/users/cpy397/.local/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/gpfs3/well/margulies/users/cpy397/.local/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/gpfs3/well/margulies/users/cpy397/.local/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/gpfs3/well/margulies/users/cpy397/.local/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/gpfs3/well/margulies/users/cpy397/.local/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc87e0dd400>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# time to cry\n",
    "# I probably messed up the original loss. Went over it multiple times\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "sns.scatterplot(emb_features, x = 'Dim_1', y = 'Dim_2', s = 100, alpha = 0.8, hue = 'sub', palette = 'nipy_spectral', label='Features')\n",
    "sns.scatterplot(emb_targets, x = 'Dim_1', marker = 'v', y = 'Dim_2', s = 100, alpha = 0.8, hue = 'sub', palette = 'nipy_spectral', label='Targets')\n",
    "plt.xlim(-1.2, 1.2)\n",
    "plt.ylim(-1.2, 1.2)\n",
    "\n",
    "feature_handle = mlines.Line2D([], [], color='black', marker='o', linestyle='None', markersize=10, label='Features')\n",
    "target_handle = mlines.Line2D([], [], color='black', marker='v', linestyle='None', markersize=10, label='Targets')\n",
    "\n",
    "plt.legend(handles=[feature_handle, target_handle])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
