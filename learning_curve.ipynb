{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99de795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from nilearn.connectome import sym_matrix_to_vec\n",
    "from scipy.stats import pearsonr\n",
    "from cmath import isinf\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import math\n",
    "from cmath import isinf\n",
    "from sklearn.model_selection import train_test_split, KFold, LearningCurveDisplay, learning_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_percentage_error, r2_score\n",
    "import multiprocessing\n",
    "from skorch import NeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d147112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f7d8635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader_to_numpy(loader):\n",
    "    features, targets = [], []\n",
    "    for feat, targ in loader:\n",
    "        features.append(feat.numpy())\n",
    "        targets.append(targ.numpy())\n",
    "    return np.concatenate(features), np.concatenate(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5aa9e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim_feat = 499500, input_dim_target = 1, hidden_dim_feat = 1000, output_dim = 2, dropout_rate = 0):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # Xavier initialization for feature MLP\n",
    "        self.feat_mlp = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_dim_feat),\n",
    "            nn.Linear(input_dim_feat, hidden_dim_feat),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(hidden_dim_feat, output_dim)\n",
    "        )\n",
    "        self.init_weights(self.feat_mlp)\n",
    "\n",
    "        # Xavier initialization for target MLP\n",
    "        self.target_mlp = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_dim_target),\n",
    "            nn.Linear(input_dim_target, output_dim)\n",
    "        )\n",
    "        self.init_weights(self.target_mlp)\n",
    "        \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        features = self.feat_mlp(x)\n",
    "        targets = self.target_mlp(y)\n",
    "        features = nn.functional.normalize(features, p=2, dim=1)\n",
    "        targets = nn.functional.normalize(targets, p=2, dim=1)\n",
    "        return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93673abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgeEstimator(BaseEstimator):\n",
    "    \"\"\" Define the age estimator on latent space network features.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        n_jobs = multiprocessing.cpu_count()\n",
    "        self.age_estimator = GridSearchCV(\n",
    "            Ridge(), param_grid={\"alpha\": 10.**np.arange(-2, 3)}, cv=5,\n",
    "            scoring=\"r2\", n_jobs=n_jobs)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.age_estimator.fit(X, y)\n",
    "        return self.score(X, y), self.r2(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = self.age_estimator.predict(X)\n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.age_estimator.predict(X)\n",
    "        return mean_absolute_percentage_error(y, y_pred)\n",
    "    \n",
    "    def r2(self, X, y):\n",
    "        y_pred = self.age_estimator.predict(X)\n",
    "        return r2_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5caa4aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatData(Dataset):\n",
    "    def __init__(self, path_feat, path_target, target_name, transform = None, train=True, train_size = 0.8, test_size=None, test_site = None, regions = None, threshold_mat = False, threshold_percent = None, random_state=42):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with the capability to handle training and testing splits, \n",
    "        including multiple views for augmented data.\n",
    "        \n",
    "        Args:\n",
    "            path_feat (str): Path to the features file.\n",
    "            path_target (str): Path to the target file.\n",
    "            transform (callable): A transformation function to apply for augmentation.\n",
    "            train (bool): Whether the dataset is used for training. False will load the test set.\n",
    "            test_size (float): Proportion of the dataset to include in the test split.\n",
    "            random_state (int): Random state for reproducible train-test splits.\n",
    "        \"\"\"\n",
    "        # Load the entire dataset\n",
    "        features = np.load(path_feat)      \n",
    "        participant_data = pd.read_csv(path_target)\n",
    "        targets = np.expand_dims(participant_data[target_name].values, axis = 1)\n",
    "        \n",
    "\n",
    "        # Split the dataset into training and test sets\n",
    "        if test_site is None:\n",
    "            train_indices, test_indices = train_test_split(np.arange(len(features)), \n",
    "                                                       train_size = train_size,\n",
    "                                                       test_size=test_size,                \n",
    "                                                       random_state=random_state)\n",
    "        else:\n",
    "            test_indices = participant_data.index[participant_data['dataset'] == test_site].values\n",
    "            train_indices = np.delete(np.arange(len(features)), test_indices)\n",
    "        \n",
    "        if train:\n",
    "            selected_indices = train_indices\n",
    "        else:\n",
    "            selected_indices = test_indices\n",
    "        \n",
    "        # Select the subset of data for the current mode (train/test)\n",
    "        features = features[selected_indices]\n",
    "        if threshold_mat:\n",
    "            thresholded_feat = []\n",
    "            for matrix in features:\n",
    "                threshold = np.percentile(matrix, threshold_percent)\n",
    "                matrix[matrix < threshold] = 0\n",
    "                thresholded_feat.append(matrix)\n",
    "            threshold_feat = np.stack(thresholded_feat)\n",
    "            features = threshold_feat\n",
    "        targets = targets[selected_indices]\n",
    "        \n",
    "\n",
    "        self.n_sub = len(features)\n",
    "        self.n_views = 1\n",
    "        self.transform = transform\n",
    "        self.targets = targets\n",
    "        \n",
    "        vectorized_feat = np.array([sym_matrix_to_vec(mat, discard_diagonal=True) for mat in features])\n",
    "        self.n_features = vectorized_feat.shape[-1]\n",
    "        \n",
    "        if (train and transform is not None):\n",
    "            # augmentation only in training mode\n",
    "            if transform != \"copy\":\n",
    "                augmented_features = np.array([self.transform(sample, regions = regions) for sample in features])\n",
    "\n",
    "                self.n_views = self.n_views + augmented_features.shape[1]\n",
    "                self.features = np.zeros((self.n_sub, self.n_views, self.n_features))\n",
    "                for sub in range(self.n_sub):\n",
    "                    self.features[sub, 0, :] = vectorized_feat[sub]\n",
    "                    self.features[sub, 1:, :] = augmented_features[sub]\n",
    "            else:\n",
    "                self.features = np.repeat(np.expand_dims(vectorized_feat, axis = 1), 2, axis=1)\n",
    "        else:\n",
    "            self.features = np.expand_dims(vectorized_feat, axis = 1)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.n_sub\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.features[idx]\n",
    "        targets = self.targets[idx]\n",
    "        features = torch.from_numpy(features).float()\n",
    "        targets = torch.from_numpy(targets).float()\n",
    "        \n",
    "        return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f506c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNet(NeuralNet):\n",
    "    def __init__(self, module, train_data, estimator, criterion_pft=None, criterion_ptt=None, n_views = None, **kwargs):\n",
    "        super(CustomNet, self).__init__(module, **kwargs)\n",
    "        self.criterion_pft_cls = criterion_pft\n",
    "        self.criterion_ptt_cls = criterion_ptt\n",
    "        self.n_views = n_views\n",
    "        slef.estimator = estimator\n",
    "        self.train_data = train_data\n",
    "\n",
    "    def initialize_criterion(self):\n",
    "        super().initialize_criterion()  # Initialize default criterion first\n",
    "        # Setup custom criterion with parameters from get_params_for\n",
    "        if self.criterion_pft_cls is not None:\n",
    "            self.criterion_pft_ = self.criterion_pft_cls(**self.get_params_for('criterion_pft'))\n",
    "        if self.criterion_ptt_cls is not None:\n",
    "            self.criterion_ptt_ = self.criterion_ptt_cls(**self.get_params_for('criterion_ptt'))\n",
    "        return self\n",
    "    \n",
    "    def train_step_single(self, batch, **fit_params):\n",
    "        self.module_.train()\n",
    "        features, targets = batch\n",
    "        features, targets = features.to(self.device), targets.to(self.device)\n",
    "        self.optimizer_.zero_grad()\n",
    "        mlp_out = self.module_(features, targets)  # Pass both Xi and yi to forward\n",
    "        loss = self.get_loss(mlp_out, targets, training=True)\n",
    "        loss.backward()\n",
    "        self.optimizer_.step()\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def validation_step(self, batch, **fit_params):\n",
    "        self.module_.eval()  # Set the module in evaluation mode\n",
    "        features, targets = batch\n",
    "        with torch.no_grad():\n",
    "            features, targets = features.to(self.device), targets.to(self.device)\n",
    "            out_feat, out_target = self.module_(features, targets)\n",
    "        \n",
    "        loss = self.get_loss((out_feat, out_target), targets, training=False)\n",
    "        \n",
    "        out_feat_train, _ = self.module_(self.train_data)\n",
    "        self.estimator = self.estimator.fit(out_feat_train)\n",
    "        y_pred = self.estimator.pred(out_feat)\n",
    "        return {'loss': loss, 'y_pred': y_pred}\n",
    "\n",
    "    def get_loss(self, mlp_out, targets, training=False):\n",
    "        n_views = self.n_views\n",
    "        out_feat, out_target = mlp_out\n",
    "        bsz = out_feat.shape[0]\n",
    "        out_feat = torch.split(out_feat, [bsz]*n_views, dim=0)\n",
    "        out_feat = torch.cat([f.unsqueeze(1) for f in out_feat], dim=1)\n",
    "        \n",
    "        loss = self.criterion_pft_(out_feat, targets)\n",
    "        out_target = torch.split(out_target, [bsz]*n_views, dim=0)\n",
    "        out_target = torch.cat([f.unsqueeze(1) for f in out_target], dim=1)\n",
    "        \n",
    "        loss += self.criterion_ptt_(out_target, targets)\n",
    "        loss += torch.nn.functional.mse_loss(out_feat.view(bsz * n_views, 2), out_target.view(bsz * n_views, 2))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aeeb48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x, krnl_sigma):\n",
    "    x = x - x.T\n",
    "    return torch.exp(-(x**2) / (2*(krnl_sigma**2))) / (math.sqrt(2*torch.pi)*krnl_sigma)\n",
    "\n",
    "def cauchy(x, krnl_sigma):\n",
    "        x = x - x.T\n",
    "        return  1. / (krnl_sigma*(x**2) + 1)\n",
    "\n",
    "def rbf(x, krnl_sigma):\n",
    "        x = x - x.T\n",
    "        return torch.exp(-(x**2)/(2*(krnl_sigma**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f739858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss from: https://github.com/EIDOSLAB/contrastive-brain-age-prediction/blob/master/src/losses.py\n",
    "# modified to accept input shape [bsz, n_feats]. In the age paper: [bsz, n_views, n_feats].\n",
    "class KernelizedSupCon(nn.Module):\n",
    "    \"\"\"Supervised contrastive loss: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\n",
    "    Based on: https://github.com/HobbitLong/SupContrast\"\"\"\n",
    "    def __init__(self, method: str='expw', temperature: float=0.03, contrast_mode: str='all',\n",
    "                 base_temperature: float=0.07, krnl_sigma: float = 1., kernel: callable=cauchy, delta_reduction: str='sum'):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "        self.method = method\n",
    "        self.kernel = kernel\n",
    "        self.krnl_sigma = krnl_sigma\n",
    "        self.delta_reduction = delta_reduction\n",
    "\n",
    "        if kernel is not None and method == 'supcon':\n",
    "            raise ValueError('Kernel must be none if method=supcon')\n",
    "        \n",
    "        if kernel is None and method != 'supcon':\n",
    "            raise ValueError('Kernel must not be none if method != supcon')\n",
    "\n",
    "        if delta_reduction not in ['mean', 'sum']:\n",
    "            raise ValueError(f\"Invalid reduction {delta_reduction}\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__} ' \\\n",
    "               f'(t={self.temperature}, ' \\\n",
    "               f'method={self.method}, ' \\\n",
    "               f'kernel={self.kernel is not None}, ' \\\n",
    "               f'delta_reduction={self.delta_reduction})'\n",
    "\n",
    "    def forward(self, features, labels=None):\n",
    "        \"\"\"Compute loss for model. If `labels` is None, \n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, n_features]. \n",
    "                input has to be rearranged to [bsz, n_views, n_features] and labels [bsz],\n",
    "            labels: ground truth of shape [bsz].\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "        device = features.device\n",
    "\n",
    "        if len(features.shape) != 3:\n",
    "            raise ValueError('`features` needs to be [bsz, n_views, n_feats],'\n",
    "                             '3 dimensions are required')\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        contrast_count = features.shape[1]\n",
    "\n",
    "        if labels is None:\n",
    "            mask = torch.eye(batch_size, device=device)\n",
    "        \n",
    "        else:\n",
    "            labels = labels.view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError('Num of labels does not match num of features')\n",
    "            \n",
    "            if self.kernel is None:\n",
    "                mask = torch.eq(labels, labels.T)\n",
    "            else:\n",
    "                mask = self.kernel(labels, krnl_sigma = self.krnl_sigma)     \n",
    "        \n",
    "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == 'one':\n",
    "            anchor_feature = features[:, 0]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == 'all':\n",
    "            anchor_feature = contrast_feature\n",
    "            anchor_count = contrast_count\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
    "\n",
    "        # Tile mask\n",
    "        mask = mask.repeat(anchor_count, contrast_count)\n",
    "        # Inverse of torch-eye to remove self-contrast (diagonal)\n",
    "        inv_diagonal = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size*anchor_count, device=device).view(-1, 1),\n",
    "            0\n",
    "        )\n",
    "        # compute similarity\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            self.temperature\n",
    "        )\n",
    "\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        alignment = logits \n",
    "\n",
    "        # base case is:\n",
    "        # - supcon if kernel = none \n",
    "        # - y-aware is kernel != none\n",
    "        uniformity = torch.exp(logits) * inv_diagonal \n",
    "\n",
    "        if self.method == 'threshold':\n",
    "            repeated = mask.unsqueeze(-1).repeat(1, 1, mask.shape[0]) # repeat kernel mask\n",
    "\n",
    "            delta = (mask[:, None].T - repeated.T).transpose(1, 2) # compute the difference w_k - w_j for every k,j\n",
    "            delta = (delta > 0.).float()\n",
    "\n",
    "            # for each z_i, repel only samples j s.t. K(z_i, z_j) < K(z_i, z_k)\n",
    "            uniformity = uniformity.unsqueeze(-1).repeat(1, 1, mask.shape[0])\n",
    "\n",
    "            if self.delta_reduction == 'mean':\n",
    "                uniformity = (uniformity * delta).mean(-1)\n",
    "            else:\n",
    "                uniformity = (uniformity * delta).sum(-1)\n",
    "    \n",
    "        elif self.method == 'expw':\n",
    "            # exp weight e^(s_j(1-w_j))\n",
    "            uniformity = torch.exp(logits * (1 - mask)) * inv_diagonal\n",
    "\n",
    "        uniformity = torch.log(uniformity.sum(1, keepdim=True))\n",
    "\n",
    "\n",
    "        # positive mask contains the anchor-positive pairs\n",
    "        # excluding <self,self> on the diagonal\n",
    "        positive_mask = mask * inv_diagonal\n",
    "\n",
    "        log_prob = alignment - uniformity # log(alignment/uniformity) = log(alignment) - log(uniformity)\n",
    "        log_prob = (positive_mask * log_prob).sum(1) / positive_mask.sum(1) # compute mean of log-likelihood over positive\n",
    " \n",
    "        # loss\n",
    "        loss = - (self.temperature / self.base_temperature) * log_prob\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfe60d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim_feat = 499500 # vectorized mat, diagonal discarded\n",
    "# the rest is arbitrary\n",
    "hidden_dim_feat = 1000\n",
    "input_dim_target = 1\n",
    "output_dim = 2\n",
    "num_epochs = 100\n",
    "\n",
    "lr = 0.01 # too low values return nan loss\n",
    "kernel = cauchy\n",
    "batch_size = 30 # too low values return nan loss\n",
    "dropout_rate = 0\n",
    "weight_decay = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80f251f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomNet(\n",
    "    module=MLP,\n",
    "    optimizer=optim.Adam,\n",
    "    lr = 0.01,\n",
    "    estimator = \n",
    "    module__input_dim_feat = 499500,\n",
    "    module__hidden_dim_feat = 1000,\n",
    "    module__input_dim_target = 1,\n",
    "    module__output_dim = 2,\n",
    "    n_views = 1,\n",
    "    criterion_pft=KernelizedSupCon,\n",
    "    criterion_ptt=KernelizedSupCon,\n",
    "    criterion=torch.nn.MSELoss(),  # Default criterion, actual loss handled in get_loss\n",
    "    batch_size=30,\n",
    "    max_epochs=100,\n",
    "    device='cuda',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90d9c11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "train_dataset = MatData(\"matrices.npy\", \"participants.csv\", \"age\", train=True, test_size = 0.2)\n",
    "test_dataset = MatData(\"matrices.npy\", \"participants.csv\", \"age\", train=False, test_size = 0.2)\n",
    "dataset = ConcatDataset([train_dataset, test_dataset])\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle = False)\n",
    "features, targets = loader_to_numpy(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5198e0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  16.319875717163086\n",
      "Loss:  4.596426486968994\n",
      "Loss:  3.674405097961426\n",
      "Loss:  3.4993159770965576\n",
      "Loss:  3.43544602394104\n",
      "Loss:  3.053532600402832\n",
      "Loss:  3.0746428966522217\n",
      "Loss:  2.9442903995513916\n",
      "Loss:  2.9357991218566895\n",
      "Loss:  2.9062116146087646\n",
      "Loss:  2.9092061519622803\n",
      "Loss:  2.8639962673187256\n",
      "Loss:  2.8776121139526367\n",
      "Loss:  2.742440700531006\n",
      "Loss:  3.0218026638031006\n",
      "Loss:  2.970330238342285\n",
      "Loss:  2.858966827392578\n",
      "Loss:  2.7810487747192383\n",
      "Loss:  2.8471286296844482\n",
      "Loss:  2.7045159339904785\n",
      "Loss:  2.7584352493286133\n",
      "Loss:  2.6810054779052734\n",
      "Loss:  2.711137294769287\n",
      "Loss:  2.739959955215454\n",
      "Loss:  2.6077935695648193\n",
      "Loss:  2.7372987270355225\n",
      "Loss:  2.8451695442199707\n",
      "Loss:  2.6975162029266357\n",
      "Loss:  2.8641114234924316\n",
      "Loss:  2.7417733669281006\n",
      "Loss:  2.7559664249420166\n",
      "Loss:  1.5878994464874268\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.5431\u001b[0m        \u001b[32m2.7232\u001b[0m  5.7686\n",
      "Loss:  2.832012176513672\n",
      "Loss:  2.8535282611846924\n",
      "Loss:  2.7648205757141113\n",
      "Loss:  2.7097654342651367\n",
      "Loss:  2.7397444248199463\n",
      "Loss:  2.6869988441467285\n",
      "Loss:  2.6921684741973877\n",
      "Loss:  2.670426368713379\n",
      "Loss:  2.6735782623291016\n",
      "Loss:  2.6683573722839355\n",
      "Loss:  2.680540084838867\n",
      "Loss:  2.6734910011291504\n",
      "Loss:  2.681091547012329\n",
      "Loss:  2.6335391998291016\n",
      "Loss:  2.7883548736572266\n",
      "Loss:  2.776571750640869\n",
      "Loss:  2.7078709602355957\n",
      "Loss:  2.6730105876922607\n",
      "Loss:  2.725033760070801\n",
      "Loss:  2.6186130046844482\n",
      "Loss:  2.664029598236084\n",
      "Loss:  2.5970849990844727\n",
      "Loss:  2.6325113773345947\n",
      "Loss:  2.656255006790161\n",
      "Loss:  2.532957077026367\n",
      "Loss:  2.6147470474243164\n",
      "Loss:  2.757596492767334\n",
      "Loss:  2.610686779022217\n",
      "Loss:  2.7594494819641113\n",
      "Loss:  2.6307883262634277\n",
      "Loss:  2.658355712890625\n",
      "Loss:  1.517271637916565\n",
      "      2        \u001b[36m2.6937\u001b[0m        \u001b[32m2.6228\u001b[0m  5.2160\n",
      "Loss:  2.758437156677246\n",
      "Loss:  2.7813773155212402\n",
      "Loss:  2.6947555541992188\n",
      "Loss:  2.6402037143707275\n",
      "Loss:  2.6758530139923096\n",
      "Loss:  2.626802682876587\n",
      "Loss:  2.6330978870391846\n",
      "Loss:  2.613039255142212\n",
      "Loss:  2.6178624629974365\n",
      "Loss:  2.6203560829162598\n",
      "Loss:  2.6342906951904297\n",
      "Loss:  2.628998041152954\n",
      "Loss:  2.6442313194274902\n",
      "Loss:  2.59511137008667\n",
      "Loss:  2.744278907775879\n",
      "Loss:  2.7350447177886963\n",
      "Loss:  2.6726233959198\n",
      "Loss:  2.6393394470214844\n",
      "Loss:  2.6865007877349854\n",
      "Loss:  2.5937654972076416\n",
      "Loss:  2.635737180709839\n",
      "Loss:  2.570855140686035\n",
      "Loss:  2.6129419803619385\n",
      "Loss:  2.629354953765869\n",
      "Loss:  2.5162179470062256\n",
      "Loss:  2.590345859527588\n",
      "Loss:  2.7230453491210938\n",
      "Loss:  2.5889227390289307\n",
      "Loss:  2.7172605991363525\n",
      "Loss:  2.608851194381714\n",
      "Loss:  2.626635789871216\n",
      "Loss:  1.5102782249450684\n",
      "      3        \u001b[36m2.6484\u001b[0m        \u001b[32m2.5943\u001b[0m  5.2752\n",
      "Loss:  2.7277605533599854\n",
      "Loss:  2.74131441116333\n",
      "Loss:  2.664137125015259\n",
      "Loss:  2.6147639751434326\n",
      "Loss:  2.6464321613311768\n",
      "Loss:  2.605928659439087\n",
      "Loss:  2.612846612930298\n",
      "Loss:  2.5949461460113525\n",
      "Loss:  2.595431089401245\n",
      "Loss:  2.60768461227417\n",
      "Loss:  2.6171317100524902\n",
      "Loss:  2.60876727104187\n",
      "Loss:  2.635500192642212\n",
      "Loss:  2.584064245223999\n",
      "Loss:  2.7096102237701416\n",
      "Loss:  2.7031633853912354\n",
      "Loss:  2.6514432430267334\n",
      "Loss:  2.6214728355407715\n",
      "Loss:  2.6619505882263184\n",
      "Loss:  2.5842065811157227\n",
      "Loss:  2.6216001510620117\n",
      "Loss:  2.5598204135894775\n",
      "Loss:  2.6039276123046875\n",
      "Loss:  2.6135478019714355\n",
      "Loss:  2.5104053020477295\n",
      "Loss:  2.5877702236175537\n",
      "Loss:  2.6950345039367676\n",
      "Loss:  2.576641082763672\n",
      "Loss:  2.6863181591033936\n",
      "Loss:  2.601989507675171\n",
      "Loss:  2.6085166931152344\n",
      "Loss:  1.505307912826538\n",
      "      4        \u001b[36m2.6282\u001b[0m        \u001b[32m2.5784\u001b[0m  5.2161\n",
      "Loss:  2.705930233001709\n",
      "Loss:  2.709829568862915\n",
      "Loss:  2.6417486667633057\n",
      "Loss:  2.601489782333374\n",
      "Loss:  2.624789237976074\n",
      "Loss:  2.591221332550049\n",
      "Loss:  2.6019203662872314\n",
      "Loss:  2.5848097801208496\n",
      "Loss:  2.580963134765625\n",
      "Loss:  2.605851411819458\n",
      "Loss:  2.6082606315612793\n",
      "Loss:  2.594855785369873\n",
      "Loss:  2.6350631713867188\n",
      "Loss:  2.5795929431915283\n",
      "Loss:  2.6845204830169678\n",
      "Loss:  2.6796889305114746\n",
      "Loss:  2.6351335048675537\n",
      "Loss:  2.6113486289978027\n",
      "Loss:  2.6463186740875244\n",
      "Loss:  2.580281972885132\n",
      "Loss:  2.614947557449341\n",
      "Loss:  2.557074546813965\n",
      "Loss:  2.5989654064178467\n",
      "Loss:  2.6036698818206787\n",
      "Loss:  2.5092852115631104\n",
      "Loss:  2.5926408767700195\n",
      "Loss:  2.6757428646087646\n",
      "Loss:  2.572033643722534\n",
      "Loss:  2.6660892963409424\n",
      "Loss:  2.5992462635040283\n",
      "Loss:  2.6005258560180664\n",
      "Loss:  1.4985610246658325\n",
      "      5        \u001b[36m2.6158\u001b[0m        \u001b[32m2.5701\u001b[0m  5.2296\n",
      "Loss:  2.6895270347595215\n",
      "Loss:  2.687478542327881\n",
      "Loss:  2.626772403717041\n",
      "Loss:  2.596999406814575\n",
      "Loss:  2.6109976768493652\n",
      "Loss:  2.582362651824951\n",
      "Loss:  2.5982933044433594\n",
      "Loss:  2.5801780223846436\n",
      "Loss:  2.574038028717041\n",
      "Loss:  2.6095519065856934\n",
      "Loss:  2.6050124168395996\n",
      "Loss:  2.587847948074341\n",
      "Loss:  2.6364452838897705\n",
      "Loss:  2.5798537731170654\n",
      "Loss:  2.668560743331909\n",
      "Loss:  2.6647186279296875\n",
      "Loss:  2.6236820220947266\n",
      "Loss:  2.607196092605591\n",
      "Loss:  2.637746572494507\n",
      "Loss:  2.580021381378174\n",
      "Loss:  2.613260269165039\n",
      "Loss:  2.5590386390686035\n",
      "Loss:  2.5974481105804443\n",
      "Loss:  2.598503589630127\n",
      "Loss:  2.510348320007324\n",
      "Loss:  2.5973846912384033\n",
      "Loss:  2.665834903717041\n",
      "Loss:  2.571319818496704\n",
      "Loss:  2.6555895805358887\n",
      "Loss:  2.597698211669922\n",
      "Loss:  2.5983614921569824\n",
      "Loss:  1.495378017425537\n",
      "      6        \u001b[36m2.6093\u001b[0m        \u001b[32m2.5667\u001b[0m  5.2502\n",
      "Loss:  2.6792562007904053\n",
      "Loss:  2.6751387119293213\n",
      "Loss:  2.6183018684387207\n",
      "Loss:  2.5969340801239014\n",
      "Loss:  2.604123830795288\n",
      "Loss:  2.5778143405914307\n",
      "Loss:  2.5986862182617188\n",
      "Loss:  2.578817129135132\n",
      "Loss:  2.5720481872558594\n",
      "Loss:  2.613759994506836\n",
      "Loss:  2.6043965816497803\n",
      "Loss:  2.585400104522705\n",
      "Loss:  2.6360738277435303\n",
      "Loss:  2.5823051929473877\n",
      "Loss:  2.659874439239502\n",
      "Loss:  2.6567904949188232\n",
      "Loss:  2.6173923015594482\n",
      "Loss:  2.6060073375701904\n",
      "Loss:  2.633894681930542\n",
      "Loss:  2.58066987991333\n",
      "Loss:  2.613041400909424\n",
      "Loss:  2.5606889724731445\n",
      "Loss:  2.5970065593719482\n",
      "Loss:  2.5959908962249756\n",
      "Loss:  2.5112979412078857\n",
      "Loss:  2.598653793334961\n",
      "Loss:  2.6626269817352295\n",
      "Loss:  2.5706214904785156\n",
      "Loss:  2.650946855545044\n",
      "Loss:  2.5955278873443604\n",
      "Loss:  2.597946882247925\n",
      "Loss:  1.4976885318756104\n",
      "      7        \u001b[36m2.6065\u001b[0m        \u001b[32m2.5653\u001b[0m  5.2139\n",
      "Loss:  2.6736669540405273\n",
      "Loss:  2.6699607372283936\n",
      "Loss:  2.614168405532837\n",
      "Loss:  2.597777843475342\n",
      "Loss:  2.601280689239502\n",
      "Loss:  2.5761594772338867\n",
      "Loss:  2.599538803100586\n",
      "Loss:  2.5789783000946045\n",
      "Loss:  2.571974992752075\n",
      "Loss:  2.6155223846435547\n",
      "Loss:  2.6037914752960205\n",
      "Loss:  2.584465265274048\n",
      "Loss:  2.6344523429870605\n",
      "Loss:  2.5845353603363037\n",
      "Loss:  2.656240463256836\n",
      "Loss:  2.6535489559173584\n",
      "Loss:  2.6144561767578125\n",
      "Loss:  2.6056718826293945\n",
      "Loss:  2.6323349475860596\n",
      "Loss:  2.580662250518799\n",
      "Loss:  2.6124961376190186\n",
      "Loss:  2.5606939792633057\n",
      "Loss:  2.5966129302978516\n",
      "Loss:  2.5947399139404297\n",
      "Loss:  2.511945962905884\n",
      "Loss:  2.59834885597229\n",
      "Loss:  2.661940813064575\n",
      "Loss:  2.5700573921203613\n",
      "Loss:  2.649155378341675\n",
      "Loss:  2.593747615814209\n",
      "Loss:  2.597919225692749\n",
      "Loss:  1.5006165504455566\n",
      "      8        \u001b[36m2.6053\u001b[0m        \u001b[32m2.5646\u001b[0m  5.2043\n",
      "Loss:  2.6703221797943115\n",
      "Loss:  2.6679742336273193\n",
      "Loss:  2.6123006343841553\n",
      "Loss:  2.597954511642456\n",
      "Loss:  2.6000118255615234\n",
      "Loss:  2.575939416885376\n",
      "Loss:  2.5996387004852295\n",
      "Loss:  2.579378604888916\n",
      "Loss:  2.5720648765563965\n",
      "Loss:  2.6158294677734375\n",
      "Loss:  2.603358507156372\n",
      "Loss:  2.584198474884033\n",
      "Loss:  2.63327956199646\n",
      "Loss:  2.5854363441467285\n",
      "Loss:  2.6550698280334473\n",
      "Loss:  2.6523778438568115\n",
      "Loss:  2.6127333641052246\n",
      "Loss:  2.6054434776306152\n",
      "Loss:  2.6316123008728027\n",
      "Loss:  2.5802934169769287\n",
      "Loss:  2.6118810176849365\n",
      "Loss:  2.5603139400482178\n",
      "Loss:  2.5964250564575195\n",
      "Loss:  2.5940213203430176\n",
      "Loss:  2.5123977661132812\n",
      "Loss:  2.597660541534424\n",
      "Loss:  2.6617729663848877\n",
      "Loss:  2.5697152614593506\n",
      "Loss:  2.6485610008239746\n",
      "Loss:  2.5926010608673096\n",
      "Loss:  2.597867012023926\n",
      "Loss:  1.5022647380828857\n",
      "      9        \u001b[36m2.6047\u001b[0m        \u001b[32m2.5642\u001b[0m  5.2138\n",
      "Loss:  2.667815685272217\n",
      "Loss:  2.6671571731567383\n",
      "Loss:  2.6113805770874023\n",
      "Loss:  2.597618579864502\n",
      "Loss:  2.5993330478668213\n",
      "Loss:  2.5759384632110596\n",
      "Loss:  2.5994317531585693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  2.5795435905456543\n",
      "Loss:  2.57192325592041\n",
      "Loss:  2.615762710571289\n",
      "Loss:  2.6032114028930664\n",
      "Loss:  2.5841941833496094\n",
      "Loss:  2.632446765899658\n",
      "Loss:  2.585608959197998\n",
      "Loss:  2.6547462940216064\n",
      "Loss:  2.651897430419922\n",
      "Loss:  2.611384630203247\n",
      "Loss:  2.6052494049072266\n",
      "Loss:  2.631199598312378\n",
      "Loss:  2.579848051071167\n",
      "Loss:  2.611353874206543\n",
      "Loss:  2.5598578453063965\n",
      "Loss:  2.596311569213867\n",
      "Loss:  2.593529224395752\n",
      "Loss:  2.5126705169677734\n",
      "Loss:  2.5967533588409424\n",
      "Loss:  2.6618964672088623\n",
      "Loss:  2.5693206787109375\n",
      "Loss:  2.6484503746032715\n",
      "Loss:  2.591674327850342\n",
      "Loss:  2.5977542400360107\n",
      "Loss:  1.5036251544952393\n",
      "     10        \u001b[36m2.6042\u001b[0m        \u001b[32m2.5639\u001b[0m  5.2567\n",
      "Loss:  2.6657156944274902\n",
      "Loss:  2.666884183883667\n",
      "Loss:  2.6108646392822266\n",
      "Loss:  2.5971181392669678\n",
      "Loss:  2.598935604095459\n",
      "Loss:  2.5759499073028564\n",
      "Loss:  2.5991759300231934\n",
      "Loss:  2.579561948776245\n",
      "Loss:  2.5716686248779297\n",
      "Loss:  2.615569829940796\n",
      "Loss:  2.6031641960144043\n",
      "Loss:  2.584256410598755\n",
      "Loss:  2.6317169666290283\n",
      "Loss:  2.585513114929199\n",
      "Loss:  2.654695510864258\n",
      "Loss:  2.651658535003662\n",
      "Loss:  2.6101291179656982\n",
      "Loss:  2.605076551437378\n",
      "Loss:  2.6309115886688232\n",
      "Loss:  2.5793886184692383\n",
      "Loss:  2.6109161376953125\n",
      "Loss:  2.5593597888946533\n",
      "Loss:  2.596200704574585\n",
      "Loss:  2.5931665897369385\n",
      "Loss:  2.5128424167633057\n",
      "Loss:  2.595825672149658\n",
      "Loss:  2.662153720855713\n",
      "Loss:  2.5689010620117188\n",
      "Loss:  2.6485378742218018\n",
      "Loss:  2.5908772945404053\n",
      "Loss:  2.597637414932251\n",
      "Loss:  1.5049248933792114\n",
      "     11        \u001b[36m2.6039\u001b[0m        \u001b[32m2.5636\u001b[0m  5.2219\n",
      "Loss:  2.6638097763061523\n",
      "Loss:  2.666830062866211\n",
      "Loss:  2.6105246543884277\n",
      "Loss:  2.596574544906616\n",
      "Loss:  2.5986948013305664\n",
      "Loss:  2.5760090351104736\n",
      "Loss:  2.5989527702331543\n",
      "Loss:  2.579507827758789\n",
      "Loss:  2.5713727474212646\n",
      "Loss:  2.615363836288452\n",
      "Loss:  2.603170394897461\n",
      "Loss:  2.5843451023101807\n",
      "Loss:  2.6310431957244873\n",
      "Loss:  2.585292339324951\n",
      "Loss:  2.654719591140747\n",
      "Loss:  2.651510715484619\n",
      "Loss:  2.608881950378418\n",
      "Loss:  2.6049203872680664\n",
      "Loss:  2.6306655406951904\n",
      "Loss:  2.578948497772217\n",
      "Loss:  2.610565185546875\n",
      "Loss:  2.5588572025299072\n",
      "Loss:  2.596076250076294\n",
      "Loss:  2.5928900241851807\n",
      "Loss:  2.5129456520080566\n",
      "Loss:  2.594960927963257\n",
      "Loss:  2.6624596118927\n",
      "Loss:  2.5684823989868164\n",
      "Loss:  2.6487042903900146\n",
      "Loss:  2.590183973312378\n",
      "Loss:  2.597541332244873\n",
      "Loss:  1.5061911344528198\n",
      "     12        \u001b[36m2.6035\u001b[0m        \u001b[32m2.5634\u001b[0m  5.2058\n",
      "Loss:  2.6620004177093506\n",
      "Loss:  2.666867733001709\n",
      "Loss:  2.610260009765625\n",
      "Loss:  2.596017837524414\n",
      "Loss:  2.5985541343688965\n",
      "Loss:  2.5761234760284424\n",
      "Loss:  2.598773717880249\n",
      "Loss:  2.5793936252593994\n",
      "Loss:  2.571052312850952\n",
      "Loss:  2.6151797771453857\n",
      "Loss:  2.6032114028930664\n",
      "Loss:  2.5844461917877197\n",
      "Loss:  2.630394220352173\n",
      "Loss:  2.585000514984131\n",
      "Loss:  2.6547458171844482\n",
      "Loss:  2.6513965129852295\n",
      "Loss:  2.607619047164917\n",
      "Loss:  2.604779005050659\n",
      "Loss:  2.630419969558716\n",
      "Loss:  2.5785415172576904\n",
      "Loss:  2.6102864742279053\n",
      "Loss:  2.5583598613739014\n",
      "Loss:  2.5959298610687256\n",
      "Loss:  2.5926756858825684\n",
      "Loss:  2.5129849910736084\n",
      "Loss:  2.594179630279541\n",
      "Loss:  2.662775754928589\n",
      "Loss:  2.568061351776123\n",
      "Loss:  2.648902177810669\n",
      "Loss:  2.5895817279815674\n",
      "Loss:  2.5974738597869873\n",
      "Loss:  1.5074383020401\n",
      "     13        \u001b[36m2.6032\u001b[0m        \u001b[32m2.5632\u001b[0m  5.2054\n",
      "Loss:  2.660236120223999\n",
      "Loss:  2.666950225830078\n",
      "Loss:  2.6100234985351562\n",
      "Loss:  2.5954442024230957\n",
      "Loss:  2.598480463027954\n",
      "Loss:  2.5762884616851807\n",
      "Loss:  2.5986251831054688\n",
      "Loss:  2.5792131423950195\n",
      "Loss:  2.5707037448883057\n",
      "Loss:  2.6150169372558594\n",
      "Loss:  2.60327410697937\n",
      "Loss:  2.5845518112182617\n",
      "Loss:  2.6297476291656494\n",
      "Loss:  2.584653854370117\n",
      "Loss:  2.6547493934631348\n",
      "Loss:  2.6512889862060547\n",
      "Loss:  2.606326103210449\n",
      "Loss:  2.6046526432037354\n",
      "Loss:  2.6301541328430176\n",
      "Loss:  2.578172445297241\n",
      "Loss:  2.6100664138793945\n",
      "Loss:  2.557868242263794\n",
      "Loss:  2.5957586765289307\n",
      "Loss:  2.5925095081329346\n",
      "Loss:  2.51295804977417\n",
      "Loss:  2.593480110168457\n",
      "Loss:  2.6630804538726807\n",
      "Loss:  2.5676279067993164\n",
      "Loss:  2.64910888671875\n",
      "Loss:  2.589064598083496\n",
      "Loss:  2.5974338054656982\n",
      "Loss:  1.5086699724197388\n",
      "     14        \u001b[36m2.6029\u001b[0m        \u001b[32m2.5631\u001b[0m  5.2028\n",
      "Loss:  2.6584787368774414\n",
      "Loss:  2.6670596599578857\n",
      "Loss:  2.6097869873046875\n",
      "Loss:  2.5948431491851807\n",
      "Loss:  2.598453998565674\n",
      "Loss:  2.576490879058838\n",
      "Loss:  2.598491907119751\n",
      "Loss:  2.578953742980957\n",
      "Loss:  2.570317506790161\n",
      "Loss:  2.614863157272339\n",
      "Loss:  2.6033518314361572\n",
      "Loss:  2.5846574306488037\n",
      "Loss:  2.629084825515747\n",
      "Loss:  2.5842576026916504\n",
      "Loss:  2.6547203063964844\n",
      "Loss:  2.6511709690093994\n",
      "Loss:  2.6049911975860596\n",
      "Loss:  2.6045422554016113\n",
      "Loss:  2.629856824874878\n",
      "Loss:  2.577840805053711\n",
      "Loss:  2.609894037246704\n",
      "Loss:  2.5573806762695312\n",
      "Loss:  2.595562696456909\n",
      "Loss:  2.5923802852630615\n",
      "Loss:  2.512859582901001\n",
      "Loss:  2.592855453491211\n",
      "Loss:  2.663358211517334\n",
      "Loss:  2.567171812057495\n",
      "Loss:  2.6493146419525146\n",
      "Loss:  2.5886294841766357\n",
      "Loss:  2.597423791885376\n",
      "Loss:  1.5098814964294434\n",
      "     15        \u001b[36m2.6027\u001b[0m        \u001b[32m2.5630\u001b[0m  5.2192\n",
      "Loss:  2.656698226928711\n",
      "Loss:  2.6671876907348633\n",
      "Loss:  2.609532356262207\n",
      "Loss:  2.5942037105560303\n",
      "Loss:  2.5984601974487305\n",
      "Loss:  2.576718807220459\n",
      "Loss:  2.598357915878296\n",
      "Loss:  2.5786070823669434\n",
      "Loss:  2.569884777069092\n",
      "Loss:  2.6147055625915527\n",
      "Loss:  2.603435754776001\n",
      "Loss:  2.584761381149292\n",
      "Loss:  2.628387689590454\n",
      "Loss:  2.583812952041626\n",
      "Loss:  2.6546542644500732\n",
      "Loss:  2.651033401489258\n",
      "Loss:  2.603599786758423\n",
      "Loss:  2.6044487953186035\n",
      "Loss:  2.6295242309570312\n",
      "Loss:  2.5775434970855713\n",
      "Loss:  2.609757423400879\n",
      "Loss:  2.556891679763794\n",
      "Loss:  2.5953426361083984\n",
      "Loss:  2.5922772884368896\n",
      "Loss:  2.5126864910125732\n",
      "Loss:  2.592299222946167\n",
      "Loss:  2.6635947227478027\n",
      "Loss:  2.5666840076446533\n",
      "Loss:  2.64951229095459\n",
      "Loss:  2.5882744789123535\n",
      "Loss:  2.5974478721618652\n",
      "Loss:  1.5110679864883423\n",
      "     16        \u001b[36m2.6023\u001b[0m        \u001b[32m2.5629\u001b[0m  5.2424\n",
      "Loss:  2.6548733711242676\n",
      "Loss:  2.6673264503479004\n",
      "Loss:  2.6092443466186523\n",
      "Loss:  2.593515396118164\n",
      "Loss:  2.59848690032959\n",
      "Loss:  2.5769572257995605\n",
      "Loss:  2.5982089042663574\n",
      "Loss:  2.578165292739868\n",
      "Loss:  2.569401264190674\n",
      "Loss:  2.614530324935913\n",
      "Loss:  2.603515625\n",
      "Loss:  2.584862232208252\n",
      "Loss:  2.6276416778564453\n",
      "Loss:  2.583317279815674\n",
      "Loss:  2.6545491218566895\n",
      "Loss:  2.6508686542510986\n",
      "Loss:  2.602139711380005\n",
      "Loss:  2.604372501373291\n",
      "Loss:  2.629160165786743\n",
      "Loss:  2.5772757530212402\n",
      "Loss:  2.609649419784546\n",
      "Loss:  2.556396722793579\n",
      "Loss:  2.5950992107391357\n",
      "Loss:  2.5921895503997803\n",
      "Loss:  2.5124351978302\n",
      "Loss:  2.5918028354644775\n",
      "Loss:  2.6637799739837646\n",
      "Loss:  2.566160202026367\n",
      "Loss:  2.6496963500976562\n",
      "Loss:  2.587998390197754\n",
      "Loss:  2.59751296043396\n",
      "Loss:  1.5122181177139282\n",
      "     17        \u001b[36m2.6020\u001b[0m        \u001b[32m2.5628\u001b[0m  5.2208\n",
      "Loss:  2.65299391746521\n",
      "Loss:  2.667468547821045\n",
      "Loss:  2.6089110374450684\n",
      "Loss:  2.5927700996398926\n",
      "Loss:  2.598524332046509\n",
      "Loss:  2.5771937370300293\n",
      "Loss:  2.598033905029297\n",
      "Loss:  2.5776257514953613\n",
      "Loss:  2.568864345550537\n",
      "Loss:  2.6143224239349365\n",
      "Loss:  2.603579044342041\n",
      "Loss:  2.584959030151367\n",
      "Loss:  2.626835823059082\n",
      "Loss:  2.5827674865722656\n",
      "Loss:  2.6544015407562256\n",
      "Loss:  2.6506710052490234\n",
      "Loss:  2.6005990505218506\n",
      "Loss:  2.6043121814727783\n",
      "Loss:  2.6287753582000732\n",
      "Loss:  2.5770294666290283\n",
      "Loss:  2.609560489654541\n",
      "Loss:  2.555893898010254\n",
      "Loss:  2.5948326587677\n",
      "Loss:  2.5921053886413574\n",
      "Loss:  2.5121023654937744\n",
      "Loss:  2.591362476348877\n",
      "Loss:  2.66390323638916\n",
      "Loss:  2.56559681892395\n",
      "Loss:  2.6498639583587646\n",
      "Loss:  2.587799072265625\n",
      "Loss:  2.5976264476776123\n",
      "Loss:  1.513322114944458\n",
      "     18        \u001b[36m2.6016\u001b[0m        \u001b[32m2.5627\u001b[0m  5.2067\n",
      "Loss:  2.651059150695801\n",
      "Loss:  2.6676082611083984\n",
      "Loss:  2.608520269393921\n",
      "Loss:  2.591960906982422\n",
      "Loss:  2.5985586643218994\n",
      "Loss:  2.577415704727173\n",
      "Loss:  2.597822904586792\n",
      "Loss:  2.5769896507263184\n",
      "Loss:  2.568275213241577\n",
      "Loss:  2.614065647125244\n",
      "Loss:  2.6036102771759033\n",
      "Loss:  2.5850515365600586\n",
      "Loss:  2.625955581665039\n",
      "Loss:  2.582153558731079\n",
      "Loss:  2.6542065143585205\n",
      "Loss:  2.650434732437134\n",
      "Loss:  2.5989644527435303\n",
      "Loss:  2.6042675971984863\n",
      "Loss:  2.6283884048461914\n",
      "Loss:  2.5767977237701416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  2.6094839572906494\n",
      "Loss:  2.55537748336792\n",
      "Loss:  2.59454345703125\n",
      "Loss:  2.5920093059539795\n",
      "Loss:  2.511683940887451\n",
      "Loss:  2.5909721851348877\n",
      "Loss:  2.663954973220825\n",
      "Loss:  2.5649912357330322\n",
      "Loss:  2.6500110626220703\n",
      "Loss:  2.587676525115967\n",
      "Loss:  2.597801685333252\n",
      "Loss:  1.514370083808899\n",
      "     19        \u001b[36m2.6012\u001b[0m        \u001b[32m2.5626\u001b[0m  5.2068\n",
      "Loss:  2.6490728855133057\n",
      "Loss:  2.6677417755126953\n",
      "Loss:  2.608059883117676\n",
      "Loss:  2.5910778045654297\n",
      "Loss:  2.598576545715332\n",
      "Loss:  2.5776126384735107\n",
      "Loss:  2.597564458847046\n",
      "Loss:  2.5762557983398438\n",
      "Loss:  2.5676376819610596\n",
      "Loss:  2.6137399673461914\n",
      "Loss:  2.60359263420105\n",
      "Loss:  2.585139274597168\n",
      "Loss:  2.624990701675415\n",
      "Loss:  2.5814602375030518\n",
      "Loss:  2.6539433002471924\n",
      "Loss:  2.650153160095215\n",
      "Loss:  2.5972228050231934\n",
      "Loss:  2.6042370796203613\n",
      "Loss:  2.628030776977539\n",
      "Loss:  2.576568603515625\n",
      "Loss:  2.6094117164611816\n",
      "Loss:  2.5548453330993652\n",
      "Loss:  2.594231605529785\n",
      "Loss:  2.591885805130005\n",
      "Loss:  2.511171579360962\n",
      "Loss:  2.590627908706665\n",
      "Loss:  2.663928985595703\n",
      "Loss:  2.564340353012085\n",
      "Loss:  2.6501355171203613\n",
      "Loss:  2.587629556655884\n",
      "Loss:  2.598045587539673\n",
      "Loss:  1.5153511762619019\n",
      "     20        \u001b[36m2.6008\u001b[0m        \u001b[32m2.5626\u001b[0m  5.2024\n",
      "Loss:  2.6470518112182617\n",
      "Loss:  2.667853355407715\n",
      "Loss:  2.6075174808502197\n",
      "Loss:  2.5901107788085938\n",
      "Loss:  2.5985631942749023\n",
      "Loss:  2.577775239944458\n",
      "Loss:  2.5972464084625244\n",
      "Loss:  2.5754261016845703\n",
      "Loss:  2.566955089569092\n",
      "Loss:  2.6133217811584473\n",
      "Loss:  2.603505849838257\n",
      "Loss:  2.585221529006958\n",
      "Loss:  2.6239261627197266\n",
      "Loss:  2.580667495727539\n",
      "Loss:  2.6536121368408203\n",
      "Loss:  2.6498188972473145\n",
      "Loss:  2.595357894897461\n",
      "Loss:  2.6042215824127197\n",
      "Loss:  2.6277453899383545\n",
      "Loss:  2.576334238052368\n",
      "Loss:  2.6093361377716064\n",
      "Loss:  2.554293394088745\n",
      "Loss:  2.593895196914673\n",
      "Loss:  2.59171724319458\n",
      "Loss:  2.5105583667755127\n",
      "Loss:  2.590325355529785\n",
      "Loss:  2.663822650909424\n",
      "Loss:  2.5636374950408936\n",
      "Loss:  2.650235891342163\n",
      "Loss:  2.5876598358154297\n",
      "Loss:  2.5983710289001465\n",
      "Loss:  1.516260027885437\n",
      "     21        \u001b[36m2.6003\u001b[0m        \u001b[32m2.5625\u001b[0m  5.2179\n",
      "Loss:  2.6450109481811523\n",
      "Loss:  2.6679368019104004\n",
      "Loss:  2.60687518119812\n",
      "Loss:  2.589046001434326\n",
      "Loss:  2.5985023975372314\n",
      "Loss:  2.5778937339782715\n",
      "Loss:  2.5968542098999023\n",
      "Loss:  2.57450008392334\n",
      "Loss:  2.5662293434143066\n",
      "Loss:  2.612783193588257\n",
      "Loss:  2.6033244132995605\n",
      "Loss:  2.585296630859375\n",
      "Loss:  2.622746229171753\n",
      "Loss:  2.579745292663574\n",
      "Loss:  2.6531975269317627\n",
      "Loss:  2.6494202613830566\n",
      "Loss:  2.5933475494384766\n",
      "Loss:  2.6042215824127197\n",
      "Loss:  2.6275792121887207\n",
      "Loss:  2.5760838985443115\n",
      "Loss:  2.6092541217803955\n",
      "Loss:  2.553715705871582\n",
      "Loss:  2.5935328006744385\n",
      "Loss:  2.5914878845214844\n",
      "Loss:  2.5098352432250977\n",
      "Loss:  2.5900614261627197\n",
      "Loss:  2.6636276245117188\n",
      "Loss:  2.562877655029297\n",
      "Loss:  2.65030837059021\n",
      "Loss:  2.587766647338867\n",
      "Loss:  2.598792791366577\n",
      "Loss:  1.517092227935791\n",
      "     22        \u001b[36m2.5998\u001b[0m        \u001b[32m2.5624\u001b[0m  5.2388\n",
      "Loss:  2.6429688930511475\n",
      "Loss:  2.6679892539978027\n",
      "Loss:  2.6061127185821533\n",
      "Loss:  2.587862491607666\n",
      "Loss:  2.598370313644409\n",
      "Loss:  2.5779612064361572\n",
      "Loss:  2.596369981765747\n",
      "Loss:  2.5734779834747314\n",
      "Loss:  2.565467596054077\n",
      "Loss:  2.612086772918701\n",
      "Loss:  2.6030263900756836\n",
      "Loss:  2.58536696434021\n",
      "Loss:  2.6214237213134766\n",
      "Loss:  2.578645706176758\n",
      "Loss:  2.6526763439178467\n",
      "Loss:  2.6489434242248535\n",
      "Loss:  2.591172456741333\n",
      "Loss:  2.6042368412017822\n",
      "Loss:  2.627577066421509\n",
      "Loss:  2.5757997035980225\n",
      "Loss:  2.6091396808624268\n",
      "Loss:  2.5531165599823\n",
      "Loss:  2.593139886856079\n",
      "Loss:  2.5911948680877686\n",
      "Loss:  2.5089893341064453\n",
      "Loss:  2.5898232460021973\n",
      "Loss:  2.663342237472534\n",
      "Loss:  2.5620553493499756\n",
      "Loss:  2.650357484817505\n",
      "Loss:  2.5879549980163574\n",
      "Loss:  2.599325180053711\n",
      "Loss:  1.5178505182266235\n",
      "     23        \u001b[36m2.5992\u001b[0m        \u001b[32m2.5624\u001b[0m  5.2213\n",
      "Loss:  2.640928268432617\n",
      "Loss:  2.6679670810699463\n",
      "Loss:  2.605203628540039\n",
      "Loss:  2.5865375995635986\n",
      "Loss:  2.5981476306915283\n",
      "Loss:  2.5779712200164795\n",
      "Loss:  2.5957772731781006\n",
      "Loss:  2.5723443031311035\n",
      "Loss:  2.564664125442505\n",
      "Loss:  2.611191511154175\n",
      "Loss:  2.602567195892334\n",
      "Loss:  2.5854201316833496\n",
      "Loss:  2.6199309825897217\n",
      "Loss:  2.5772948265075684\n",
      "Loss:  2.652014970779419\n",
      "Loss:  2.6483654975891113\n",
      "Loss:  2.588796615600586\n",
      "Loss:  2.604271411895752\n",
      "Loss:  2.627746820449829\n",
      "Loss:  2.5754621028900146\n",
      "Loss:  2.6089937686920166\n",
      "Loss:  2.5524818897247314\n",
      "Loss:  2.592710256576538\n",
      "Loss:  2.5908024311065674\n",
      "Loss:  2.5080060958862305\n",
      "Loss:  2.5895912647247314\n",
      "Loss:  2.6629528999328613\n",
      "Loss:  2.5611605644226074\n",
      "Loss:  2.6503818035125732\n",
      "Loss:  2.5882275104522705\n",
      "Loss:  2.59999418258667\n",
      "Loss:  1.5185271501541138\n",
      "     24        \u001b[36m2.5985\u001b[0m        \u001b[32m2.5623\u001b[0m  5.2052\n",
      "Loss:  2.6388866901397705\n",
      "Loss:  2.6678552627563477\n",
      "Loss:  2.604111433029175\n",
      "Loss:  2.5850329399108887\n",
      "Loss:  2.597804069519043\n",
      "Loss:  2.5779173374176025\n",
      "Loss:  2.5950374603271484\n",
      "Loss:  2.5710835456848145\n",
      "Loss:  2.5637691020965576\n",
      "Loss:  2.610025405883789\n",
      "Loss:  2.6019060611724854\n",
      "Loss:  2.5854575634002686\n",
      "Loss:  2.6181881427764893\n",
      "Loss:  2.575594902038574\n",
      "Loss:  2.651158332824707\n",
      "Loss:  2.64762544631958\n",
      "Loss:  2.586181402206421\n",
      "Loss:  2.6043238639831543\n",
      "Loss:  2.628061532974243\n",
      "Loss:  2.575045108795166\n",
      "Loss:  2.6087896823883057\n",
      "Loss:  2.551791191101074\n",
      "Loss:  2.5922317504882812\n",
      "Loss:  2.5902881622314453\n",
      "Loss:  2.5068600177764893\n",
      "Loss:  2.589355230331421\n",
      "Loss:  2.6624488830566406\n",
      "Loss:  2.5601773262023926\n",
      "Loss:  2.650390863418579\n",
      "Loss:  2.588594436645508\n",
      "Loss:  2.600842237472534\n",
      "Loss:  1.519148588180542\n",
      "     25        \u001b[36m2.5976\u001b[0m        \u001b[32m2.5623\u001b[0m  5.2090\n",
      "Loss:  2.6368162631988525\n",
      "Loss:  2.6676368713378906\n",
      "Loss:  2.6027987003326416\n",
      "Loss:  2.5832626819610596\n",
      "Loss:  2.59733247756958\n",
      "Loss:  2.5777933597564697\n",
      "Loss:  2.594109535217285\n",
      "Loss:  2.5696301460266113\n",
      "Loss:  2.5627803802490234\n",
      "Loss:  2.6085050106048584\n",
      "Loss:  2.600947618484497\n",
      "Loss:  2.5854837894439697\n",
      "Loss:  2.6161036491394043\n",
      "Loss:  2.5733795166015625\n",
      "Loss:  2.6499762535095215\n",
      "Loss:  2.6466708183288574\n",
      "Loss:  2.58329176902771\n",
      "Loss:  2.604386568069458\n",
      "Loss:  2.628401041030884\n",
      "Loss:  2.5745067596435547\n",
      "Loss:  2.60850191116333\n",
      "Loss:  2.5510072708129883\n",
      "Loss:  2.5917201042175293\n",
      "Loss:  2.5896193981170654\n",
      "Loss:  2.505521774291992\n",
      "Loss:  2.5890891551971436\n",
      "Loss:  2.6617939472198486\n",
      "Loss:  2.5590827465057373\n",
      "Loss:  2.650393486022949\n",
      "Loss:  2.5890567302703857\n",
      "Loss:  2.601968765258789\n",
      "Loss:  1.5197032690048218\n",
      "     26        \u001b[36m2.5967\u001b[0m        \u001b[32m2.5622\u001b[0m  5.2044\n",
      "Loss:  2.634697675704956\n",
      "Loss:  2.6673550605773926\n",
      "Loss:  2.601198673248291\n",
      "Loss:  2.581132173538208\n",
      "Loss:  2.5966837406158447\n",
      "Loss:  2.577591896057129\n",
      "Loss:  2.592912197113037\n",
      "Loss:  2.5679383277893066\n",
      "Loss:  2.5616819858551025\n",
      "Loss:  2.6065030097961426\n",
      "Loss:  2.599550485610962\n",
      "Loss:  2.5855026245117188\n",
      "Loss:  2.613553047180176\n",
      "Loss:  2.5704474449157715\n",
      "Loss:  2.648308515548706\n",
      "Loss:  2.6453981399536133\n",
      "Loss:  2.580171823501587\n",
      "Loss:  2.6044492721557617\n",
      "Loss:  2.628584146499634\n",
      "Loss:  2.57379150390625\n",
      "Loss:  2.6080322265625\n",
      "Loss:  2.5501163005828857\n",
      "Loss:  2.5911073684692383\n",
      "Loss:  2.588780403137207\n",
      "Loss:  2.5038647651672363\n",
      "Loss:  2.588764190673828\n",
      "Loss:  2.6609902381896973\n",
      "Loss:  2.557831287384033\n",
      "Loss:  2.650350332260132\n",
      "Loss:  2.5896291732788086\n",
      "Loss:  2.603421449661255\n",
      "Loss:  1.5201835632324219\n",
      "     27        \u001b[36m2.5954\u001b[0m        \u001b[32m2.5622\u001b[0m  5.2099\n",
      "Loss:  2.632488250732422\n",
      "Loss:  2.6667709350585938\n",
      "Loss:  2.5992491245269775\n",
      "Loss:  2.5781774520874023\n",
      "Loss:  2.595797538757324\n",
      "Loss:  2.5773048400878906\n",
      "Loss:  2.591371536254883\n",
      "Loss:  2.5658490657806396\n",
      "Loss:  2.5604248046875\n",
      "Loss:  2.6038084030151367\n",
      "Loss:  2.597489356994629\n",
      "Loss:  2.585507392883301\n",
      "Loss:  2.61020565032959\n",
      "Loss:  2.566246747970581\n",
      "Loss:  2.645817279815674\n",
      "Loss:  2.6436378955841064\n",
      "Loss:  2.576791763305664\n",
      "Loss:  2.604560613632202\n",
      "Loss:  2.6282923221588135\n",
      "Loss:  2.572831630706787\n",
      "Loss:  2.6073760986328125\n",
      "Loss:  2.5490097999572754\n",
      "Loss:  2.590264081954956\n",
      "Loss:  2.587620973587036\n",
      "Loss:  2.501736640930176\n",
      "Loss:  2.5883264541625977\n",
      "Loss:  2.660012722015381\n",
      "Loss:  2.556337833404541\n",
      "Loss:  2.6501410007476807\n",
      "Loss:  2.5902888774871826\n",
      "Loss:  2.6055562496185303\n",
      "Loss:  1.5205950736999512\n",
      "     28        \u001b[36m2.5938\u001b[0m        \u001b[32m2.5622\u001b[0m  5.2728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  2.63018536567688\n",
      "Loss:  2.665879487991333\n",
      "Loss:  2.5968315601348877\n",
      "Loss:  2.57411527633667\n",
      "Loss:  2.594536066055298\n",
      "Loss:  2.576916217803955\n",
      "Loss:  2.589357376098633\n",
      "Loss:  2.5632712841033936\n",
      "Loss:  2.558748722076416\n",
      "Loss:  2.600233793258667\n",
      "Loss:  2.5943775177001953\n",
      "Loss:  2.585526466369629\n",
      "Loss:  2.605700731277466\n",
      "Loss:  2.5597612857818604\n",
      "Loss:  2.641941785812378\n",
      "Loss:  2.641030788421631\n",
      "Loss:  2.5730855464935303\n",
      "Loss:  2.604586601257324\n",
      "Loss:  2.627293586730957\n",
      "Loss:  2.57163143157959\n",
      "Loss:  2.606182336807251\n",
      "Loss:  2.547551393508911\n",
      "Loss:  2.589047908782959\n",
      "Loss:  2.5861406326293945\n",
      "Loss:  2.4987411499023438\n",
      "Loss:  2.5876359939575195\n",
      "Loss:  2.6588168144226074\n",
      "Loss:  2.5544419288635254\n",
      "Loss:  2.6496822834014893\n",
      "Loss:  2.590933084487915\n",
      "Loss:  2.608894109725952\n",
      "Loss:  1.5208929777145386\n",
      "     29        \u001b[36m2.5916\u001b[0m        \u001b[32m2.5621\u001b[0m  5.2087\n",
      "Loss:  2.6278345584869385\n",
      "Loss:  2.6640145778656006\n",
      "Loss:  2.5937180519104004\n",
      "Loss:  2.5682373046875\n",
      "Loss:  2.5927066802978516\n",
      "Loss:  2.5762579441070557\n",
      "Loss:  2.5868377685546875\n",
      "Loss:  2.5597498416900635\n",
      "Loss:  2.556173801422119\n",
      "Loss:  2.595082998275757\n",
      "Loss:  2.5898168087005615\n",
      "Loss:  2.585665464401245\n",
      "Loss:  2.5992119312286377\n",
      "Loss:  2.5499207973480225\n",
      "Loss:  2.6358063220977783\n",
      "Loss:  2.6370038986206055\n",
      "Loss:  2.5683674812316895\n",
      "Loss:  2.6049017906188965\n",
      "Loss:  2.625230312347412\n",
      "Loss:  2.569904088973999\n",
      "Loss:  2.604400157928467\n",
      "Loss:  2.5455172061920166\n",
      "Loss:  2.5871856212615967\n",
      "Loss:  2.5840299129486084\n",
      "Loss:  2.4940497875213623\n",
      "Loss:  2.5866549015045166\n",
      "Loss:  2.6571178436279297\n",
      "Loss:  2.5520482063293457\n",
      "Loss:  2.6485025882720947\n",
      "Loss:  2.5915238857269287\n",
      "Loss:  2.6140501499176025\n",
      "Loss:  1.5204808712005615\n",
      "     30        \u001b[36m2.5883\u001b[0m        \u001b[32m2.5620\u001b[0m  5.2144\n",
      "Loss:  2.6249823570251465\n",
      "Loss:  2.6614036560058594\n",
      "Loss:  2.5893871784210205\n",
      "Loss:  2.5595703125\n",
      "Loss:  2.590050458908081\n",
      "Loss:  2.5752854347229004\n",
      "Loss:  2.583717107772827\n",
      "Loss:  2.554672956466675\n",
      "Loss:  2.5522637367248535\n",
      "Loss:  2.5874669551849365\n",
      "Loss:  2.5823872089385986\n",
      "Loss:  2.585960626602173\n",
      "Loss:  2.5889079570770264\n",
      "Loss:  2.53491473197937\n",
      "Loss:  2.6265878677368164\n",
      "Loss:  2.630608081817627\n",
      "Loss:  2.5618720054626465\n",
      "Loss:  2.6047654151916504\n",
      "Loss:  2.622164011001587\n",
      "Loss:  2.5676205158233643\n",
      "Loss:  2.60099458694458\n",
      "Loss:  2.54254412651062\n",
      "Loss:  2.5843417644500732\n",
      "Loss:  2.5810916423797607\n",
      "Loss:  2.486781120300293\n",
      "Loss:  2.5847833156585693\n",
      "Loss:  2.654923439025879\n",
      "Loss:  2.548676013946533\n",
      "Loss:  2.645725727081299\n",
      "Loss:  2.591184616088867\n",
      "Loss:  2.622649669647217\n",
      "Loss:  1.5186452865600586\n",
      "     31        \u001b[36m2.5835\u001b[0m        \u001b[32m2.5616\u001b[0m  5.2091\n",
      "Loss:  2.6205716133117676\n",
      "Loss:  2.6566243171691895\n",
      "Loss:  2.5825095176696777\n",
      "Loss:  2.5488150119781494\n",
      "Loss:  2.5860347747802734\n",
      "Loss:  2.5739920139312744\n",
      "Loss:  2.5794596672058105\n",
      "Loss:  2.547485589981079\n",
      "Loss:  2.5457441806793213\n",
      "Loss:  2.576932430267334\n",
      "Loss:  2.5716452598571777\n",
      "Loss:  2.5859618186950684\n",
      "Loss:  2.5748178958892822\n",
      "Loss:  2.5131442546844482\n",
      "Loss:  2.6146178245544434\n",
      "Loss:  2.620516777038574\n",
      "Loss:  2.552091360092163\n",
      "Loss:  2.6027169227600098\n",
      "Loss:  2.6180686950683594\n",
      "Loss:  2.5646555423736572\n",
      "Loss:  2.5957388877868652\n",
      "Loss:  2.5378775596618652\n",
      "Loss:  2.5800321102142334\n",
      "Loss:  2.5771827697753906\n",
      "Loss:  2.4743809700012207\n",
      "Loss:  2.5819225311279297\n",
      "Loss:  2.6534218788146973\n",
      "Loss:  2.5442402362823486\n",
      "Loss:  2.641455888748169\n",
      "Loss:  2.5898690223693848\n",
      "Loss:  2.636165142059326\n",
      "Loss:  1.516312599182129\n",
      "     32        \u001b[36m2.5763\u001b[0m        \u001b[32m2.5614\u001b[0m  5.2029\n",
      "Loss:  2.6141788959503174\n",
      "Loss:  2.6494977474212646\n",
      "Loss:  2.571619749069214\n",
      "Loss:  2.5366270542144775\n",
      "Loss:  2.580432176589966\n",
      "Loss:  2.5710761547088623\n",
      "Loss:  2.5726423263549805\n",
      "Loss:  2.537064790725708\n",
      "Loss:  2.5367307662963867\n",
      "Loss:  2.5648326873779297\n",
      "Loss:  2.557300090789795\n",
      "Loss:  2.5830395221710205\n",
      "Loss:  2.5564894676208496\n",
      "Loss:  2.483243942260742\n",
      "Loss:  2.6052803993225098\n",
      "Loss:  2.6063003540039062\n",
      "Loss:  2.5409433841705322\n",
      "Loss:  2.5963144302368164\n",
      "Loss:  2.612482786178589\n",
      "Loss:  2.5576910972595215\n",
      "Loss:  2.5878634452819824\n",
      "Loss:  2.5324785709381104\n",
      "Loss:  2.5731887817382812\n",
      "Loss:  2.571707248687744\n",
      "Loss:  2.455712080001831\n",
      "Loss:  2.578299045562744\n",
      "Loss:  2.6523427963256836\n",
      "Loss:  2.539207935333252\n",
      "Loss:  2.6369316577911377\n",
      "Loss:  2.58862566947937\n",
      "Loss:  2.6538937091827393\n",
      "Loss:  1.5140509605407715\n",
      "     33        \u001b[36m2.5665\u001b[0m        2.5617  5.2076\n",
      "Loss:  2.60188364982605\n",
      "Loss:  2.6407558917999268\n",
      "Loss:  2.5465891361236572\n",
      "Loss:  2.527071475982666\n",
      "Loss:  2.5725042819976807\n",
      "Loss:  2.565828323364258\n",
      "Loss:  2.5572047233581543\n",
      "Loss:  2.5225491523742676\n",
      "Loss:  2.527184247970581\n",
      "Loss:  2.5604653358459473\n",
      "Loss:  2.540724277496338\n",
      "Loss:  2.573570489883423\n",
      "Loss:  2.530486822128296\n",
      "Loss:  2.4456090927124023\n",
      "Loss:  2.6063382625579834\n",
      "Loss:  2.5892980098724365\n",
      "Loss:  2.528371572494507\n",
      "Loss:  2.587486743927002\n",
      "Loss:  2.6049675941467285\n",
      "Loss:  2.5472989082336426\n",
      "Loss:  2.575622081756592\n",
      "Loss:  2.523162841796875\n",
      "Loss:  2.5624196529388428\n",
      "Loss:  2.562713384628296\n",
      "Loss:  2.4306528568267822\n",
      "Loss:  2.572697162628174\n",
      "Loss:  2.65063738822937\n",
      "Loss:  2.532520055770874\n",
      "Loss:  2.6337358951568604\n",
      "Loss:  2.5892860889434814\n",
      "Loss:  2.6783499717712402\n",
      "Loss:  1.5106488466262817\n",
      "     34        \u001b[36m2.5536\u001b[0m        2.5628  5.2558\n",
      "Loss:  2.581289768218994\n",
      "Loss:  2.636040449142456\n",
      "Loss:  2.523078680038452\n",
      "Loss:  2.5213980674743652\n",
      "Loss:  2.5601136684417725\n",
      "Loss:  2.553858757019043\n",
      "Loss:  2.537019968032837\n",
      "Loss:  2.5095458030700684\n",
      "Loss:  2.5135350227355957\n",
      "Loss:  2.8677737712860107\n",
      "Loss:  2.517266035079956\n",
      "Loss:  2.559483766555786\n",
      "Loss:  2.5032553672790527\n",
      "Loss:  2.4298832416534424\n",
      "Loss:  2.5994668006896973\n",
      "Loss:  2.575199842453003\n",
      "Loss:  2.5175886154174805\n",
      "Loss:  2.584568500518799\n",
      "Loss:  2.603088855743408\n",
      "Loss:  2.530921697616577\n",
      "Loss:  2.5645627975463867\n",
      "Loss:  2.517108678817749\n",
      "Loss:  2.5501456260681152\n",
      "Loss:  2.560908317565918\n",
      "Loss:  2.4192354679107666\n",
      "Loss:  2.568176507949829\n",
      "Loss:  2.6361122131347656\n",
      "Loss:  2.5298779010772705\n",
      "Loss:  2.6271138191223145\n",
      "Loss:  2.5887718200683594\n",
      "Loss:  2.67801833152771\n",
      "Loss:  1.512375831604004\n",
      "     35        2.5538        \u001b[32m2.5582\u001b[0m  5.2202\n",
      "Loss:  2.5617334842681885\n",
      "Loss:  2.635573387145996\n",
      "Loss:  2.58847975730896\n",
      "Loss:  2.5297744274139404\n",
      "Loss:  2.5432372093200684\n",
      "Loss:  2.555116891860962\n",
      "Loss:  2.5214059352874756\n",
      "Loss:  2.4989545345306396\n",
      "Loss:  2.4956703186035156\n",
      "Loss:  2.6435933113098145\n",
      "Loss:  2.515413999557495\n",
      "Loss:  2.545332431793213\n",
      "Loss:  2.486584424972534\n",
      "Loss:  2.405778646469116\n",
      "Loss:  2.6032261848449707\n",
      "Loss:  2.553159713745117\n",
      "Loss:  2.4974725246429443\n",
      "Loss:  2.5783531665802\n",
      "Loss:  2.5809195041656494\n",
      "Loss:  2.519395112991333\n",
      "Loss:  2.539855480194092\n",
      "Loss:  2.4968273639678955\n",
      "Loss:  2.5346803665161133\n",
      "Loss:  2.550703287124634\n",
      "Loss:  2.3903751373291016\n",
      "Loss:  2.56295108795166\n",
      "Loss:  2.630244731903076\n",
      "Loss:  2.516063928604126\n",
      "Loss:  2.6295902729034424\n",
      "Loss:  2.596791982650757\n",
      "Loss:  2.652737855911255\n",
      "Loss:  1.5072394609451294\n",
      "     36        \u001b[36m2.5353\u001b[0m        \u001b[32m2.5516\u001b[0m  5.2059\n",
      "Loss:  2.550802230834961\n",
      "Loss:  2.622922897338867\n",
      "Loss:  2.636653184890747\n",
      "Loss:  2.536170482635498\n",
      "Loss:  2.5196309089660645\n",
      "Loss:  2.5521817207336426\n",
      "Loss:  2.506505012512207\n",
      "Loss:  2.505401611328125\n",
      "Loss:  2.467585802078247\n",
      "Loss:  2.5739498138427734\n",
      "Loss:  2.4818873405456543\n",
      "Loss:  2.5126559734344482\n",
      "Loss:  2.452024459838867\n",
      "Loss:  2.392576217651367\n",
      "Loss:  2.611326217651367\n",
      "Loss:  2.530087471008301\n",
      "Loss:  2.461266040802002\n",
      "Loss:  2.5833561420440674\n",
      "Loss:  2.58201003074646\n",
      "Loss:  2.488539934158325\n",
      "Loss:  2.525517463684082\n",
      "Loss:  2.4900107383728027\n",
      "Loss:  2.5094521045684814\n",
      "Loss:  2.532115936279297\n",
      "Loss:  2.3647663593292236\n",
      "Loss:  2.560948133468628\n",
      "Loss:  2.628885269165039\n",
      "Loss:  2.520733118057251\n",
      "Loss:  2.6246254444122314\n",
      "Loss:  2.600452184677124\n",
      "Loss:  2.6818416118621826\n",
      "Loss:  1.5212570428848267\n",
      "     37        \u001b[36m2.5200\u001b[0m        2.5569  5.2064\n",
      "Loss:  2.539663553237915\n",
      "Loss:  2.6026840209960938\n",
      "Loss:  2.663153886795044\n",
      "Loss:  2.5685505867004395\n",
      "Loss:  2.4770030975341797\n",
      "Loss:  2.5270047187805176\n",
      "Loss:  2.48054838180542\n",
      "Loss:  2.455326557159424\n",
      "Loss:  2.424345016479492\n",
      "Loss:  2.531641960144043\n",
      "Loss:  2.4532532691955566\n",
      "Loss:  2.4872748851776123\n",
      "Loss:  2.4232547283172607\n",
      "Loss:  2.379394054412842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  2.5978686809539795\n",
      "Loss:  2.49448561668396\n",
      "Loss:  2.451066732406616\n",
      "Loss:  2.571578025817871\n",
      "Loss:  2.5560872554779053\n",
      "Loss:  2.4637680053710938\n",
      "Loss:  2.530704975128174\n",
      "Loss:  2.4481987953186035\n",
      "Loss:  2.4562618732452393\n",
      "Loss:  2.5237467288970947\n",
      "Loss:  2.358001947402954\n",
      "Loss:  2.56693696975708\n",
      "Loss:  2.6245980262756348\n",
      "Loss:  2.516566276550293\n",
      "Loss:  2.609206199645996\n",
      "Loss:  2.6659069061279297\n",
      "Loss:  2.6880013942718506\n",
      "Loss:  1.5148017406463623\n",
      "     38        \u001b[36m2.4990\u001b[0m        2.5652  5.2040\n",
      "Loss:  2.5399625301361084\n",
      "Loss:  2.5731871128082275\n",
      "Loss:  2.523909568786621\n",
      "Loss:  2.5266027450561523\n",
      "Loss:  2.451725721359253\n",
      "Loss:  2.5030429363250732\n",
      "Loss:  2.4455106258392334\n",
      "Loss:  2.408345937728882\n",
      "Loss:  2.38558292388916\n",
      "Loss:  2.483330011367798\n",
      "Loss:  2.433821439743042\n",
      "Loss:  2.469761610031128\n",
      "Loss:  2.38862681388855\n",
      "Loss:  2.3889145851135254\n",
      "Loss:  2.6059446334838867\n",
      "Loss:  2.4842000007629395\n",
      "Loss:  2.480173110961914\n",
      "Loss:  2.560513973236084\n",
      "Loss:  2.5562658309936523\n",
      "Loss:  2.4527060985565186\n",
      "Loss:  2.485990285873413\n",
      "Loss:  2.4351532459259033\n",
      "Loss:  2.451486587524414\n",
      "Loss:  2.513570547103882\n",
      "Loss:  2.3376548290252686\n",
      "Loss:  2.550173282623291\n",
      "Loss:  2.6258275508880615\n",
      "Loss:  2.5158493518829346\n",
      "Loss:  2.623016357421875\n",
      "Loss:  2.637948751449585\n",
      "Loss:  2.68733286857605\n",
      "Loss:  1.5363327264785767\n",
      "     39        \u001b[36m2.4758\u001b[0m        2.5611  5.2056\n",
      "Loss:  2.5251917839050293\n",
      "Loss:  2.5591652393341064\n",
      "Loss:  2.587399959564209\n",
      "Loss:  2.48688006401062\n",
      "Loss:  2.4141478538513184\n",
      "Loss:  2.4765737056732178\n",
      "Loss:  2.4282472133636475\n",
      "Loss:  2.382725238800049\n",
      "Loss:  2.3620479106903076\n",
      "Loss:  2.464759111404419\n",
      "Loss:  2.421555995941162\n",
      "Loss:  2.4949920177459717\n",
      "Loss:  2.36193585395813\n",
      "Loss:  2.4715559482574463\n",
      "Loss:  2.623187780380249\n",
      "Loss:  2.462604522705078\n",
      "Loss:  2.411853551864624\n",
      "Loss:  2.5499327182769775\n",
      "Loss:  2.534849166870117\n",
      "Loss:  2.4429702758789062\n",
      "Loss:  2.459745168685913\n",
      "Loss:  2.4412524700164795\n",
      "Loss:  2.461596727371216\n",
      "Loss:  2.503218650817871\n",
      "Loss:  2.30780029296875\n",
      "Loss:  2.556330442428589\n",
      "Loss:  2.6143317222595215\n",
      "Loss:  2.5001258850097656\n",
      "Loss:  2.6231231689453125\n",
      "Loss:  2.678656578063965\n",
      "Loss:  2.704493522644043\n",
      "Loss:  1.5366142988204956\n",
      "     40        \u001b[36m2.4659\u001b[0m        2.5670  5.2533\n",
      "Loss:  2.50597882270813\n",
      "Loss:  2.5453598499298096\n",
      "Loss:  2.454498529434204\n",
      "Loss:  2.4642438888549805\n",
      "Loss:  2.3897674083709717\n",
      "Loss:  2.450279474258423\n",
      "Loss:  2.405526638031006\n",
      "Loss:  2.3553552627563477\n",
      "Loss:  2.345161199569702\n",
      "Loss:  2.429666757583618\n",
      "Loss:  2.399730920791626\n",
      "Loss:  2.4957754611968994\n",
      "Loss:  2.3558743000030518\n",
      "Loss:  2.338125705718994\n",
      "Loss:  2.5887298583984375\n",
      "Loss:  2.444688558578491\n",
      "Loss:  2.5297489166259766\n",
      "Loss:  2.5172407627105713\n",
      "Loss:  2.509422540664673\n",
      "Loss:  2.425506353378296\n",
      "Loss:  2.406252861022949\n",
      "Loss:  2.4513607025146484\n",
      "Loss:  2.45782732963562\n",
      "Loss:  2.4943056106567383\n",
      "Loss:  2.2851028442382812\n",
      "Loss:  2.560753583908081\n",
      "Loss:  2.6418213844299316\n",
      "Loss:  2.5101184844970703\n",
      "Loss:  2.6409311294555664\n",
      "Loss:  2.668562889099121\n",
      "Loss:  2.791430950164795\n",
      "Loss:  1.5356107950210571\n",
      "     41        \u001b[36m2.4422\u001b[0m        2.5888  5.2230\n",
      "Loss:  2.5373306274414062\n",
      "Loss:  2.5398831367492676\n",
      "Loss:  2.4604859352111816\n",
      "Loss:  2.478745460510254\n",
      "Loss:  2.3776755332946777\n",
      "Loss:  2.423640489578247\n",
      "Loss:  2.3904545307159424\n",
      "Loss:  2.3249456882476807\n",
      "Loss:  2.3630712032318115\n",
      "Loss:  2.403552293777466\n",
      "Loss:  2.3763208389282227\n",
      "Loss:  2.5185930728912354\n",
      "Loss:  2.3550727367401123\n",
      "Loss:  2.376605987548828\n",
      "Loss:  2.603022813796997\n",
      "Loss:  2.4226505756378174\n",
      "Loss:  2.528752326965332\n",
      "Loss:  2.54830265045166\n",
      "Loss:  2.5095977783203125\n",
      "Loss:  2.429424285888672\n",
      "Loss:  2.396061658859253\n",
      "Loss:  2.4182205200195312\n",
      "Loss:  2.3961424827575684\n",
      "Loss:  2.5088818073272705\n",
      "Loss:  2.294321060180664\n",
      "Loss:  2.5181989669799805\n",
      "Loss:  2.6109728813171387\n",
      "Loss:  2.4903645515441895\n",
      "Loss:  2.657399892807007\n",
      "Loss:  2.7794570922851562\n",
      "Loss:  2.637996196746826\n",
      "Loss:  1.574357509613037\n",
      "     42        \u001b[36m2.4397\u001b[0m        2.5714  5.2070\n",
      "Loss:  2.483839988708496\n",
      "Loss:  2.6522762775421143\n",
      "Loss:  2.598398447036743\n",
      "Loss:  2.4530627727508545\n",
      "Loss:  2.378688097000122\n",
      "Loss:  2.4584243297576904\n",
      "Loss:  2.453179359436035\n",
      "Loss:  2.3631629943847656\n",
      "Loss:  2.401700258255005\n",
      "Loss:  2.3919320106506348\n",
      "Loss:  2.454608917236328\n",
      "Loss:  2.5224897861480713\n",
      "Loss:  2.473179578781128\n",
      "Loss:  2.450953245162964\n",
      "Loss:  2.5713350772857666\n",
      "Loss:  2.5456032752990723\n",
      "Loss:  2.4734110832214355\n",
      "Loss:  2.586505651473999\n",
      "Loss:  2.479001522064209\n",
      "Loss:  2.4086036682128906\n",
      "Loss:  2.3976614475250244\n",
      "Loss:  2.4235339164733887\n",
      "Loss:  2.4270436763763428\n",
      "Loss:  2.5249545574188232\n",
      "Loss:  2.2751095294952393\n",
      "Loss:  2.547916889190674\n",
      "Loss:  2.6443583965301514\n",
      "Loss:  2.5245330333709717\n",
      "Loss:  2.6204073429107666\n",
      "Loss:  2.675593614578247\n",
      "Loss:  2.6877269744873047\n",
      "Loss:  1.5384037494659424\n",
      "     43        2.4665        2.5709  5.2063\n",
      "Loss:  2.517333745956421\n",
      "Loss:  2.677806854248047\n",
      "Loss:  2.594346761703491\n",
      "Loss:  2.4266128540039062\n",
      "Loss:  2.370328903198242\n",
      "Loss:  2.4082534313201904\n",
      "Loss:  2.4248907566070557\n",
      "Loss:  2.323204278945923\n",
      "Loss:  2.3370237350463867\n",
      "Loss:  2.393465518951416\n",
      "Loss:  2.381948471069336\n",
      "Loss:  2.4039371013641357\n",
      "Loss:  2.4541127681732178\n",
      "Loss:  2.3105928897857666\n",
      "Loss:  2.6281285285949707\n",
      "Loss:  2.4406590461730957\n",
      "Loss:  2.407621383666992\n",
      "Loss:  2.491769552230835\n",
      "Loss:  2.4469239711761475\n",
      "Loss:  2.4534082412719727\n",
      "Loss:  2.414712429046631\n",
      "Loss:  2.3916358947753906\n",
      "Loss:  2.3581204414367676\n",
      "Loss:  2.4980661869049072\n",
      "Loss:  2.3102076053619385\n",
      "Loss:  2.5178661346435547\n",
      "Loss:  2.6565022468566895\n",
      "Loss:  2.5113141536712646\n",
      "Loss:  2.799070119857788\n",
      "Loss:  2.8907105922698975\n",
      "Loss:  2.673783302307129\n",
      "Loss:  1.6952850818634033\n",
      "     44        \u001b[36m2.4349\u001b[0m        2.6332  5.2042\n",
      "Loss:  2.483102560043335\n",
      "Loss:  2.619371175765991\n",
      "Loss:  2.414119005203247\n",
      "Loss:  2.397185802459717\n",
      "Loss:  2.3597536087036133\n",
      "Loss:  2.413609504699707\n",
      "Loss:  2.4963138103485107\n",
      "Loss:  2.371288537979126\n",
      "Loss:  2.3350329399108887\n",
      "Loss:  2.3877384662628174\n",
      "Loss:  2.376673460006714\n",
      "Loss:  2.441988945007324\n",
      "Loss:  2.3908326625823975\n",
      "Loss:  2.276503801345825\n",
      "Loss:  2.5630290508270264\n",
      "Loss:  2.3911027908325195\n",
      "Loss:  2.372771978378296\n",
      "Loss:  2.4495575428009033\n",
      "Loss:  2.4101502895355225\n",
      "Loss:  2.3947460651397705\n",
      "Loss:  2.355301856994629\n",
      "Loss:  2.3311221599578857\n",
      "Loss:  2.3409640789031982\n",
      "Loss:  2.4494216442108154\n",
      "Loss:  2.2555480003356934\n",
      "Loss:  2.510084867477417\n",
      "Loss:  2.6451785564422607\n",
      "Loss:  2.511798143386841\n",
      "Loss:  2.731360912322998\n",
      "Loss:  2.807373046875\n",
      "Loss:  2.7017765045166016\n",
      "Loss:  1.615636944770813\n",
      "     45        \u001b[36m2.4035\u001b[0m        2.6072  5.2066\n",
      "Loss:  2.4832379817962646\n",
      "Loss:  2.529423713684082\n",
      "Loss:  2.3514602184295654\n",
      "Loss:  2.3972530364990234\n",
      "Loss:  2.3500518798828125\n",
      "Loss:  2.3608858585357666\n",
      "Loss:  2.3913965225219727\n",
      "Loss:  2.3491339683532715\n",
      "Loss:  2.304429292678833\n",
      "Loss:  2.3990535736083984\n",
      "Loss:  2.3571128845214844\n",
      "Loss:  2.360515832901001\n",
      "Loss:  2.3383889198303223\n",
      "Loss:  2.2740495204925537\n",
      "Loss:  2.625584840774536\n",
      "Loss:  2.4232327938079834\n",
      "Loss:  2.349137544631958\n",
      "Loss:  2.4132118225097656\n",
      "Loss:  2.404386520385742\n",
      "Loss:  2.4582126140594482\n",
      "Loss:  2.3773298263549805\n",
      "Loss:  2.3584628105163574\n",
      "Loss:  2.327785015106201\n",
      "Loss:  2.4775333404541016\n",
      "Loss:  2.2957537174224854\n",
      "Loss:  2.5300002098083496\n",
      "Loss:  2.6933746337890625\n",
      "Loss:  2.552642345428467\n",
      "Loss:  2.9139373302459717\n",
      "Loss:  2.9894728660583496\n",
      "Loss:  2.7513020038604736\n",
      "Loss:  1.8372762203216553\n",
      "     46        \u001b[36m2.3905\u001b[0m        2.7001  5.2541\n",
      "Loss:  2.476107120513916\n",
      "Loss:  2.5342884063720703\n",
      "Loss:  2.322117567062378\n",
      "Loss:  2.3815603256225586\n",
      "Loss:  2.337815761566162\n",
      "Loss:  2.343078374862671\n",
      "Loss:  2.4172232151031494\n",
      "Loss:  2.432722568511963\n",
      "Loss:  2.287862777709961\n",
      "Loss:  2.3711047172546387\n",
      "Loss:  2.3114755153656006\n",
      "Loss:  2.354458808898926\n",
      "Loss:  2.3760616779327393\n",
      "Loss:  2.2851874828338623\n",
      "Loss:  2.569631814956665\n",
      "Loss:  2.3718132972717285\n",
      "Loss:  2.326680898666382\n",
      "Loss:  2.4234261512756348\n",
      "Loss:  2.4207382202148438\n",
      "Loss:  2.3748555183410645\n",
      "Loss:  2.3350670337677\n",
      "Loss:  2.3851919174194336\n",
      "Loss:  2.3359696865081787\n",
      "Loss:  2.4552550315856934\n",
      "Loss:  2.2629430294036865\n",
      "Loss:  2.526630163192749\n",
      "Loss:  2.6756553649902344\n",
      "Loss:  2.558631420135498\n",
      "Loss:  2.812960624694824\n",
      "Loss:  2.827974796295166\n",
      "Loss:  2.7331008911132812\n",
      "Loss:  1.7140141725540161\n",
      "     47        \u001b[36m2.3800\u001b[0m        2.6477  5.2205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  2.471972703933716\n",
      "Loss:  2.4983561038970947\n",
      "Loss:  2.35562801361084\n",
      "Loss:  2.430964708328247\n",
      "Loss:  2.3822386264801025\n",
      "Loss:  2.3107645511627197\n",
      "Loss:  2.3756954669952393\n",
      "Loss:  2.546065330505371\n",
      "Loss:  2.348055362701416\n",
      "Loss:  2.412824869155884\n",
      "Loss:  2.3442580699920654\n",
      "Loss:  2.342463254928589\n",
      "Loss:  2.3679921627044678\n",
      "Loss:  2.3296244144439697\n",
      "Loss:  2.6773064136505127\n",
      "Loss:  2.388216972351074\n",
      "Loss:  2.3238489627838135\n",
      "Loss:  2.4049549102783203\n",
      "Loss:  2.4101974964141846\n",
      "Loss:  2.4013288021087646\n",
      "Loss:  2.3394715785980225\n",
      "Loss:  2.33068585395813\n",
      "Loss:  2.3113794326782227\n",
      "Loss:  2.435678720474243\n",
      "Loss:  2.226039409637451\n",
      "Loss:  2.5323097705841064\n",
      "Loss:  2.656672477722168\n",
      "Loss:  2.5301432609558105\n",
      "Loss:  2.800208568572998\n",
      "Loss:  2.8849570751190186\n",
      "Loss:  2.709252119064331\n",
      "Loss:  1.679050326347351\n",
      "     48        2.3911        2.6428  5.2045\n",
      "Loss:  2.4472672939300537\n",
      "Loss:  2.4837708473205566\n",
      "Loss:  2.3735251426696777\n",
      "Loss:  2.4069435596466064\n",
      "Loss:  2.375098705291748\n",
      "Loss:  2.3049123287200928\n",
      "Loss:  2.3208091259002686\n",
      "Loss:  2.319559335708618\n",
      "Loss:  2.2862424850463867\n",
      "Loss:  2.379462480545044\n",
      "Loss:  2.3672239780426025\n",
      "Loss:  2.4318180084228516\n",
      "Loss:  2.3676538467407227\n",
      "Loss:  2.3198094367980957\n",
      "Loss:  2.5951244831085205\n",
      "Loss:  2.379380226135254\n",
      "Loss:  2.402418851852417\n",
      "Loss:  2.621882200241089\n",
      "Loss:  2.505138635635376\n",
      "Loss:  2.3454411029815674\n",
      "Loss:  2.351384401321411\n",
      "Loss:  2.4509825706481934\n",
      "Loss:  2.434731960296631\n",
      "Loss:  2.518397092819214\n",
      "Loss:  2.236781120300293\n",
      "Loss:  2.5284595489501953\n",
      "Loss:  2.644019365310669\n",
      "Loss:  2.533595561981201\n",
      "Loss:  2.7858822345733643\n",
      "Loss:  2.835249423980713\n",
      "Loss:  2.7059879302978516\n",
      "Loss:  1.6953363418579102\n",
      "     49        2.4015        2.6306  5.2062\n",
      "Loss:  2.4495255947113037\n",
      "Loss:  2.543835401535034\n",
      "Loss:  2.503653049468994\n",
      "Loss:  2.4962515830993652\n",
      "Loss:  2.4166789054870605\n",
      "Loss:  2.298454523086548\n",
      "Loss:  2.316481590270996\n",
      "Loss:  2.315269947052002\n",
      "Loss:  2.3218300342559814\n",
      "Loss:  2.3799498081207275\n",
      "Loss:  2.3524794578552246\n",
      "Loss:  2.373192310333252\n",
      "Loss:  2.306659698486328\n",
      "Loss:  2.2496111392974854\n",
      "Loss:  2.557715892791748\n",
      "Loss:  2.3583059310913086\n",
      "Loss:  2.330894947052002\n",
      "Loss:  2.5381288528442383\n",
      "Loss:  2.558547258377075\n",
      "Loss:  2.3818068504333496\n",
      "Loss:  2.339587926864624\n",
      "Loss:  2.3510324954986572\n",
      "Loss:  2.3632900714874268\n",
      "Loss:  2.5432541370391846\n",
      "Loss:  2.347181558609009\n",
      "Loss:  2.690884590148926\n",
      "Loss:  2.731117010116577\n",
      "Loss:  2.590634822845459\n",
      "Loss:  2.6682841777801514\n",
      "Loss:  2.734320878982544\n",
      "Loss:  2.7016561031341553\n",
      "Loss:  1.619552731513977\n",
      "     50        2.3999        2.6408  5.2054\n",
      "Loss:  2.5747668743133545\n",
      "Loss:  2.589129686355591\n",
      "Loss:  2.42406964302063\n",
      "Loss:  2.4041311740875244\n",
      "Loss:  2.4844939708709717\n",
      "Loss:  2.505117893218994\n",
      "Loss:  2.583742380142212\n",
      "Loss:  2.3087713718414307\n",
      "Loss:  2.3532328605651855\n",
      "Loss:  2.3499832153320312\n",
      "Loss:  2.405848741531372\n",
      "Loss:  2.590846538543701\n",
      "Loss:  2.5250155925750732\n",
      "Loss:  2.442603349685669\n",
      "Loss:  2.5917394161224365\n",
      "Loss:  2.3876848220825195\n",
      "Loss:  2.3201611042022705\n",
      "Loss:  2.581307888031006\n",
      "Loss:  2.700652599334717\n",
      "Loss:  2.5683212280273438\n",
      "Loss:  2.441720724105835\n",
      "Loss:  2.345097541809082\n",
      "Loss:  2.320183277130127\n",
      "Loss:  2.486894369125366\n",
      "Loss:  2.3604023456573486\n",
      "Loss:  2.839284896850586\n",
      "Loss:  2.8826639652252197\n",
      "Loss:  2.6894869804382324\n",
      "Loss:  2.826525926589966\n",
      "Loss:  2.7868640422821045\n",
      "Loss:  2.7916808128356934\n",
      "Loss:  1.7494251728057861\n",
      "     51        2.4661        2.7579  5.2079\n",
      "Loss:  2.6628923416137695\n",
      "Loss:  2.576730966567993\n",
      "Loss:  2.5689408779144287\n",
      "Loss:  2.4954330921173096\n",
      "Loss:  2.364161491394043\n",
      "Loss:  2.357296943664551\n",
      "Loss:  2.4246439933776855\n",
      "Loss:  2.3713808059692383\n",
      "Loss:  2.5388758182525635\n",
      "Loss:  2.5663363933563232\n",
      "Loss:  2.36716365814209\n",
      "Loss:  2.3993594646453857\n",
      "Loss:  2.341407299041748\n",
      "Loss:  2.3945729732513428\n",
      "Loss:  2.785447120666504\n",
      "Loss:  2.76115345954895\n",
      "Loss:  2.598541021347046\n",
      "Loss:  2.477956771850586\n",
      "Loss:  2.464369297027588\n",
      "Loss:  2.386779546737671\n",
      "Loss:  2.5156474113464355\n",
      "Loss:  2.559474468231201\n",
      "Loss:  2.611333131790161\n",
      "Loss:  2.6052258014678955\n",
      "Loss:  2.2570037841796875\n",
      "Loss:  2.5690455436706543\n",
      "Loss:  2.6440885066986084\n",
      "Loss:  2.518883228302002\n",
      "Loss:  2.608086585998535\n",
      "Loss:  2.683793067932129\n",
      "Loss:  2.6014950275421143\n",
      "Loss:  1.5453059673309326\n",
      "     52        2.4987        2.5592  5.2533\n",
      "Loss:  2.493495225906372\n",
      "Loss:  2.4965717792510986\n",
      "Loss:  2.4606778621673584\n",
      "Loss:  2.598327875137329\n",
      "Loss:  2.47084379196167\n",
      "Loss:  2.380089282989502\n",
      "Loss:  2.4124815464019775\n",
      "Loss:  2.279475450515747\n",
      "Loss:  2.3356611728668213\n",
      "Loss:  2.3718535900115967\n",
      "Loss:  2.402625560760498\n",
      "Loss:  2.446432590484619\n",
      "Loss:  2.4809165000915527\n",
      "Loss:  2.4028239250183105\n",
      "Loss:  2.5592782497406006\n",
      "Loss:  2.410443067550659\n",
      "Loss:  2.4107015132904053\n",
      "Loss:  2.470367193222046\n",
      "Loss:  2.4903218746185303\n",
      "Loss:  2.4826507568359375\n",
      "Loss:  2.3461740016937256\n",
      "Loss:  2.3212790489196777\n",
      "Loss:  2.3665053844451904\n",
      "Loss:  2.514916181564331\n",
      "Loss:  2.288259267807007\n",
      "Loss:  2.536194086074829\n",
      "Loss:  2.6720685958862305\n",
      "Loss:  2.5225770473480225\n",
      "Loss:  2.840007781982422\n",
      "Loss:  2.8757481575012207\n",
      "Loss:  2.606086015701294\n",
      "Loss:  1.7477494478225708\n",
      "     53        2.4281        2.6360  5.2218\n",
      "Loss:  2.4715211391448975\n",
      "Loss:  2.525078773498535\n",
      "Loss:  2.3507981300354004\n",
      "Loss:  2.3722898960113525\n",
      "Loss:  2.330662965774536\n",
      "Loss:  2.3301124572753906\n",
      "Loss:  2.386525869369507\n",
      "Loss:  2.3166496753692627\n",
      "Loss:  2.324357271194458\n",
      "Loss:  2.3906047344207764\n",
      "Loss:  2.3017544746398926\n",
      "Loss:  2.307682991027832\n",
      "Loss:  2.316171646118164\n",
      "Loss:  2.2428555488586426\n",
      "Loss:  2.5630879402160645\n",
      "Loss:  2.3872201442718506\n",
      "Loss:  2.3302605152130127\n",
      "Loss:  2.370725154876709\n",
      "Loss:  2.373227119445801\n",
      "Loss:  2.3418874740600586\n",
      "Loss:  2.320805788040161\n",
      "Loss:  2.3120782375335693\n",
      "Loss:  2.302816867828369\n",
      "Loss:  2.384976863861084\n",
      "Loss:  2.203089714050293\n",
      "Loss:  2.5225753784179688\n",
      "Loss:  2.6506338119506836\n",
      "Loss:  2.5094833374023438\n",
      "Loss:  2.7708187103271484\n",
      "Loss:  2.8127200603485107\n",
      "Loss:  2.6481614112854004\n",
      "Loss:  1.6287707090377808\n",
      "     54        \u001b[36m2.3547\u001b[0m        2.6088  5.2038\n",
      "Loss:  2.44931960105896\n",
      "Loss:  2.462463140487671\n",
      "Loss:  2.315338611602783\n",
      "Loss:  2.347245216369629\n",
      "Loss:  2.3310537338256836\n",
      "Loss:  2.2736480236053467\n",
      "Loss:  2.306283712387085\n",
      "Loss:  2.257766008377075\n",
      "Loss:  2.2460880279541016\n",
      "Loss:  2.330280065536499\n",
      "Loss:  2.2985448837280273\n",
      "Loss:  2.284235715866089\n",
      "Loss:  2.304161787033081\n",
      "Loss:  2.210564374923706\n",
      "Loss:  2.5078353881835938\n",
      "Loss:  2.3521728515625\n",
      "Loss:  2.3108694553375244\n",
      "Loss:  2.3569414615631104\n",
      "Loss:  2.3648359775543213\n",
      "Loss:  2.297098398208618\n",
      "Loss:  2.2999958992004395\n",
      "Loss:  2.273155927658081\n",
      "Loss:  2.28669810295105\n",
      "Loss:  2.351206064224243\n",
      "Loss:  2.1818652153015137\n",
      "Loss:  2.558274030685425\n",
      "Loss:  2.6426775455474854\n",
      "Loss:  2.5323984622955322\n",
      "Loss:  2.715435028076172\n",
      "Loss:  2.7890851497650146\n",
      "Loss:  2.644360303878784\n",
      "Loss:  1.5977833271026611\n",
      "     55        \u001b[36m2.3204\u001b[0m        2.6024  5.2084\n",
      "Loss:  2.4498634338378906\n",
      "Loss:  2.4403886795043945\n",
      "Loss:  2.2890474796295166\n",
      "Loss:  2.316649913787842\n",
      "Loss:  2.3009791374206543\n",
      "Loss:  2.245372772216797\n",
      "Loss:  2.2804782390594482\n",
      "Loss:  2.2299468517303467\n",
      "Loss:  2.233299732208252\n",
      "Loss:  2.309688091278076\n",
      "Loss:  2.2686450481414795\n",
      "Loss:  2.2689831256866455\n",
      "Loss:  2.286856174468994\n",
      "Loss:  2.20876145362854\n",
      "Loss:  2.49967098236084\n",
      "Loss:  2.3652141094207764\n",
      "Loss:  2.302095413208008\n",
      "Loss:  2.334754467010498\n",
      "Loss:  2.3476788997650146\n",
      "Loss:  2.2877907752990723\n",
      "Loss:  2.3000080585479736\n",
      "Loss:  2.263406753540039\n",
      "Loss:  2.291229724884033\n",
      "Loss:  2.341092586517334\n",
      "Loss:  2.1668202877044678\n",
      "Loss:  2.57161808013916\n",
      "Loss:  2.645266532897949\n",
      "Loss:  2.527308940887451\n",
      "Loss:  2.7169039249420166\n",
      "Loss:  2.804577589035034\n",
      "Loss:  2.621195077896118\n",
      "Loss:  1.5889935493469238\n",
      "     56        \u001b[36m2.3055\u001b[0m        2.6028  5.2061\n",
      "Loss:  2.4391777515411377\n",
      "Loss:  2.471632242202759\n",
      "Loss:  2.2715322971343994\n",
      "Loss:  2.311767339706421\n",
      "Loss:  2.2771153450012207\n",
      "Loss:  2.2294797897338867\n",
      "Loss:  2.264712333679199\n",
      "Loss:  2.2095401287078857\n",
      "Loss:  2.2291829586029053\n",
      "Loss:  2.3045709133148193\n",
      "Loss:  2.253941774368286\n",
      "Loss:  2.245137929916382\n",
      "Loss:  2.2843542098999023\n",
      "Loss:  2.1964192390441895\n",
      "Loss:  2.4737398624420166\n",
      "Loss:  2.3697400093078613\n",
      "Loss:  2.298114776611328\n",
      "Loss:  2.3198769092559814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  2.3358137607574463\n",
      "Loss:  2.275320291519165\n",
      "Loss:  2.2978718280792236\n",
      "Loss:  2.259068250656128\n",
      "Loss:  2.2853996753692627\n",
      "Loss:  2.3258161544799805\n",
      "Loss:  2.1587040424346924\n",
      "Loss:  2.557488441467285\n",
      "Loss:  2.6592037677764893\n",
      "Loss:  2.5262091159820557\n",
      "Loss:  2.756732225418091\n",
      "Loss:  2.825770854949951\n",
      "Loss:  2.6317696571350098\n",
      "Loss:  1.6098250150680542\n",
      "     57        \u001b[36m2.2959\u001b[0m        2.6149  5.2021\n",
      "Loss:  2.4325966835021973\n",
      "Loss:  2.4159746170043945\n",
      "Loss:  2.2599587440490723\n",
      "Loss:  2.3060524463653564\n",
      "Loss:  2.2609312534332275\n",
      "Loss:  2.23325777053833\n",
      "Loss:  2.2669477462768555\n",
      "Loss:  2.2120723724365234\n",
      "Loss:  2.2345149517059326\n",
      "Loss:  2.2979257106781006\n",
      "Loss:  2.2456564903259277\n",
      "Loss:  2.2326409816741943\n",
      "Loss:  2.2842984199523926\n",
      "Loss:  2.1960153579711914\n",
      "Loss:  2.4671802520751953\n",
      "Loss:  2.3770394325256348\n",
      "Loss:  2.327894687652588\n",
      "Loss:  2.3104500770568848\n",
      "Loss:  2.322841167449951\n",
      "Loss:  2.271162748336792\n",
      "Loss:  2.287431478500366\n",
      "Loss:  2.2524163722991943\n",
      "Loss:  2.293501377105713\n",
      "Loss:  2.321578025817871\n",
      "Loss:  2.1607553958892822\n",
      "Loss:  2.596550941467285\n",
      "Loss:  2.6633894443511963\n",
      "Loss:  2.51971173286438\n",
      "Loss:  2.7655575275421143\n",
      "Loss:  2.863476037979126\n",
      "Loss:  2.6180574893951416\n",
      "Loss:  1.5980669260025024\n",
      "     58        \u001b[36m2.2912\u001b[0m        2.6255  5.2742\n",
      "Loss:  2.420409917831421\n",
      "Loss:  2.402937173843384\n",
      "Loss:  2.2491586208343506\n",
      "Loss:  2.297032594680786\n",
      "Loss:  2.2531824111938477\n",
      "Loss:  2.2443244457244873\n",
      "Loss:  2.271284580230713\n",
      "Loss:  2.220916986465454\n",
      "Loss:  2.233217239379883\n",
      "Loss:  2.2933030128479004\n",
      "Loss:  2.23865008354187\n",
      "Loss:  2.2211554050445557\n",
      "Loss:  2.2900125980377197\n",
      "Loss:  2.217068672180176\n",
      "Loss:  2.4637279510498047\n",
      "Loss:  2.389591932296753\n",
      "Loss:  2.3486735820770264\n",
      "Loss:  2.29413104057312\n",
      "Loss:  2.326051712036133\n",
      "Loss:  2.2849786281585693\n",
      "Loss:  2.2847542762756348\n",
      "Loss:  2.25250506401062\n",
      "Loss:  2.289252996444702\n",
      "Loss:  2.320223808288574\n",
      "Loss:  2.1683509349823\n",
      "Loss:  2.5459201335906982\n",
      "Loss:  2.686872720718384\n",
      "Loss:  2.5425357818603516\n",
      "Loss:  2.8966474533081055\n",
      "Loss:  2.910128593444824\n",
      "Loss:  2.629500389099121\n",
      "Loss:  1.7032862901687622\n",
      "     59        2.2913        2.6594  5.2056\n",
      "Loss:  2.4263336658477783\n",
      "Loss:  2.4283978939056396\n",
      "Loss:  2.2430286407470703\n",
      "Loss:  2.280458927154541\n",
      "Loss:  2.233696699142456\n",
      "Loss:  2.2783894538879395\n",
      "Loss:  2.314971923828125\n",
      "Loss:  2.3005082607269287\n",
      "Loss:  2.282118558883667\n",
      "Loss:  2.3040273189544678\n",
      "Loss:  2.2371039390563965\n",
      "Loss:  2.238698959350586\n",
      "Loss:  2.3075506687164307\n",
      "Loss:  2.2331626415252686\n",
      "Loss:  2.5030932426452637\n",
      "Loss:  2.421710968017578\n",
      "Loss:  2.3606016635894775\n",
      "Loss:  2.293558359146118\n",
      "Loss:  2.324129819869995\n",
      "Loss:  2.285621404647827\n",
      "Loss:  2.279733419418335\n",
      "Loss:  2.272390842437744\n",
      "Loss:  2.29787540435791\n",
      "Loss:  2.3089945316314697\n",
      "Loss:  2.173771858215332\n",
      "Loss:  2.6013879776000977\n",
      "Loss:  2.70255184173584\n",
      "Loss:  2.542048931121826\n",
      "Loss:  2.834287643432617\n",
      "Loss:  2.8811209201812744\n",
      "Loss:  2.606619119644165\n",
      "Loss:  1.6793091297149658\n",
      "     60        2.3055        2.6515  5.2089\n",
      "Loss:  2.4064178466796875\n",
      "Loss:  2.417999505996704\n",
      "Loss:  2.239670515060425\n",
      "Loss:  2.2880656719207764\n",
      "Loss:  2.2380928993225098\n",
      "Loss:  2.277353525161743\n",
      "Loss:  2.288158893585205\n",
      "Loss:  2.3688108921051025\n",
      "Loss:  2.3002941608428955\n",
      "Loss:  2.315884828567505\n",
      "Loss:  2.254617214202881\n",
      "Loss:  2.2636845111846924\n",
      "Loss:  2.3021280765533447\n",
      "Loss:  2.2434699535369873\n",
      "Loss:  2.5379133224487305\n",
      "Loss:  2.405911445617676\n",
      "Loss:  2.3242878913879395\n",
      "Loss:  2.2827951908111572\n",
      "Loss:  2.309631586074829\n",
      "Loss:  2.2922472953796387\n",
      "Loss:  2.309291124343872\n",
      "Loss:  2.3368446826934814\n",
      "Loss:  2.2927441596984863\n",
      "Loss:  2.281212329864502\n",
      "Loss:  2.1611673831939697\n",
      "Loss:  2.6053106784820557\n",
      "Loss:  2.6987502574920654\n",
      "Loss:  2.588894844055176\n",
      "Loss:  2.9395933151245117\n",
      "Loss:  2.972903251647949\n",
      "Loss:  2.6167821884155273\n",
      "Loss:  1.8505113124847412\n",
      "     61        2.3099        2.6993  5.2070\n",
      "Loss:  2.4175570011138916\n",
      "Loss:  2.470586061477661\n",
      "Loss:  2.3224165439605713\n",
      "Loss:  2.3319778442382812\n",
      "Loss:  2.2751405239105225\n",
      "Loss:  2.2971272468566895\n",
      "Loss:  2.3199827671051025\n",
      "Loss:  2.2978973388671875\n",
      "Loss:  2.3684487342834473\n",
      "Loss:  2.346287965774536\n",
      "Loss:  2.279449224472046\n",
      "Loss:  2.2746286392211914\n",
      "Loss:  2.2879536151885986\n",
      "Loss:  2.2154054641723633\n",
      "Loss:  2.533777952194214\n",
      "Loss:  2.3822646141052246\n",
      "Loss:  2.3183248043060303\n",
      "Loss:  2.337341785430908\n",
      "Loss:  2.3320419788360596\n",
      "Loss:  2.263547897338867\n",
      "Loss:  2.2817611694335938\n",
      "Loss:  2.3019864559173584\n",
      "Loss:  2.315368890762329\n",
      "Loss:  2.32494854927063\n",
      "Loss:  2.1395440101623535\n",
      "Loss:  2.5712573528289795\n",
      "Loss:  2.7011914253234863\n",
      "Loss:  2.53373384475708\n",
      "Loss:  2.7606494426727295\n",
      "Loss:  2.8156192302703857\n",
      "Loss:  2.642610549926758\n",
      "Loss:  1.6437115669250488\n",
      "     62        2.3219        2.6271  5.2084\n",
      "Loss:  2.410682201385498\n",
      "Loss:  2.424931287765503\n",
      "Loss:  2.2660183906555176\n",
      "Loss:  2.3818020820617676\n",
      "Loss:  2.397878885269165\n",
      "Loss:  2.2912378311157227\n",
      "Loss:  2.249197483062744\n",
      "Loss:  2.225454092025757\n",
      "Loss:  2.2364795207977295\n",
      "Loss:  2.4009299278259277\n",
      "Loss:  2.353092908859253\n",
      "Loss:  2.408414840698242\n",
      "Loss:  2.3221724033355713\n",
      "Loss:  2.1921842098236084\n",
      "Loss:  2.494133472442627\n",
      "Loss:  2.369230031967163\n",
      "Loss:  2.3512744903564453\n",
      "Loss:  2.413327693939209\n",
      "Loss:  2.4157817363739014\n",
      "Loss:  2.2821125984191895\n",
      "Loss:  2.280665159225464\n",
      "Loss:  2.276298761367798\n",
      "Loss:  2.300616502761841\n",
      "Loss:  2.398465156555176\n",
      "Loss:  2.188312530517578\n",
      "Loss:  2.615717649459839\n",
      "Loss:  2.6928822994232178\n",
      "Loss:  2.5874125957489014\n",
      "Loss:  2.7269811630249023\n",
      "Loss:  2.74385404586792\n",
      "Loss:  2.6681787967681885\n",
      "Loss:  1.6765309572219849\n",
      "     63        2.3336        2.6301  5.2028\n",
      "Loss:  2.455881357192993\n",
      "Loss:  2.3882877826690674\n",
      "Loss:  2.2237727642059326\n",
      "Loss:  2.3070552349090576\n",
      "Loss:  2.329307794570923\n",
      "Loss:  2.3275861740112305\n",
      "Loss:  2.3313634395599365\n",
      "Loss:  2.1989665031433105\n",
      "Loss:  2.2347559928894043\n",
      "Loss:  2.333082675933838\n",
      "Loss:  2.2951579093933105\n",
      "Loss:  2.421379566192627\n",
      "Loss:  2.429800033569336\n",
      "Loss:  2.3299736976623535\n",
      "Loss:  2.5740044116973877\n",
      "Loss:  2.4314451217651367\n",
      "Loss:  2.277156114578247\n",
      "Loss:  2.454972982406616\n",
      "Loss:  2.6326539516448975\n",
      "Loss:  2.5403318405151367\n",
      "Loss:  2.427929162979126\n",
      "Loss:  2.2740230560302734\n",
      "Loss:  2.2513926029205322\n",
      "Loss:  2.3898675441741943\n",
      "Loss:  2.25913143157959\n",
      "Loss:  2.8770487308502197\n",
      "Loss:  2.951042652130127\n",
      "Loss:  2.72532320022583\n",
      "Loss:  2.8933708667755127\n",
      "Loss:  2.876927137374878\n",
      "Loss:  2.816864490509033\n",
      "Loss:  1.8173781633377075\n",
      "     64        2.3651        2.8125  5.2561\n",
      "Loss:  2.599065065383911\n",
      "Loss:  2.5382978916168213\n",
      "Loss:  2.4434385299682617\n",
      "Loss:  2.3374719619750977\n",
      "Loss:  2.277884006500244\n",
      "Loss:  2.2962167263031006\n",
      "Loss:  2.4112401008605957\n",
      "Loss:  2.3585867881774902\n",
      "Loss:  2.4590840339660645\n",
      "Loss:  2.3560903072357178\n",
      "Loss:  2.288839340209961\n",
      "Loss:  2.3029348850250244\n",
      "Loss:  2.341996431350708\n",
      "Loss:  2.267916440963745\n",
      "Loss:  2.7216033935546875\n",
      "Loss:  2.592050790786743\n",
      "Loss:  2.4272818565368652\n",
      "Loss:  2.3302390575408936\n",
      "Loss:  2.3567066192626953\n",
      "Loss:  2.353304147720337\n",
      "Loss:  2.5346028804779053\n",
      "Loss:  2.4899191856384277\n",
      "Loss:  2.5043375492095947\n",
      "Loss:  2.4251210689544678\n",
      "Loss:  2.152567148208618\n",
      "Loss:  2.643941879272461\n",
      "Loss:  2.690613269805908\n",
      "Loss:  2.5544140338897705\n",
      "Loss:  2.6795105934143066\n",
      "Loss:  2.7716193199157715\n",
      "Loss:  2.614821195602417\n",
      "Loss:  1.6146372556686401\n",
      "     65        2.4074        2.6147  5.2229\n",
      "Loss:  2.467360258102417\n",
      "Loss:  2.4212658405303955\n",
      "Loss:  2.4246320724487305\n",
      "Loss:  2.5677859783172607\n",
      "Loss:  2.4020817279815674\n",
      "Loss:  2.283397912979126\n",
      "Loss:  2.2831318378448486\n",
      "Loss:  2.2016634941101074\n",
      "Loss:  2.299140691757202\n",
      "Loss:  2.3592135906219482\n",
      "Loss:  2.4350485801696777\n",
      "Loss:  2.3896336555480957\n",
      "Loss:  2.4143917560577393\n",
      "Loss:  2.3079211711883545\n",
      "Loss:  2.4985039234161377\n",
      "Loss:  2.4155025482177734\n",
      "Loss:  2.435413122177124\n",
      "Loss:  2.516085386276245\n",
      "Loss:  2.5073938369750977\n",
      "Loss:  2.4551963806152344\n",
      "Loss:  2.29931378364563\n",
      "Loss:  2.261326313018799\n",
      "Loss:  2.3893535137176514\n",
      "Loss:  2.4866859912872314\n",
      "Loss:  2.3099308013916016\n",
      "Loss:  2.6278812885284424\n",
      "Loss:  2.755854845046997\n",
      "Loss:  2.570345163345337\n",
      "Loss:  2.953519105911255\n",
      "Loss:  3.0254745483398438\n",
      "Loss:  2.59614634513855\n",
      "Loss:  1.8556195497512817\n",
      "     66        2.3935        2.7166  5.2033\n",
      "Loss:  2.4690444469451904\n",
      "Loss:  2.5251176357269287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  2.2939279079437256\n",
      "Loss:  2.2887935638427734\n",
      "Loss:  2.2831716537475586\n",
      "Loss:  2.3284740447998047\n",
      "Loss:  2.414970636367798\n",
      "Loss:  2.3177194595336914\n",
      "Loss:  2.297443389892578\n",
      "Loss:  2.3529646396636963\n",
      "Loss:  2.262651205062866\n",
      "Loss:  2.2787861824035645\n",
      "Loss:  2.3341758251190186\n",
      "Loss:  2.2800092697143555\n",
      "Loss:  2.580751419067383\n",
      "Loss:  2.4238154888153076\n",
      "Loss:  2.3258414268493652\n",
      "Loss:  2.289158821105957\n",
      "Loss:  2.3128814697265625\n",
      "Loss:  2.31844162940979\n",
      "Loss:  2.3215270042419434\n",
      "Loss:  2.3079802989959717\n",
      "Loss:  2.2593071460723877\n",
      "Loss:  2.2742691040039062\n",
      "Loss:  2.1724159717559814\n",
      "Loss:  2.570955514907837\n",
      "Loss:  2.681983470916748\n",
      "Loss:  2.54069185256958\n",
      "Loss:  2.8854355812072754\n",
      "Loss:  2.9272143840789795\n",
      "Loss:  2.627913236618042\n",
      "Loss:  1.716869592666626\n",
      "     67        2.3330        2.6636  5.2037\n",
      "Loss:  2.424791097640991\n",
      "Loss:  2.4593286514282227\n",
      "Loss:  2.2678394317626953\n",
      "Loss:  2.3326025009155273\n",
      "Loss:  2.27156925201416\n",
      "Loss:  2.2174084186553955\n",
      "Loss:  2.2611470222473145\n",
      "Loss:  2.2245540618896484\n",
      "Loss:  2.223090171813965\n",
      "Loss:  2.313476800918579\n",
      "Loss:  2.2735612392425537\n",
      "Loss:  2.2437186241149902\n",
      "Loss:  2.2917816638946533\n",
      "Loss:  2.192509412765503\n",
      "Loss:  2.4517176151275635\n",
      "Loss:  2.3438756465911865\n",
      "Loss:  2.2933804988861084\n",
      "Loss:  2.275315523147583\n",
      "Loss:  2.3081185817718506\n",
      "Loss:  2.249690055847168\n",
      "Loss:  2.258326292037964\n",
      "Loss:  2.2261829376220703\n",
      "Loss:  2.250750780105591\n",
      "Loss:  2.2495064735412598\n",
      "Loss:  2.1181089878082275\n",
      "Loss:  2.5809643268585205\n",
      "Loss:  2.6653363704681396\n",
      "Loss:  2.558265447616577\n",
      "Loss:  2.8041727542877197\n",
      "Loss:  2.8519253730773926\n",
      "Loss:  2.6344666481018066\n",
      "Loss:  1.6418063640594482\n",
      "     68        \u001b[36m2.2813\u001b[0m        2.6382  5.2045\n",
      "Loss:  2.411001443862915\n",
      "Loss:  2.390315294265747\n",
      "Loss:  2.219172239303589\n",
      "Loss:  2.2584447860717773\n",
      "Loss:  2.2281360626220703\n",
      "Loss:  2.1878206729888916\n",
      "Loss:  2.218216896057129\n",
      "Loss:  2.173410177230835\n",
      "Loss:  2.1977217197418213\n",
      "Loss:  2.281379461288452\n",
      "Loss:  2.239325523376465\n",
      "Loss:  2.224843740463257\n",
      "Loss:  2.282292604446411\n",
      "Loss:  2.184020757675171\n",
      "Loss:  2.444744825363159\n",
      "Loss:  2.3522136211395264\n",
      "Loss:  2.2761199474334717\n",
      "Loss:  2.263575792312622\n",
      "Loss:  2.290256977081299\n",
      "Loss:  2.2360422611236572\n",
      "Loss:  2.2660679817199707\n",
      "Loss:  2.2236430644989014\n",
      "Loss:  2.2421603202819824\n",
      "Loss:  2.2309677600860596\n",
      "Loss:  2.095766544342041\n",
      "Loss:  2.6071465015411377\n",
      "Loss:  2.6839213371276855\n",
      "Loss:  2.576589822769165\n",
      "Loss:  2.8378710746765137\n",
      "Loss:  2.872734546661377\n",
      "Loss:  2.6376006603240967\n",
      "Loss:  1.6465855836868286\n",
      "     69        \u001b[36m2.2571\u001b[0m        2.6577  5.2031\n",
      "Loss:  2.413395881652832\n",
      "Loss:  2.3715975284576416\n",
      "Loss:  2.2026190757751465\n",
      "Loss:  2.2506401538848877\n",
      "Loss:  2.2203729152679443\n",
      "Loss:  2.1796789169311523\n",
      "Loss:  2.2182743549346924\n",
      "Loss:  2.165163278579712\n",
      "Loss:  2.1926817893981934\n",
      "Loss:  2.272674083709717\n",
      "Loss:  2.2262749671936035\n",
      "Loss:  2.19392728805542\n",
      "Loss:  2.2735085487365723\n",
      "Loss:  2.162853479385376\n",
      "Loss:  2.4188358783721924\n",
      "Loss:  2.3594610691070557\n",
      "Loss:  2.292130708694458\n",
      "Loss:  2.258887529373169\n",
      "Loss:  2.2843923568725586\n",
      "Loss:  2.2238974571228027\n",
      "Loss:  2.2757344245910645\n",
      "Loss:  2.2241733074188232\n",
      "Loss:  2.2707982063293457\n",
      "Loss:  2.249847650527954\n",
      "Loss:  2.103471279144287\n",
      "Loss:  2.615562915802002\n",
      "Loss:  2.6864776611328125\n",
      "Loss:  2.5785751342773438\n",
      "Loss:  2.9359285831451416\n",
      "Loss:  2.9415509700775146\n",
      "Loss:  2.6189262866973877\n",
      "Loss:  1.694736123085022\n",
      "     70        \u001b[36m2.2526\u001b[0m        2.6855  5.2251\n",
      "Loss:  2.3996918201446533\n",
      "Loss:  2.3668627738952637\n",
      "Loss:  2.191080093383789\n",
      "Loss:  2.2411048412323\n",
      "Loss:  2.216256618499756\n",
      "Loss:  2.2028589248657227\n",
      "Loss:  2.2369229793548584\n",
      "Loss:  2.1577184200286865\n",
      "Loss:  2.1952056884765625\n",
      "Loss:  2.2691187858581543\n",
      "Loss:  2.2212913036346436\n",
      "Loss:  2.1967344284057617\n",
      "Loss:  2.2928688526153564\n",
      "Loss:  2.183744192123413\n",
      "Loss:  2.4067907333374023\n",
      "Loss:  2.33743953704834\n",
      "Loss:  2.2961554527282715\n",
      "Loss:  2.2528977394104004\n",
      "Loss:  2.2828633785247803\n",
      "Loss:  2.237644672393799\n",
      "Loss:  2.297258138656616\n",
      "Loss:  2.2189767360687256\n",
      "Loss:  2.2394778728485107\n",
      "Loss:  2.233476400375366\n",
      "Loss:  2.1050543785095215\n",
      "Loss:  2.6360037326812744\n",
      "Loss:  2.702786684036255\n",
      "Loss:  2.5705854892730713\n",
      "Loss:  2.896094560623169\n",
      "Loss:  2.987917900085449\n",
      "Loss:  2.623154401779175\n",
      "Loss:  1.673995018005371\n",
      "     71        \u001b[36m2.2516\u001b[0m        2.6909  5.2510\n",
      "Loss:  2.399401903152466\n",
      "Loss:  2.405458688735962\n",
      "Loss:  2.202052593231201\n",
      "Loss:  2.2458109855651855\n",
      "Loss:  2.2075345516204834\n",
      "Loss:  2.224073648452759\n",
      "Loss:  2.2487475872039795\n",
      "Loss:  2.1849889755249023\n",
      "Loss:  2.1981844902038574\n",
      "Loss:  2.2875070571899414\n",
      "Loss:  2.2232911586761475\n",
      "Loss:  2.187376022338867\n",
      "Loss:  2.294416666030884\n",
      "Loss:  2.215852737426758\n",
      "Loss:  2.4311580657958984\n",
      "Loss:  2.344975233078003\n",
      "Loss:  2.2897825241088867\n",
      "Loss:  2.2583882808685303\n",
      "Loss:  2.3147025108337402\n",
      "Loss:  2.2609431743621826\n",
      "Loss:  2.2934136390686035\n",
      "Loss:  2.2470457553863525\n",
      "Loss:  2.2289674282073975\n",
      "Loss:  2.226701259613037\n",
      "Loss:  2.109934091567993\n",
      "Loss:  2.6996986865997314\n",
      "Loss:  2.795372724533081\n",
      "Loss:  2.6610817909240723\n",
      "Loss:  3.058580160140991\n",
      "Loss:  3.0326449871063232\n",
      "Loss:  2.6876654624938965\n",
      "Loss:  1.854205846786499\n",
      "     72        2.2616        2.7813  5.2043\n",
      "Loss:  2.406578540802002\n",
      "Loss:  2.4232864379882812\n",
      "Loss:  2.2464101314544678\n",
      "Loss:  2.2934327125549316\n",
      "Loss:  2.2490925788879395\n",
      "Loss:  2.2094321250915527\n",
      "Loss:  2.224346876144409\n",
      "Loss:  2.186594009399414\n",
      "Loss:  2.2058768272399902\n",
      "Loss:  2.3565080165863037\n",
      "Loss:  2.2878565788269043\n",
      "Loss:  2.215632915496826\n",
      "Loss:  2.2864553928375244\n",
      "Loss:  2.2516274452209473\n",
      "Loss:  2.459662675857544\n",
      "Loss:  2.3750343322753906\n",
      "Loss:  2.3218777179718018\n",
      "Loss:  2.2838592529296875\n",
      "Loss:  2.3062314987182617\n",
      "Loss:  2.2170016765594482\n",
      "Loss:  2.2741246223449707\n",
      "Loss:  2.281987190246582\n",
      "Loss:  2.267505407333374\n",
      "Loss:  2.2504265308380127\n",
      "Loss:  2.114196300506592\n",
      "Loss:  2.6874117851257324\n",
      "Loss:  2.6724114418029785\n",
      "Loss:  2.566424608230591\n",
      "Loss:  2.8136773109436035\n",
      "Loss:  2.957184314727783\n",
      "Loss:  2.623551368713379\n",
      "Loss:  1.679567813873291\n",
      "     73        2.2802        2.6758  5.2073\n",
      "Loss:  2.396641731262207\n",
      "Loss:  2.4094834327697754\n",
      "Loss:  2.3100650310516357\n",
      "Loss:  2.3251023292541504\n",
      "Loss:  2.2336673736572266\n",
      "Loss:  2.1966712474823\n",
      "Loss:  2.21894907951355\n",
      "Loss:  2.1771962642669678\n",
      "Loss:  2.2056851387023926\n",
      "Loss:  2.392179012298584\n",
      "Loss:  2.2457687854766846\n",
      "Loss:  2.264195203781128\n",
      "Loss:  2.3063464164733887\n",
      "Loss:  2.265828847885132\n",
      "Loss:  2.449190378189087\n",
      "Loss:  2.3481221199035645\n",
      "Loss:  2.2681362628936768\n",
      "Loss:  2.292132616043091\n",
      "Loss:  2.3555941581726074\n",
      "Loss:  2.2519679069519043\n",
      "Loss:  2.258403778076172\n",
      "Loss:  2.2469301223754883\n",
      "Loss:  2.2541165351867676\n",
      "Loss:  2.266836404800415\n",
      "Loss:  2.127356767654419\n",
      "Loss:  2.658137798309326\n",
      "Loss:  2.7304539680480957\n",
      "Loss:  2.6185290813446045\n",
      "Loss:  2.803209066390991\n",
      "Loss:  2.817079782485962\n",
      "Loss:  2.703972578048706\n",
      "Loss:  1.7055251598358154\n",
      "     74        2.2831        2.6786  5.2109\n",
      "Loss:  2.4357619285583496\n",
      "Loss:  2.3891756534576416\n",
      "Loss:  2.250047445297241\n",
      "Loss:  2.2826051712036133\n",
      "Loss:  2.2461628913879395\n",
      "Loss:  2.2866809368133545\n",
      "Loss:  2.350531816482544\n",
      "Loss:  2.2063651084899902\n",
      "Loss:  2.253479480743408\n",
      "Loss:  2.339479684829712\n",
      "Loss:  2.2579846382141113\n",
      "Loss:  2.3287529945373535\n",
      "Loss:  2.3645613193511963\n",
      "Loss:  2.2829666137695312\n",
      "Loss:  2.4895589351654053\n",
      "Loss:  2.426755666732788\n",
      "Loss:  2.281205654144287\n",
      "Loss:  2.2730307579040527\n",
      "Loss:  2.361517906188965\n",
      "Loss:  2.297837734222412\n",
      "Loss:  2.343205213546753\n",
      "Loss:  2.2748661041259766\n",
      "Loss:  2.238015651702881\n",
      "Loss:  2.2326560020446777\n",
      "Loss:  2.1134560108184814\n",
      "Loss:  2.727431297302246\n",
      "Loss:  2.7595067024230957\n",
      "Loss:  2.6095924377441406\n",
      "Loss:  2.732584238052368\n",
      "Loss:  2.821802854537964\n",
      "Loss:  2.658139705657959\n",
      "Loss:  1.751916527748108\n",
      "     75        2.3048        2.6771  5.2035\n",
      "Loss:  2.4660654067993164\n",
      "Loss:  2.410240650177002\n",
      "Loss:  2.2922492027282715\n",
      "Loss:  2.2695300579071045\n",
      "Loss:  2.223356008529663\n",
      "Loss:  2.1797642707824707\n",
      "Loss:  2.2343969345092773\n",
      "Loss:  2.212312936782837\n",
      "Loss:  2.338693857192993\n",
      "Loss:  2.3583998680114746\n",
      "Loss:  2.261434316635132\n",
      "Loss:  2.2167654037475586\n",
      "Loss:  2.2782390117645264\n",
      "Loss:  2.2020511627197266\n",
      "Loss:  2.5346548557281494\n",
      "Loss:  2.4544119834899902\n",
      "Loss:  2.388185977935791\n",
      "Loss:  2.300081968307495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  2.28218674659729\n",
      "Loss:  2.2320408821105957\n",
      "Loss:  2.3358066082000732\n",
      "Loss:  2.323315382003784\n",
      "Loss:  2.358686923980713\n",
      "Loss:  2.29892897605896\n",
      "Loss:  2.1339035034179688\n",
      "Loss:  2.5869710445404053\n",
      "Loss:  2.6577887535095215\n",
      "Loss:  2.5540077686309814\n",
      "Loss:  2.830449342727661\n",
      "Loss:  2.8594632148742676\n",
      "Loss:  2.625839948654175\n",
      "Loss:  1.620732069015503\n",
      "     76        2.3039        2.6404  5.2297\n",
      "Loss:  2.408897876739502\n",
      "Loss:  2.3602559566497803\n",
      "Loss:  2.2552530765533447\n",
      "Loss:  2.270203113555908\n",
      "Loss:  2.237344741821289\n",
      "Loss:  2.209493637084961\n",
      "Loss:  2.2480173110961914\n",
      "Loss:  2.155914068222046\n",
      "Loss:  2.2335190773010254\n",
      "Loss:  2.3138954639434814\n",
      "Loss:  2.2657124996185303\n",
      "Loss:  2.243624687194824\n",
      "Loss:  2.3049979209899902\n",
      "Loss:  2.184227705001831\n",
      "Loss:  2.41953182220459\n",
      "Loss:  2.3654277324676514\n",
      "Loss:  2.324700355529785\n",
      "Loss:  2.343153953552246\n",
      "Loss:  2.3964128494262695\n",
      "Loss:  2.330152750015259\n",
      "Loss:  2.276949882507324\n",
      "Loss:  2.261324167251587\n",
      "Loss:  2.2900543212890625\n",
      "Loss:  2.3346781730651855\n",
      "Loss:  2.2173986434936523\n",
      "Loss:  2.7228798866271973\n",
      "Loss:  2.8016676902770996\n",
      "Loss:  2.6733789443969727\n",
      "Loss:  3.1126601696014404\n",
      "Loss:  3.123532295227051\n",
      "Loss:  2.64868426322937\n",
      "Loss:  1.9986320734024048\n",
      "     77        2.2902        2.8110  5.2507\n",
      "Loss:  2.4568824768066406\n",
      "Loss:  2.462634325027466\n",
      "Loss:  2.2455861568450928\n",
      "Loss:  2.22712779045105\n",
      "Loss:  2.233208656311035\n",
      "Loss:  2.2800471782684326\n",
      "Loss:  2.3743696212768555\n",
      "Loss:  2.2565183639526367\n",
      "Loss:  2.245809555053711\n",
      "Loss:  2.2841014862060547\n",
      "Loss:  2.2162697315216064\n",
      "Loss:  2.227400302886963\n",
      "Loss:  2.319904088973999\n",
      "Loss:  2.2522644996643066\n",
      "Loss:  2.5052809715270996\n",
      "Loss:  2.4147591590881348\n",
      "Loss:  2.3052687644958496\n",
      "Loss:  2.2594571113586426\n",
      "Loss:  2.314300537109375\n",
      "Loss:  2.296041488647461\n",
      "Loss:  2.310349702835083\n",
      "Loss:  2.286726713180542\n",
      "Loss:  2.2490882873535156\n",
      "Loss:  2.2161128520965576\n",
      "Loss:  2.097245693206787\n",
      "Loss:  2.735349178314209\n",
      "Loss:  2.772730827331543\n",
      "Loss:  2.662144184112549\n",
      "Loss:  3.00747013092041\n",
      "Loss:  3.1063344478607178\n",
      "Loss:  2.6659445762634277\n",
      "Loss:  1.9044171571731567\n",
      "     78        2.2940        2.7858  5.2038\n",
      "Loss:  2.4160678386688232\n",
      "Loss:  2.487872362136841\n",
      "Loss:  2.292198419570923\n",
      "Loss:  2.341186761856079\n",
      "Loss:  2.2621469497680664\n",
      "Loss:  2.1731085777282715\n",
      "Loss:  2.2111289501190186\n",
      "Loss:  2.1967873573303223\n",
      "Loss:  2.228250503540039\n",
      "Loss:  2.345752000808716\n",
      "Loss:  2.285862922668457\n",
      "Loss:  2.2177176475524902\n",
      "Loss:  2.2718656063079834\n",
      "Loss:  2.1888060569763184\n",
      "Loss:  2.4177563190460205\n",
      "Loss:  2.3610517978668213\n",
      "Loss:  2.317589044570923\n",
      "Loss:  2.290170907974243\n",
      "Loss:  2.3025076389312744\n",
      "Loss:  2.2168900966644287\n",
      "Loss:  2.2327451705932617\n",
      "Loss:  2.2316532135009766\n",
      "Loss:  2.2811644077301025\n",
      "Loss:  2.256883382797241\n",
      "Loss:  2.099773406982422\n",
      "Loss:  2.640167236328125\n",
      "Loss:  2.686661958694458\n",
      "Loss:  2.592700958251953\n",
      "Loss:  2.81174898147583\n",
      "Loss:  2.8305611610412598\n",
      "Loss:  2.661022663116455\n",
      "Loss:  1.677729845046997\n",
      "     79        2.2776        2.6601  5.2112\n",
      "Loss:  2.399807929992676\n",
      "Loss:  2.381040096282959\n",
      "Loss:  2.1975419521331787\n",
      "Loss:  2.2553422451019287\n",
      "Loss:  2.248430013656616\n",
      "Loss:  2.218256711959839\n",
      "Loss:  2.2547900676727295\n",
      "Loss:  2.1862401962280273\n",
      "Loss:  2.192756414413452\n",
      "Loss:  2.284111738204956\n",
      "Loss:  2.2516558170318604\n",
      "Loss:  2.267178535461426\n",
      "Loss:  2.335998058319092\n",
      "Loss:  2.243104934692383\n",
      "Loss:  2.478461503982544\n",
      "Loss:  2.358297109603882\n",
      "Loss:  2.250319719314575\n",
      "Loss:  2.259860038757324\n",
      "Loss:  2.325681447982788\n",
      "Loss:  2.28020977973938\n",
      "Loss:  2.3047378063201904\n",
      "Loss:  2.2259321212768555\n",
      "Loss:  2.214254140853882\n",
      "Loss:  2.2187721729278564\n",
      "Loss:  2.1005351543426514\n",
      "Loss:  2.752121686935425\n",
      "Loss:  2.7240490913391113\n",
      "Loss:  2.619661569595337\n",
      "Loss:  2.761321783065796\n",
      "Loss:  2.8654325008392334\n",
      "Loss:  2.6622471809387207\n",
      "Loss:  1.6709821224212646\n",
      "     80        2.2698        2.6857  5.2143\n",
      "Loss:  2.4470603466033936\n",
      "Loss:  2.4061858654022217\n",
      "Loss:  2.236851215362549\n",
      "Loss:  2.24389910697937\n",
      "Loss:  2.2302863597869873\n",
      "Loss:  2.188399314880371\n",
      "Loss:  2.243572950363159\n",
      "Loss:  2.21281099319458\n",
      "Loss:  2.2269644737243652\n",
      "Loss:  2.3094546794891357\n",
      "Loss:  2.237337589263916\n",
      "Loss:  2.1833317279815674\n",
      "Loss:  2.275930643081665\n",
      "Loss:  2.1832895278930664\n",
      "Loss:  2.471414089202881\n",
      "Loss:  2.429971933364868\n",
      "Loss:  2.3413634300231934\n",
      "Loss:  2.2754368782043457\n",
      "Loss:  2.2730941772460938\n",
      "Loss:  2.202719211578369\n",
      "Loss:  2.2940285205841064\n",
      "Loss:  2.2676119804382324\n",
      "Loss:  2.3109281063079834\n",
      "Loss:  2.271717071533203\n",
      "Loss:  2.088867425918579\n",
      "Loss:  2.643857002258301\n",
      "Loss:  2.6645090579986572\n",
      "Loss:  2.570089340209961\n",
      "Loss:  2.877082347869873\n",
      "Loss:  2.913320541381836\n",
      "Loss:  2.580724000930786\n",
      "Loss:  1.6902672052383423\n",
      "     81        2.2746        2.6649  5.2102\n",
      "Loss:  2.393789529800415\n",
      "Loss:  2.3481297492980957\n",
      "Loss:  2.2000558376312256\n",
      "Loss:  2.239212989807129\n",
      "Loss:  2.2236132621765137\n",
      "Loss:  2.1958518028259277\n",
      "Loss:  2.2156670093536377\n",
      "Loss:  2.143303394317627\n",
      "Loss:  2.186908721923828\n",
      "Loss:  2.2883150577545166\n",
      "Loss:  2.231086254119873\n",
      "Loss:  2.203289031982422\n",
      "Loss:  2.284909248352051\n",
      "Loss:  2.1825716495513916\n",
      "Loss:  2.412201166152954\n",
      "Loss:  2.3433496952056885\n",
      "Loss:  2.2878201007843018\n",
      "Loss:  2.298341989517212\n",
      "Loss:  2.355739116668701\n",
      "Loss:  2.2708773612976074\n",
      "Loss:  2.2521283626556396\n",
      "Loss:  2.2005321979522705\n",
      "Loss:  2.2550110816955566\n",
      "Loss:  2.250809669494629\n",
      "Loss:  2.1490299701690674\n",
      "Loss:  2.7610795497894287\n",
      "Loss:  2.8186655044555664\n",
      "Loss:  2.703233242034912\n",
      "Loss:  3.145975351333618\n",
      "Loss:  3.1513469219207764\n",
      "Loss:  2.6617431640625\n",
      "Loss:  2.02397084236145\n",
      "     82        2.2568        2.8375  5.2341\n",
      "Loss:  2.437840223312378\n",
      "Loss:  2.4594361782073975\n",
      "Loss:  2.2454421520233154\n",
      "Loss:  2.2282094955444336\n",
      "Loss:  2.199069023132324\n",
      "Loss:  2.2094063758850098\n",
      "Loss:  2.2962934970855713\n",
      "Loss:  2.2110490798950195\n",
      "Loss:  2.2090752124786377\n",
      "Loss:  2.279409170150757\n",
      "Loss:  2.2095346450805664\n",
      "Loss:  2.173978328704834\n",
      "Loss:  2.2880492210388184\n",
      "Loss:  2.19850492477417\n",
      "Loss:  2.439070701599121\n",
      "Loss:  2.389469861984253\n",
      "Loss:  2.284646511077881\n",
      "Loss:  2.242964744567871\n",
      "Loss:  2.290117025375366\n",
      "Loss:  2.2400357723236084\n",
      "Loss:  2.274797201156616\n",
      "Loss:  2.281722068786621\n",
      "Loss:  2.24350905418396\n",
      "Loss:  2.2098898887634277\n",
      "Loss:  2.0674407482147217\n",
      "Loss:  2.7354683876037598\n",
      "Loss:  2.7474560737609863\n",
      "Loss:  2.6588704586029053\n",
      "Loss:  3.0014078617095947\n",
      "Loss:  3.0539090633392334\n",
      "Loss:  2.6750974655151367\n",
      "Loss:  1.8401468992233276\n",
      "     83        2.2649        2.7707  5.2355\n",
      "Loss:  2.3950400352478027\n",
      "Loss:  2.429605722427368\n",
      "Loss:  2.2562918663024902\n",
      "Loss:  2.302435874938965\n",
      "Loss:  2.2742483615875244\n",
      "Loss:  2.1703929901123047\n",
      "Loss:  2.196455478668213\n",
      "Loss:  2.149508237838745\n",
      "Loss:  2.183090925216675\n",
      "Loss:  2.3036720752716064\n",
      "Loss:  2.2686045169830322\n",
      "Loss:  2.2198734283447266\n",
      "Loss:  2.2750284671783447\n",
      "Loss:  2.1880011558532715\n",
      "Loss:  2.381303548812866\n",
      "Loss:  2.3439128398895264\n",
      "Loss:  2.2859230041503906\n",
      "Loss:  2.2744996547698975\n",
      "Loss:  2.2972400188446045\n",
      "Loss:  2.222388744354248\n",
      "Loss:  2.2305080890655518\n",
      "Loss:  2.198716402053833\n",
      "Loss:  2.2529563903808594\n",
      "Loss:  2.2411484718322754\n",
      "Loss:  2.1057705879211426\n",
      "Loss:  2.6822245121002197\n",
      "Loss:  2.6895928382873535\n",
      "Loss:  2.602216958999634\n",
      "Loss:  2.796276569366455\n",
      "Loss:  2.8461344242095947\n",
      "Loss:  2.6466352939605713\n",
      "Loss:  1.6870051622390747\n",
      "     84        2.2583        2.6670  5.2264\n",
      "Loss:  2.4085381031036377\n",
      "Loss:  2.3851277828216553\n",
      "Loss:  2.197054386138916\n",
      "Loss:  2.227555751800537\n",
      "Loss:  2.2300071716308594\n",
      "Loss:  2.2183749675750732\n",
      "Loss:  2.25718355178833\n",
      "Loss:  2.1746907234191895\n",
      "Loss:  2.2047958374023438\n",
      "Loss:  2.2940762042999268\n",
      "Loss:  2.221769094467163\n",
      "Loss:  2.22990345954895\n",
      "Loss:  2.316415548324585\n",
      "Loss:  2.2180471420288086\n",
      "Loss:  2.508344888687134\n",
      "Loss:  2.3972673416137695\n",
      "Loss:  2.2872257232666016\n",
      "Loss:  2.2271769046783447\n",
      "Loss:  2.2830331325531006\n",
      "Loss:  2.2306065559387207\n",
      "Loss:  2.3094265460968018\n",
      "Loss:  2.245668411254883\n",
      "Loss:  2.227672576904297\n",
      "Loss:  2.2044997215270996\n",
      "Loss:  2.062403678894043\n",
      "Loss:  2.757810592651367\n",
      "Loss:  2.711927652359009\n",
      "Loss:  2.609290599822998\n",
      "Loss:  2.7800276279449463\n",
      "Loss:  2.8853442668914795\n",
      "Loss:  2.630690813064575\n",
      "Loss:  1.6822490692138672\n",
      "     85        2.2632        2.6846  5.2132\n",
      "Loss:  2.429542064666748\n",
      "Loss:  2.402202844619751\n",
      "Loss:  2.2470784187316895\n",
      "Loss:  2.2697179317474365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  2.2246146202087402\n",
      "Loss:  2.159473180770874\n",
      "Loss:  2.2128899097442627\n",
      "Loss:  2.1906609535217285\n",
      "Loss:  2.238255262374878\n",
      "Loss:  2.3061914443969727\n",
      "Loss:  2.241861581802368\n",
      "Loss:  2.177976369857788\n",
      "Loss:  2.2729291915893555\n",
      "Loss:  2.167855739593506\n",
      "Loss:  2.4260966777801514\n",
      "Loss:  2.405637502670288\n",
      "Loss:  2.3387348651885986\n",
      "Loss:  2.3038721084594727\n",
      "Loss:  2.3078010082244873\n",
      "Loss:  2.2105283737182617\n",
      "Loss:  2.2578790187835693\n",
      "Loss:  2.222001075744629\n",
      "Loss:  2.293882131576538\n",
      "Loss:  2.2743709087371826\n",
      "Loss:  2.1226320266723633\n",
      "Loss:  2.7125051021575928\n",
      "Loss:  2.70800518989563\n",
      "Loss:  2.6100380420684814\n",
      "Loss:  2.9995040893554688\n",
      "Loss:  3.024003028869629\n",
      "Loss:  2.5893137454986572\n",
      "Loss:  1.8123314380645752\n",
      "     86        2.2686        2.7330  5.2081\n",
      "Loss:  2.402958631515503\n",
      "Loss:  2.3775956630706787\n",
      "Loss:  2.19162917137146\n",
      "Loss:  2.219033718109131\n",
      "Loss:  2.209376096725464\n",
      "Loss:  2.2191784381866455\n",
      "Loss:  2.2572598457336426\n",
      "Loss:  2.1623897552490234\n",
      "Loss:  2.1851015090942383\n",
      "Loss:  2.274200916290283\n",
      "Loss:  2.208824396133423\n",
      "Loss:  2.197873830795288\n",
      "Loss:  2.292771816253662\n",
      "Loss:  2.2169671058654785\n",
      "Loss:  2.424227714538574\n",
      "Loss:  2.353489875793457\n",
      "Loss:  2.2647016048431396\n",
      "Loss:  2.263589859008789\n",
      "Loss:  2.3323612213134766\n",
      "Loss:  2.2856767177581787\n",
      "Loss:  2.268716812133789\n",
      "Loss:  2.2521398067474365\n",
      "Loss:  2.2128777503967285\n",
      "Loss:  2.1962594985961914\n",
      "Loss:  2.1058013439178467\n",
      "Loss:  2.8043601512908936\n",
      "Loss:  2.8410449028015137\n",
      "Loss:  2.7278692722320557\n",
      "Loss:  3.1188273429870605\n",
      "Loss:  3.168213129043579\n",
      "Loss:  2.698768377304077\n",
      "Loss:  2.0112228393554688\n",
      "     87        2.2554        2.8557  5.2061\n",
      "Loss:  2.427455425262451\n",
      "Loss:  2.4706804752349854\n",
      "Loss:  2.2703328132629395\n",
      "Loss:  2.269491672515869\n",
      "Loss:  2.2236504554748535\n",
      "Loss:  2.1799721717834473\n",
      "Loss:  2.203493118286133\n",
      "Loss:  2.1610825061798096\n",
      "Loss:  2.1863059997558594\n",
      "Loss:  2.2842087745666504\n",
      "Loss:  2.2281200885772705\n",
      "Loss:  2.1718857288360596\n",
      "Loss:  2.2656702995300293\n",
      "Loss:  2.159966468811035\n",
      "Loss:  2.3948636054992676\n",
      "Loss:  2.3741214275360107\n",
      "Loss:  2.289520025253296\n",
      "Loss:  2.251969337463379\n",
      "Loss:  2.2707927227020264\n",
      "Loss:  2.1910319328308105\n",
      "Loss:  2.2378687858581543\n",
      "Loss:  2.226055145263672\n",
      "Loss:  2.24580717086792\n",
      "Loss:  2.2277376651763916\n",
      "Loss:  2.073120355606079\n",
      "Loss:  2.6846070289611816\n",
      "Loss:  2.665757179260254\n",
      "Loss:  2.6057827472686768\n",
      "Loss:  2.889408826828003\n",
      "Loss:  2.9184048175811768\n",
      "Loss:  2.6536965370178223\n",
      "Loss:  1.7016010284423828\n",
      "     88        2.2519        2.6922  5.2301\n",
      "Loss:  2.38433575630188\n",
      "Loss:  2.369332790374756\n",
      "Loss:  2.1943421363830566\n",
      "Loss:  2.256181478500366\n",
      "Loss:  2.241272449493408\n",
      "Loss:  2.232905864715576\n",
      "Loss:  2.214723587036133\n",
      "Loss:  2.1438493728637695\n",
      "Loss:  2.1732912063598633\n",
      "Loss:  2.262995958328247\n",
      "Loss:  2.2183268070220947\n",
      "Loss:  2.220684289932251\n",
      "Loss:  2.287080764770508\n",
      "Loss:  2.203566074371338\n",
      "Loss:  2.406726360321045\n",
      "Loss:  2.336555004119873\n",
      "Loss:  2.2515690326690674\n",
      "Loss:  2.2296125888824463\n",
      "Loss:  2.274867534637451\n",
      "Loss:  2.1953349113464355\n",
      "Loss:  2.2515180110931396\n",
      "Loss:  2.1897268295288086\n",
      "Loss:  2.1997714042663574\n",
      "Loss:  2.2002251148223877\n",
      "Loss:  2.074247121810913\n",
      "Loss:  2.727332830429077\n",
      "Loss:  2.690401554107666\n",
      "Loss:  2.618011951446533\n",
      "Loss:  2.7891273498535156\n",
      "Loss:  2.875096321105957\n",
      "Loss:  2.650420904159546\n",
      "Loss:  1.7014766931533813\n",
      "     89        \u001b[36m2.2410\u001b[0m        2.6815  5.2562\n",
      "Loss:  2.4180712699890137\n",
      "Loss:  2.39323091506958\n",
      "Loss:  2.208927631378174\n",
      "Loss:  2.2313530445098877\n",
      "Loss:  2.2018542289733887\n",
      "Loss:  2.17805814743042\n",
      "Loss:  2.2458858489990234\n",
      "Loss:  2.196138858795166\n",
      "Loss:  2.231930732727051\n",
      "Loss:  2.3003017902374268\n",
      "Loss:  2.2050583362579346\n",
      "Loss:  2.1775739192962646\n",
      "Loss:  2.2802960872650146\n",
      "Loss:  2.1756105422973633\n",
      "Loss:  2.4527981281280518\n",
      "Loss:  2.4117374420166016\n",
      "Loss:  2.3182384967803955\n",
      "Loss:  2.250506639480591\n",
      "Loss:  2.257526159286499\n",
      "Loss:  2.1731560230255127\n",
      "Loss:  2.2614552974700928\n",
      "Loss:  2.216083288192749\n",
      "Loss:  2.244511127471924\n",
      "Loss:  2.2014386653900146\n",
      "Loss:  2.0548624992370605\n",
      "Loss:  2.6955254077911377\n",
      "Loss:  2.654702663421631\n",
      "Loss:  2.587603807449341\n",
      "Loss:  2.8927574157714844\n",
      "Loss:  2.9427177906036377\n",
      "Loss:  2.5866150856018066\n",
      "Loss:  1.7125539779663086\n",
      "     90        2.2520        2.6835  5.2076\n",
      "Loss:  2.3875036239624023\n",
      "Loss:  2.348740577697754\n",
      "Loss:  2.186760902404785\n",
      "Loss:  2.219013214111328\n",
      "Loss:  2.2075085639953613\n",
      "Loss:  2.1699249744415283\n",
      "Loss:  2.1873221397399902\n",
      "Loss:  2.135225296020508\n",
      "Loss:  2.1979217529296875\n",
      "Loss:  2.2756564617156982\n",
      "Loss:  2.2170770168304443\n",
      "Loss:  2.1953887939453125\n",
      "Loss:  2.2751033306121826\n",
      "Loss:  2.1789283752441406\n",
      "Loss:  2.3819832801818848\n",
      "Loss:  2.338149309158325\n",
      "Loss:  2.2780208587646484\n",
      "Loss:  2.2710835933685303\n",
      "Loss:  2.3354671001434326\n",
      "Loss:  2.258211374282837\n",
      "Loss:  2.24103045463562\n",
      "Loss:  2.1890487670898438\n",
      "Loss:  2.214446783065796\n",
      "Loss:  2.2093560695648193\n",
      "Loss:  2.1111578941345215\n",
      "Loss:  2.8003368377685547\n",
      "Loss:  2.8170480728149414\n",
      "Loss:  2.715280771255493\n",
      "Loss:  3.1321585178375244\n",
      "Loss:  3.1546010971069336\n",
      "Loss:  2.6794285774230957\n",
      "Loss:  1.9893351793289185\n",
      "     91        \u001b[36m2.2407\u001b[0m        2.8451  5.2151\n",
      "Loss:  2.420100450515747\n",
      "Loss:  2.4231150150299072\n",
      "Loss:  2.207324504852295\n",
      "Loss:  2.2044923305511475\n",
      "Loss:  2.188826322555542\n",
      "Loss:  2.179680109024048\n",
      "Loss:  2.2195334434509277\n",
      "Loss:  2.1515557765960693\n",
      "Loss:  2.1825473308563232\n",
      "Loss:  2.272710084915161\n",
      "Loss:  2.203342914581299\n",
      "Loss:  2.1671266555786133\n",
      "Loss:  2.2746899127960205\n",
      "Loss:  2.181389808654785\n",
      "Loss:  2.4047889709472656\n",
      "Loss:  2.379513740539551\n",
      "Loss:  2.274611711502075\n",
      "Loss:  2.2234976291656494\n",
      "Loss:  2.275974988937378\n",
      "Loss:  2.2134225368499756\n",
      "Loss:  2.254835844039917\n",
      "Loss:  2.2556521892547607\n",
      "Loss:  2.213127613067627\n",
      "Loss:  2.203244924545288\n",
      "Loss:  2.0566508769989014\n",
      "Loss:  2.776510715484619\n",
      "Loss:  2.7721898555755615\n",
      "Loss:  2.6966047286987305\n",
      "Loss:  3.0923869609832764\n",
      "Loss:  3.074357032775879\n",
      "Loss:  2.693898916244507\n",
      "Loss:  1.9046841859817505\n",
      "     92        2.2418        2.8107  5.2103\n",
      "Loss:  2.3975062370300293\n",
      "Loss:  2.40866756439209\n",
      "Loss:  2.242661237716675\n",
      "Loss:  2.3046631813049316\n",
      "Loss:  2.2481348514556885\n",
      "Loss:  2.1704111099243164\n",
      "Loss:  2.1824610233306885\n",
      "Loss:  2.135496139526367\n",
      "Loss:  2.173664093017578\n",
      "Loss:  2.286670684814453\n",
      "Loss:  2.2242276668548584\n",
      "Loss:  2.183140277862549\n",
      "Loss:  2.2632672786712646\n",
      "Loss:  2.171555757522583\n",
      "Loss:  2.3791732788085938\n",
      "Loss:  2.367055892944336\n",
      "Loss:  2.2895328998565674\n",
      "Loss:  2.247620105743408\n",
      "Loss:  2.2638001441955566\n",
      "Loss:  2.174077033996582\n",
      "Loss:  2.2294821739196777\n",
      "Loss:  2.209930419921875\n",
      "Loss:  2.236820697784424\n",
      "Loss:  2.217466115951538\n",
      "Loss:  2.088697910308838\n",
      "Loss:  2.710130453109741\n",
      "Loss:  2.668086528778076\n",
      "Loss:  2.5961365699768066\n",
      "Loss:  2.8241653442382812\n",
      "Loss:  2.882181167602539\n",
      "Loss:  2.6426591873168945\n",
      "Loss:  1.6799122095108032\n",
      "     93        2.2443        2.6763  5.2091\n",
      "Loss:  2.397007465362549\n",
      "Loss:  2.370269775390625\n",
      "Loss:  2.2130885124206543\n",
      "Loss:  2.2330212593078613\n",
      "Loss:  2.230198621749878\n",
      "Loss:  2.22922682762146\n",
      "Loss:  2.26408052444458\n",
      "Loss:  2.1837027072906494\n",
      "Loss:  2.2221672534942627\n",
      "Loss:  2.2909555435180664\n",
      "Loss:  2.212660312652588\n",
      "Loss:  2.205477476119995\n",
      "Loss:  2.303868293762207\n",
      "Loss:  2.2080533504486084\n",
      "Loss:  2.485886812210083\n",
      "Loss:  2.3765218257904053\n",
      "Loss:  2.277771234512329\n",
      "Loss:  2.222165822982788\n",
      "Loss:  2.268054485321045\n",
      "Loss:  2.2210848331451416\n",
      "Loss:  2.2759366035461426\n",
      "Loss:  2.233478307723999\n",
      "Loss:  2.1930649280548096\n",
      "Loss:  2.1803603172302246\n",
      "Loss:  2.0604026317596436\n",
      "Loss:  2.7494001388549805\n",
      "Loss:  2.6936826705932617\n",
      "Loss:  2.6091344356536865\n",
      "Loss:  2.7987282276153564\n",
      "Loss:  2.8865861892700195\n",
      "Loss:  2.617961883544922\n",
      "Loss:  1.697460651397705\n",
      "     94        2.2549        2.6822  5.2333\n",
      "Loss:  2.424100399017334\n",
      "Loss:  2.405365467071533\n",
      "Loss:  2.2276103496551514\n",
      "Loss:  2.2446091175079346\n",
      "Loss:  2.218118906021118\n",
      "Loss:  2.1811773777008057\n",
      "Loss:  2.2154219150543213\n",
      "Loss:  2.192805528640747\n",
      "Loss:  2.237222194671631\n",
      "Loss:  2.3029305934906006\n",
      "Loss:  2.2449774742126465\n",
      "Loss:  2.1784043312072754\n",
      "Loss:  2.2765676975250244\n",
      "Loss:  2.1686289310455322\n",
      "Loss:  2.4108471870422363\n",
      "Loss:  2.4102983474731445\n",
      "Loss:  2.344752073287964\n",
      "Loss:  2.302398681640625\n",
      "Loss:  2.324118137359619\n",
      "Loss:  2.2310733795166016\n",
      "Loss:  2.2268424034118652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  2.2018518447875977\n",
      "Loss:  2.248495578765869\n",
      "Loss:  2.2356653213500977\n",
      "Loss:  2.093773126602173\n",
      "Loss:  2.7440991401672363\n",
      "Loss:  2.6899936199188232\n",
      "Loss:  2.627790689468384\n",
      "Loss:  2.973659038543701\n",
      "Loss:  3.0159757137298584\n",
      "Loss:  2.584132194519043\n",
      "Loss:  1.7975391149520874\n",
      "     95        2.2624        2.7311  5.2505\n",
      "Loss:  2.3937535285949707\n",
      "Loss:  2.353971481323242\n",
      "Loss:  2.1827340126037598\n",
      "Loss:  2.2026708126068115\n",
      "Loss:  2.200190544128418\n",
      "Loss:  2.1910948753356934\n",
      "Loss:  2.220478057861328\n",
      "Loss:  2.13925838470459\n",
      "Loss:  2.174239158630371\n",
      "Loss:  2.2733535766601562\n",
      "Loss:  2.206573724746704\n",
      "Loss:  2.2051477432250977\n",
      "Loss:  2.2946536540985107\n",
      "Loss:  2.217290163040161\n",
      "Loss:  2.411005735397339\n",
      "Loss:  2.3974454402923584\n",
      "Loss:  2.266340494155884\n",
      "Loss:  2.243025064468384\n",
      "Loss:  2.336850643157959\n",
      "Loss:  2.273028612136841\n",
      "Loss:  2.274599075317383\n",
      "Loss:  2.2456319332122803\n",
      "Loss:  2.194798231124878\n",
      "Loss:  2.193359851837158\n",
      "Loss:  2.0854671001434326\n",
      "Loss:  2.8144567012786865\n",
      "Loss:  2.824566602706909\n",
      "Loss:  2.7330851554870605\n",
      "Loss:  3.1330411434173584\n",
      "Loss:  3.1238980293273926\n",
      "Loss:  2.698723077774048\n",
      "Loss:  1.9715062379837036\n",
      "     96        2.2475        2.8490  5.2045\n",
      "Loss:  2.420208215713501\n",
      "Loss:  2.446744918823242\n",
      "Loss:  2.2652461528778076\n",
      "Loss:  2.2916109561920166\n",
      "Loss:  2.207855701446533\n",
      "Loss:  2.1580393314361572\n",
      "Loss:  2.189852237701416\n",
      "Loss:  2.1473500728607178\n",
      "Loss:  2.182403802871704\n",
      "Loss:  2.2990615367889404\n",
      "Loss:  2.220116376876831\n",
      "Loss:  2.156113624572754\n",
      "Loss:  2.271270751953125\n",
      "Loss:  2.1650521755218506\n",
      "Loss:  2.3916423320770264\n",
      "Loss:  2.394078016281128\n",
      "Loss:  2.2945711612701416\n",
      "Loss:  2.2516908645629883\n",
      "Loss:  2.270230531692505\n",
      "Loss:  2.1838488578796387\n",
      "Loss:  2.245403528213501\n",
      "Loss:  2.259641170501709\n",
      "Loss:  2.2530553340911865\n",
      "Loss:  2.2361185550689697\n",
      "Loss:  2.077082395553589\n",
      "Loss:  2.7036075592041016\n",
      "Loss:  2.661407470703125\n",
      "Loss:  2.591163158416748\n",
      "Loss:  2.86106014251709\n",
      "Loss:  2.8780906200408936\n",
      "Loss:  2.6402089595794678\n",
      "Loss:  1.6944124698638916\n",
      "     97        2.2516        2.6788  5.2110\n",
      "Loss:  2.3845770359039307\n",
      "Loss:  2.358201503753662\n",
      "Loss:  2.1917572021484375\n",
      "Loss:  2.258326768875122\n",
      "Loss:  2.2456793785095215\n",
      "Loss:  2.2358665466308594\n",
      "Loss:  2.239393711090088\n",
      "Loss:  2.1641299724578857\n",
      "Loss:  2.184119462966919\n",
      "Loss:  2.2670364379882812\n",
      "Loss:  2.1930549144744873\n",
      "Loss:  2.1914803981781006\n",
      "Loss:  2.273530960083008\n",
      "Loss:  2.193269729614258\n",
      "Loss:  2.394890069961548\n",
      "Loss:  2.3417797088623047\n",
      "Loss:  2.238039493560791\n",
      "Loss:  2.220271348953247\n",
      "Loss:  2.2645387649536133\n",
      "Loss:  2.193112373352051\n",
      "Loss:  2.2415874004364014\n",
      "Loss:  2.1924498081207275\n",
      "Loss:  2.1821680068969727\n",
      "Loss:  2.194969892501831\n",
      "Loss:  2.0743563175201416\n",
      "Loss:  2.7244832515716553\n",
      "Loss:  2.6882083415985107\n",
      "Loss:  2.607473134994507\n",
      "Loss:  2.820155143737793\n",
      "Loss:  2.87139630317688\n",
      "Loss:  2.647575616836548\n",
      "Loss:  1.6776219606399536\n",
      "     98        \u001b[36m2.2372\u001b[0m        2.6819  5.2078\n",
      "Loss:  2.4190144538879395\n",
      "Loss:  2.401118278503418\n",
      "Loss:  2.2086126804351807\n",
      "Loss:  2.2148025035858154\n",
      "Loss:  2.200678586959839\n",
      "Loss:  2.1826601028442383\n",
      "Loss:  2.2242343425750732\n",
      "Loss:  2.1885271072387695\n",
      "Loss:  2.2250640392303467\n",
      "Loss:  2.3022685050964355\n",
      "Loss:  2.216463327407837\n",
      "Loss:  2.157531976699829\n",
      "Loss:  2.258100748062134\n",
      "Loss:  2.151494026184082\n",
      "Loss:  2.4000256061553955\n",
      "Loss:  2.3990516662597656\n",
      "Loss:  2.3076579570770264\n",
      "Loss:  2.238217353820801\n",
      "Loss:  2.2597296237945557\n",
      "Loss:  2.1711275577545166\n",
      "Loss:  2.2257132530212402\n",
      "Loss:  2.1964409351348877\n",
      "Loss:  2.2057623863220215\n",
      "Loss:  2.1914167404174805\n",
      "Loss:  2.0458648204803467\n",
      "Loss:  2.7209064960479736\n",
      "Loss:  2.6614086627960205\n",
      "Loss:  2.614849328994751\n",
      "Loss:  2.9065728187561035\n",
      "Loss:  2.938199281692505\n",
      "Loss:  2.5845425128936768\n",
      "Loss:  1.7037593126296997\n",
      "     99        2.2402        2.6937  5.2130\n",
      "Loss:  2.3843624591827393\n",
      "Loss:  2.3520052433013916\n",
      "Loss:  2.182281017303467\n",
      "Loss:  2.2105777263641357\n",
      "Loss:  2.194746494293213\n",
      "Loss:  2.145456314086914\n",
      "Loss:  2.1793713569641113\n",
      "Loss:  2.134697914123535\n",
      "Loss:  2.181279182434082\n",
      "Loss:  2.287203550338745\n",
      "Loss:  2.2126452922821045\n",
      "Loss:  2.2005913257598877\n",
      "Loss:  2.283271312713623\n",
      "Loss:  2.1821024417877197\n",
      "Loss:  2.3810391426086426\n",
      "Loss:  2.368403673171997\n",
      "Loss:  2.2640204429626465\n",
      "Loss:  2.2509076595306396\n",
      "Loss:  2.347219228744507\n",
      "Loss:  2.2441461086273193\n",
      "Loss:  2.247084379196167\n",
      "Loss:  2.200688600540161\n",
      "Loss:  2.1812829971313477\n",
      "Loss:  2.176387071609497\n",
      "Loss:  2.0849883556365967\n",
      "Loss:  2.7903900146484375\n",
      "Loss:  2.7963879108428955\n",
      "Loss:  2.7081644535064697\n",
      "Loss:  3.1057989597320557\n",
      "Loss:  3.0938637256622314\n",
      "Loss:  2.6769111156463623\n",
      "Loss:  1.905728816986084\n",
      "    100        \u001b[36m2.2355\u001b[0m        2.8212  5.2277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class '__main__.CustomNet'>[initialized](\n",
       "  module_=MLP(\n",
       "    (feat_mlp): Sequential(\n",
       "      (0): BatchNorm1d(499500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Linear(in_features=499500, out_features=1000, bias=True)\n",
       "      (2): ReLU()\n",
       "      (3): Dropout(p=0, inplace=False)\n",
       "      (4): Linear(in_features=1000, out_features=2, bias=True)\n",
       "    )\n",
       "    (target_mlp): Sequential(\n",
       "      (0): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Linear(in_features=1, out_features=2, bias=True)\n",
       "    )\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X= features.squeeze(1),y =targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb4624b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_age_estimator = ProjectionAgeEstimator(custom_net=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b6bdb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing module because the following parameters were re-set: hidden_dim_feat, input_dim_feat, input_dim_target, output_dim.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "Loss:  14.8409423828125\n",
      "Loss:  6.3048295974731445\n",
      "Loss:  4.309867858886719\n",
      "Loss:  3.862704038619995\n",
      "Loss:  3.8481736183166504\n",
      "Loss:  3.4727208614349365\n",
      "Loss:  3.6281301975250244\n",
      "Loss:  3.440521001815796\n",
      "Loss:  3.4281177520751953\n",
      "Loss:  3.465547800064087\n",
      "Loss:  3.4668784141540527\n",
      "Loss:  3.3818492889404297\n",
      "Loss:  3.383931875228882\n",
      "Loss:  3.115739345550537\n",
      "Loss:  3.5994744300842285\n",
      "Loss:  3.4703662395477295\n",
      "Loss:  3.2510499954223633\n",
      "Loss:  3.0903844833374023\n",
      "Loss:  3.1757078170776367\n",
      "Loss:  2.95011305809021\n",
      "Loss:  3.0272836685180664\n",
      "Loss:  2.905761957168579\n",
      "Loss:  2.9082469940185547\n",
      "Loss:  2.9462461471557617\n",
      "Loss:  2.7896568775177\n",
      "Loss:  2.9061388969421387\n",
      "Loss:  3.0662286281585693\n",
      "Loss:  2.8704910278320312\n",
      "Loss:  3.100492477416992\n",
      "Loss:  2.8935303688049316\n",
      "Loss:  2.968083143234253\n",
      "Loss:  1.7194708585739136\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.9256\u001b[0m        \u001b[32m2.9144\u001b[0m  5.1848\n",
      "Loss:  3.0483741760253906\n",
      "Loss:  3.0576322078704834\n",
      "Loss:  2.9305977821350098\n",
      "Loss:  2.886018991470337\n",
      "Loss:  2.8808717727661133\n",
      "Loss:  2.8238699436187744\n",
      "Loss:  2.8318886756896973\n",
      "Loss:  2.8114569187164307\n",
      "Loss:  2.8144869804382324\n",
      "Loss:  2.8080780506134033\n",
      "Loss:  2.8136370182037354\n",
      "Loss:  2.8132553100585938\n",
      "Loss:  2.783416509628296\n",
      "Loss:  2.7489523887634277\n",
      "Loss:  2.9131479263305664\n",
      "Loss:  2.88706111907959\n",
      "Loss:  2.7996506690979004\n",
      "Loss:  2.786471128463745\n",
      "Loss:  2.852275848388672\n",
      "Loss:  2.7155821323394775\n",
      "Loss:  2.7481372356414795\n",
      "Loss:  2.7110612392425537\n",
      "Loss:  2.7086853981018066\n",
      "Loss:  2.7575244903564453\n",
      "Loss:  2.624380111694336\n",
      "Loss:  2.690927028656006\n",
      "Loss:  2.8597211837768555\n",
      "Loss:  2.714092254638672\n",
      "Loss:  2.8919990062713623\n",
      "Loss:  2.6976397037506104\n",
      "Loss:  2.770494222640991\n",
      "Loss:  1.5351084470748901\n",
      "      2        \u001b[36m2.8228\u001b[0m        \u001b[32m2.7182\u001b[0m  5.2262\n",
      "Loss:  2.8504395484924316\n",
      "Loss:  2.892103910446167\n",
      "Loss:  2.7884936332702637\n",
      "Loss:  2.7502875328063965\n",
      "Loss:  2.75780987739563\n",
      "Loss:  2.7020370960235596\n",
      "Loss:  2.7189509868621826\n",
      "Loss:  2.70493221282959\n",
      "Loss:  2.7167794704437256\n",
      "Loss:  2.7113592624664307\n",
      "Loss:  2.733069658279419\n",
      "Loss:  2.731760263442993\n",
      "Loss:  2.716568946838379\n",
      "Loss:  2.6804637908935547\n",
      "Loss:  2.8704934120178223\n",
      "Loss:  2.8423140048980713\n",
      "Loss:  2.749950408935547\n",
      "Loss:  2.73714280128479\n",
      "Loss:  2.8077526092529297\n",
      "Loss:  2.6721389293670654\n",
      "Loss:  2.709094285964966\n",
      "Loss:  2.671994209289551\n",
      "Loss:  2.6734845638275146\n",
      "Loss:  2.719921588897705\n",
      "Loss:  2.589416265487671\n",
      "Loss:  2.6541733741760254\n",
      "Loss:  2.8292062282562256\n",
      "Loss:  2.681889533996582\n",
      "Loss:  2.850769519805908\n",
      "Loss:  2.661139488220215\n",
      "Loss:  2.7376697063446045\n",
      "Loss:  1.5114476680755615\n",
      "      3        \u001b[36m2.7404\u001b[0m        \u001b[32m2.6837\u001b[0m  5.1846\n",
      "Loss:  2.8140830993652344\n",
      "Loss:  2.8588831424713135\n",
      "Loss:  2.757171869277954\n",
      "Loss:  2.715731620788574\n",
      "Loss:  2.7285149097442627\n",
      "Loss:  2.6788554191589355\n",
      "Loss:  2.6891348361968994\n",
      "Loss:  2.677838087081909\n",
      "Loss:  2.688422203063965\n",
      "Loss:  2.6809322834014893\n",
      "Loss:  2.702277660369873\n",
      "Loss:  2.7009212970733643\n",
      "Loss:  2.6895790100097656\n",
      "Loss:  2.652722120285034\n",
      "Loss:  2.844165802001953\n",
      "Loss:  2.819563627243042\n",
      "Loss:  2.726036787033081\n",
      "Loss:  2.706904172897339\n",
      "Loss:  2.776360511779785\n",
      "Loss:  2.643651008605957\n",
      "Loss:  2.6861650943756104\n",
      "Loss:  2.64266300201416\n",
      "Loss:  2.653149127960205\n",
      "Loss:  2.6915788650512695\n",
      "Loss:  2.5651495456695557\n",
      "Loss:  2.6300899982452393\n",
      "Loss:  2.8048758506774902\n",
      "Loss:  2.6550204753875732\n",
      "Loss:  2.8192734718322754\n",
      "Loss:  2.6389803886413574\n",
      "Loss:  2.709704875946045\n",
      "Loss:  1.5023174285888672\n",
      "      4        \u001b[36m2.7120\u001b[0m        \u001b[32m2.6583\u001b[0m  5.1706\n",
      "Loss:  2.7931039333343506\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprojection_age_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 15\u001b[0m, in \u001b[0;36mProjectionAgeEstimator.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_net\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[0;32m---> 15\u001b[0m     X_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mage_estimator\u001b[38;5;241m.\u001b[39mfit(X_proj, y)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/skorch/net.py:1632\u001b[0m, in \u001b[0;36mNeuralNet.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1602\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m   1603\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Where applicable, return class labels for samples in X.\u001b[39;00m\n\u001b[1;32m   1604\u001b[0m \n\u001b[1;32m   1605\u001b[0m \u001b[38;5;124;03m    If the module's forward method returns multiple outputs as a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1630\u001b[0m \n\u001b[1;32m   1631\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/skorch/net.py:1595\u001b[0m, in \u001b[0;36mNeuralNet.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1593\u001b[0m nonlin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_predict_nonlinearity()\n\u001b[1;32m   1594\u001b[0m y_probas \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m yp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_iter(X, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1596\u001b[0m     yp \u001b[38;5;241m=\u001b[39m yp[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(yp, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m yp\n\u001b[1;32m   1597\u001b[0m     yp \u001b[38;5;241m=\u001b[39m nonlin(yp)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/skorch/net.py:1441\u001b[0m, in \u001b[0;36mNeuralNet.forward_iter\u001b[0;34m(self, X, training, device)\u001b[0m\n\u001b[1;32m   1439\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(dataset, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[1;32m   1440\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[0;32m-> 1441\u001b[0m     yp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m to_device(yp, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/skorch/net.py:1134\u001b[0m, in \u001b[0;36mNeuralNet.evaluation_step\u001b[0;34m(self, batch, training)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(training):\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_training(training)\n\u001b[0;32m-> 1134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXi\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/skorch/net.py:1521\u001b[0m, in \u001b[0;36mNeuralNet.infer\u001b[0;34m(self, x, **fit_params)\u001b[0m\n\u001b[1;32m   1519\u001b[0m     x_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_x_and_fit_params(x, fit_params)\n\u001b[1;32m   1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mx_dict)\n\u001b[0;32m-> 1521\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "projection_age_estimator.fit(features.squeeze(1), targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fec6175",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(features, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27e4f2ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot clone object <class '__main__.CustomNet'>[uninitialized](\n  module=<class '__main__.MLP'>,\n  module__hidden_dim_feat=1000,\n  module__input_dim_feat=499500,\n  module__input_dim_target=1,\n  module__output_dim=2,\n), as the constructor either does not set or modifies parameter criterion_pft_cls",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/joblib/parallel.py:862\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 862\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ready_batches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m queue\u001b[38;5;241m.\u001b[39mEmpty:\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;66;03m# slice the iterator n_jobs * batchsize items at a time. If the\u001b[39;00m\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;66;03m# slice returns less than that, then the current batchsize puts\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[38;5;66;03m# accordingly to distribute evenly the last items between all\u001b[39;00m\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# workers.\u001b[39;00m\n",
      "File \u001b[0;32m/apps/eb/2020b/skylake/software/Python/3.9.6-GCCcore-11.2.0/lib/python3.9/queue.py:168\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 168\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_sizes, train_scores, test_scores \u001b[38;5;241m=\u001b[39m \u001b[43mlearning_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprojection_age_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Your skorch wrapped model\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Combined features\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Ensure targets are in the correct shape\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Example scoring metric\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:1683\u001b[0m, in \u001b[0;36mlearning_curve\u001b[0;34m(estimator, X, y, groups, train_sizes, cv, scoring, exploit_incremental_learning, n_jobs, pre_dispatch, verbose, shuffle, random_state, error_score, return_times, fit_params)\u001b[0m\n\u001b[1;32m   1680\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n_train_samples \u001b[38;5;129;01min\u001b[39;00m train_sizes_abs:\n\u001b[1;32m   1681\u001b[0m         train_test_proportions\u001b[38;5;241m.\u001b[39mappend((train[:n_train_samples], test))\n\u001b[0;32m-> 1683\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1695\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1697\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1698\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_test_proportions\u001b[49m\n\u001b[1;32m   1699\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1700\u001b[0m results \u001b[38;5;241m=\u001b[39m _aggregate_score_dicts(results)\n\u001b[1;32m   1701\u001b[0m train_scores \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, n_unique_ticks)\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/joblib/parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/joblib/parallel.py:873\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    870\u001b[0m n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_effective_n_jobs\n\u001b[1;32m    871\u001b[0m big_batch_size \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;241m*\u001b[39m n_jobs\n\u001b[0;32m--> 873\u001b[0m islice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbig_batch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(islice) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    875\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/utils/parallel.py:61\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Capture the thread-local scikit-learn configuration at the time\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Parallel.__call__ is issued since the tasks can be dispatched\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# in a different thread depending on the backend and on the value of\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# pre_dispatch and n_jobs.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m---> 61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:1685\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1680\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n_train_samples \u001b[38;5;129;01min\u001b[39;00m train_sizes_abs:\n\u001b[1;32m   1681\u001b[0m         train_test_proportions\u001b[38;5;241m.\u001b[39mappend((train[:n_train_samples], test))\n\u001b[1;32m   1683\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m   1684\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m-> 1685\u001b[0m         \u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1686\u001b[0m         X,\n\u001b[1;32m   1687\u001b[0m         y,\n\u001b[1;32m   1688\u001b[0m         scorer,\n\u001b[1;32m   1689\u001b[0m         train,\n\u001b[1;32m   1690\u001b[0m         test,\n\u001b[1;32m   1691\u001b[0m         verbose,\n\u001b[1;32m   1692\u001b[0m         parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1693\u001b[0m         fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[1;32m   1694\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1695\u001b[0m         error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[1;32m   1696\u001b[0m         return_times\u001b[38;5;241m=\u001b[39mreturn_times,\n\u001b[1;32m   1697\u001b[0m     )\n\u001b[1;32m   1698\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m train_test_proportions\n\u001b[1;32m   1699\u001b[0m )\n\u001b[1;32m   1700\u001b[0m results \u001b[38;5;241m=\u001b[39m _aggregate_score_dicts(results)\n\u001b[1;32m   1701\u001b[0m train_scores \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, n_unique_ticks)\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/base.py:75\u001b[0m, in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct a new unfitted estimator with the same parameters.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03mClone does a deep copy of the model in an estimator\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03mfound in :ref:`randomness`.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sklearn_clone__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misclass(estimator):\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__sklearn_clone__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _clone_parametrized(estimator, safe\u001b[38;5;241m=\u001b[39msafe)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/base.py:268\u001b[0m, in \u001b[0;36mBaseEstimator.__sklearn_clone__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__sklearn_clone__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_clone_parametrized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/base.py:108\u001b[0m, in \u001b[0;36m_clone_parametrized\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m    106\u001b[0m new_object_params \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mget_params(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m new_object_params\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 108\u001b[0m     new_object_params[name] \u001b[38;5;241m=\u001b[39m \u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m new_object \u001b[38;5;241m=\u001b[39m klass(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_object_params)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/base.py:76\u001b[0m, in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sklearn_clone__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misclass(estimator):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m estimator\u001b[38;5;241m.\u001b[39m__sklearn_clone__()\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_clone_parametrized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/base.py:123\u001b[0m, in \u001b[0;36m_clone_parametrized\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m    121\u001b[0m     param2 \u001b[38;5;241m=\u001b[39m params_set[name]\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m param2:\n\u001b[0;32m--> 123\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot clone object \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, as the constructor \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meither does not set or modifies parameter \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator, name)\n\u001b[1;32m    126\u001b[0m         )\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# _sklearn_output_config is used by `set_output` to configure the output\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# container of an estimator.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_sklearn_output_config\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot clone object <class '__main__.CustomNet'>[uninitialized](\n  module=<class '__main__.MLP'>,\n  module__hidden_dim_feat=1000,\n  module__input_dim_feat=499500,\n  module__input_dim_target=1,\n  module__output_dim=2,\n), as the constructor either does not set or modifies parameter criterion_pft_cls"
     ]
    }
   ],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    estimator=projection_age_estimator,  # Your skorch wrapped model\n",
    "    X=features.squeeze(1),  # Combined features\n",
    "    y=targets,  # Ensure targets are in the correct shape\n",
    "    scoring='r2'  # Example scoring metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c14312f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display = LearningCurveDisplay(train_sizes=train_sizes, train_scores=train_scores, test_scores=test_scores, score_name=\"R2\")\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "90ba46e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f5d96d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
