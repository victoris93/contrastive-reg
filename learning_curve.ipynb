{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99de795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from nilearn.connectome import sym_matrix_to_vec\n",
    "from scipy.stats import pearsonr\n",
    "from cmath import isinf\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import math\n",
    "from cmath import isinf\n",
    "from sklearn.model_selection import train_test_split, KFold, LearningCurveDisplay, learning_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_percentage_error, r2_score\n",
    "import multiprocessing\n",
    "from skorch import NeuralNet\n",
    "from skorch.callbacks import Callback\n",
    "from skorch.helper import predefined_split\n",
    "from skorch.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d147112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f7d8635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader_to_numpy(loader):\n",
    "    features, targets = [], []\n",
    "    for feat, targ in loader:\n",
    "        features.append(feat.numpy())\n",
    "        targets.append(targ.numpy())\n",
    "    return np.concatenate(features), np.concatenate(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5aa9e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim_feat = 499500, input_dim_target = 1, hidden_dim_feat = 1000, output_dim = 2, dropout_rate = 0):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # Xavier initialization for feature MLP\n",
    "        self.feat_mlp = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_dim_feat),\n",
    "            nn.Linear(input_dim_feat, hidden_dim_feat),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(hidden_dim_feat, output_dim)\n",
    "        )\n",
    "        self.init_weights(self.feat_mlp)\n",
    "\n",
    "        # Xavier initialization for target MLP\n",
    "        self.target_mlp = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_dim_target),\n",
    "            nn.Linear(input_dim_target, output_dim)\n",
    "        )\n",
    "        self.init_weights(self.target_mlp)\n",
    "        \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        features = self.feat_mlp(x)\n",
    "        targets = self.target_mlp(y)\n",
    "        features = nn.functional.normalize(features, p=2, dim=1)\n",
    "        targets = nn.functional.normalize(targets, p=2, dim=1)\n",
    "        return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93673abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgeEstimator(BaseEstimator):\n",
    "    \"\"\" Define the age estimator on latent space network features.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        n_jobs = multiprocessing.cpu_count()\n",
    "        self.age_estimator = GridSearchCV(\n",
    "            Ridge(), param_grid={\"alpha\": 10.**np.arange(-2, 3)}, cv=5,\n",
    "            scoring=\"r2\", n_jobs=n_jobs)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.age_estimator.fit(X, y)\n",
    "        return self.score(X, y), self.r2(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = self.age_estimator.predict(X)\n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.age_estimator.predict(X)\n",
    "        return mean_absolute_percentage_error(y, y_pred)\n",
    "    \n",
    "    def r2(self, X, y):\n",
    "        y_pred = self.age_estimator.predict(X)\n",
    "        return r2_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5caa4aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatData(Dataset):\n",
    "    def __init__(self, path_feat, path_target, target_name, transform = None, train=True, train_size = 0.8, test_size=None, test_site = None, regions = None, threshold_mat = False, threshold_percent = None, random_state=42):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with the capability to handle training and testing splits, \n",
    "        including multiple views for augmented data.\n",
    "        \n",
    "        Args:\n",
    "            path_feat (str): Path to the features file.\n",
    "            path_target (str): Path to the target file.\n",
    "            transform (callable): A transformation function to apply for augmentation.\n",
    "            train (bool): Whether the dataset is used for training. False will load the test set.\n",
    "            test_size (float): Proportion of the dataset to include in the test split.\n",
    "            random_state (int): Random state for reproducible train-test splits.\n",
    "        \"\"\"\n",
    "        # Load the entire dataset\n",
    "        features = np.load(path_feat)      \n",
    "        participant_data = pd.read_csv(path_target)\n",
    "        targets = np.expand_dims(participant_data[target_name].values, axis = 1)\n",
    "        \n",
    "\n",
    "        # Split the dataset into training and test sets\n",
    "        if test_site is None:\n",
    "            train_indices, test_indices = train_test_split(np.arange(len(features)), \n",
    "                                                       train_size = train_size,\n",
    "                                                       test_size=test_size,                \n",
    "                                                       random_state=random_state)\n",
    "        else:\n",
    "            test_indices = participant_data.index[participant_data['dataset'] == test_site].values\n",
    "            train_indices = np.delete(np.arange(len(features)), test_indices)\n",
    "        \n",
    "        if train:\n",
    "            selected_indices = train_indices\n",
    "        else:\n",
    "            selected_indices = test_indices\n",
    "        \n",
    "        # Select the subset of data for the current mode (train/test)\n",
    "        features = features[selected_indices]\n",
    "        if threshold_mat:\n",
    "            thresholded_feat = []\n",
    "            for matrix in features:\n",
    "                threshold = np.percentile(matrix, threshold_percent)\n",
    "                matrix[matrix < threshold] = 0\n",
    "                thresholded_feat.append(matrix)\n",
    "            threshold_feat = np.stack(thresholded_feat)\n",
    "            features = threshold_feat\n",
    "        targets = targets[selected_indices]\n",
    "        \n",
    "\n",
    "        self.n_sub = len(features)\n",
    "        self.n_views = 1\n",
    "        self.transform = transform\n",
    "        self.targets = targets\n",
    "        \n",
    "        vectorized_feat = np.array([sym_matrix_to_vec(mat, discard_diagonal=True) for mat in features])\n",
    "        self.n_features = vectorized_feat.shape[-1]\n",
    "        \n",
    "        if (train and transform is not None):\n",
    "            # augmentation only in training mode\n",
    "            if transform != \"copy\":\n",
    "                augmented_features = np.array([self.transform(sample, regions = regions) for sample in features])\n",
    "\n",
    "                self.n_views = self.n_views + augmented_features.shape[1]\n",
    "                self.features = np.zeros((self.n_sub, self.n_views, self.n_features))\n",
    "                for sub in range(self.n_sub):\n",
    "                    self.features[sub, 0, :] = vectorized_feat[sub]\n",
    "                    self.features[sub, 1:, :] = augmented_features[sub]\n",
    "            else:\n",
    "                self.features = np.repeat(np.expand_dims(vectorized_feat, axis = 1), 2, axis=1)\n",
    "        else:\n",
    "            self.features = np.expand_dims(vectorized_feat, axis = 1)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.n_sub\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.features[idx]\n",
    "        targets = self.targets[idx]\n",
    "        features = torch.from_numpy(features).float()\n",
    "        targets = torch.from_numpy(targets).float()\n",
    "        \n",
    "        return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f506c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNet(NeuralNet):\n",
    "    def __init__(self, module, estimator = None, criterion_pft=None, criterion_ptt=None, n_views = None, **kwargs):\n",
    "        super(CustomNet, self).__init__(module, **kwargs)\n",
    "        self.criterion_pft_cls = criterion_pft\n",
    "        self.criterion_ptt_cls = criterion_ptt\n",
    "        self.n_views = n_views\n",
    "        self.estimator = estimator\n",
    "        \n",
    "    def fit(self, X_train, y_train, **fit_params):\n",
    "        self.X_train_ = X_train\n",
    "        self.y_train_ = y_train\n",
    "        super().fit(X, y, **fit_params)\n",
    "        print(\"Model fitted\")\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        features, targets = X.to(self.device), y.to(self.device)\n",
    "        out_feat, _ = self.module_(features, targets)\n",
    "        self.estimator = self.estimator.fit(out_feat.cpu().numpy())\n",
    "        y_pred = self.estimator.predict(out_feat.cpu().numpy())\n",
    "        r2 = self.estimator.r2(out_feat.cpu().numpy())\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        params = super(CustomNet, self).get_params(deep=deep)\n",
    "        params.update({\n",
    "            'criterion_pft_cls': self.criterion_pft_cls,\n",
    "            'criterion_ptt_cls': self.criterion_ptt_cls,\n",
    "            'n_views': self.n_views,\n",
    "            'estimator': self.estimator,\n",
    "        })\n",
    "        return params\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        super(CustomNet, self).set_params(**{k: v for k, v in parameters.items() if k not in ['criterion_pft', 'criterion_ptt', 'n_views', 'estimator']})\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def initialize_criterion(self):\n",
    "        super().initialize_criterion()  # Initialize default criterion first\n",
    "        # Setup custom criterion with parameters from get_params_for\n",
    "        if self.criterion_pft_cls is not None:\n",
    "            self.criterion_pft_ = self.criterion_pft_cls(**self.get_params_for('criterion_pft'))\n",
    "        if self.criterion_ptt_cls is not None:\n",
    "            self.criterion_ptt_ = self.criterion_ptt_cls(**self.get_params_for('criterion_ptt'))\n",
    "        return self\n",
    "    \n",
    "    def train_step_single(self, batch, **fit_params):\n",
    "        self.module_.train()\n",
    "        features, targets = batch\n",
    "        features, targets = features.to(self.device), targets.to(self.device)\n",
    "        self.optimizer_.zero_grad()\n",
    "        mlp_out = self.module_(features, targets)  # Pass both Xi and yi to forward\n",
    "        loss = self.get_loss(mlp_out, targets, training=True)\n",
    "        loss.backward()\n",
    "        self.optimizer_.step()\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def validation_step(self, batch, **fit_params):\n",
    "        self.module_.eval()  # Set the module in evaluation mode\n",
    "        features = self.X_test_\n",
    "        targets = self.y_test_\n",
    "        with torch.no_grad():\n",
    "            features, targets = features.to(self.device), targets.to(self.device)\n",
    "            out_feat, out_target = self.module_(features, targets)\n",
    "        \n",
    "        loss = self.get_loss((out_feat, out_target), targets, training=False)\n",
    "        \n",
    "        if hasattr(self, 'train_proj_'):\n",
    "            self.estimator = self.estimator.fit(self.train_proj_.cpu().numpy())\n",
    "            y_pred = self.estimator.predict(out_feat.cpu().numpy())\n",
    "            r2 = self.estimator.r2(out_feat.cpu().numpy())\n",
    "        else:\n",
    "            y_pred = None\n",
    "            r2 = None\n",
    "        return {'loss': loss, 'y_pred': y_pred, 'r2':r2}\n",
    "\n",
    "    def get_loss(self, mlp_out, targets, training=False):\n",
    "        n_views = self.n_views\n",
    "        out_feat, out_target = mlp_out\n",
    "        bsz = out_feat.shape[0]\n",
    "        out_feat = torch.split(out_feat, [bsz]*n_views, dim=0)\n",
    "        out_feat = torch.cat([f.unsqueeze(1) for f in out_feat], dim=1)\n",
    "        \n",
    "        loss = self.criterion_pft_(out_feat, targets)\n",
    "        out_target = torch.split(out_target, [bsz]*n_views, dim=0)\n",
    "        out_target = torch.cat([f.unsqueeze(1) for f in out_target], dim=1)\n",
    "        \n",
    "        loss += self.criterion_ptt_(out_target, targets)\n",
    "        loss += torch.nn.functional.mse_loss(out_feat.view(bsz * n_views, 2), out_target.view(bsz * n_views, 2))\n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "051458d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjCallback(Callback):\n",
    "    def on_train_end(self, net, **kwargs):\n",
    "        X_train = getattr(net, 'X_train_', None)\n",
    "        y_train = getattr(net, 'y_train_', None)\n",
    "        net.module_.eval()\n",
    "        with torch.no_grad():\n",
    "            X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "            y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "            features, _ = net.module_(X_train_tensor, y_train_tensor)\n",
    "            net.train_proj_ = features\n",
    "\n",
    "class InitializeCallback(Callback):\n",
    "    def on_train_begin(self, net, X=None, y=None, **kwargs):\n",
    "        if not hasattr(net, 'estimator'):\n",
    "            net.estimator = AgeEstimator()\n",
    "        if not hasattr(net, 'criterion_pft_'):\n",
    "            net.criterion_pft_ = net.criterion_pft(**net.get_params_for('criterion_pft'))\n",
    "        if not hasattr(net, 'criterion_ptt_'):\n",
    "            net.criterion_ptt_ = net.criterion_ptt(**net.get_params_for('criterion_ptt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7aeeb48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x, krnl_sigma):\n",
    "    x = x - x.T\n",
    "    return torch.exp(-(x**2) / (2*(krnl_sigma**2))) / (math.sqrt(2*torch.pi)*krnl_sigma)\n",
    "\n",
    "def cauchy(x, krnl_sigma):\n",
    "        x = x - x.T\n",
    "        return  1. / (krnl_sigma*(x**2) + 1)\n",
    "\n",
    "def rbf(x, krnl_sigma):\n",
    "        x = x - x.T\n",
    "        return torch.exp(-(x**2)/(2*(krnl_sigma**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f739858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss from: https://github.com/EIDOSLAB/contrastive-brain-age-prediction/blob/master/src/losses.py\n",
    "# modified to accept input shape [bsz, n_feats]. In the age paper: [bsz, n_views, n_feats].\n",
    "class KernelizedSupCon(nn.Module):\n",
    "    \"\"\"Supervised contrastive loss: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\n",
    "    Based on: https://github.com/HobbitLong/SupContrast\"\"\"\n",
    "    def __init__(self, method: str='expw', temperature: float=0.03, contrast_mode: str='all',\n",
    "                 base_temperature: float=0.07, krnl_sigma: float = 1., kernel: callable=cauchy, delta_reduction: str='sum'):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "        self.method = method\n",
    "        self.kernel = kernel\n",
    "        self.krnl_sigma = krnl_sigma\n",
    "        self.delta_reduction = delta_reduction\n",
    "\n",
    "        if kernel is not None and method == 'supcon':\n",
    "            raise ValueError('Kernel must be none if method=supcon')\n",
    "        \n",
    "        if kernel is None and method != 'supcon':\n",
    "            raise ValueError('Kernel must not be none if method != supcon')\n",
    "\n",
    "        if delta_reduction not in ['mean', 'sum']:\n",
    "            raise ValueError(f\"Invalid reduction {delta_reduction}\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__} ' \\\n",
    "               f'(t={self.temperature}, ' \\\n",
    "               f'method={self.method}, ' \\\n",
    "               f'kernel={self.kernel is not None}, ' \\\n",
    "               f'delta_reduction={self.delta_reduction})'\n",
    "\n",
    "    def forward(self, features, labels=None):\n",
    "        \"\"\"Compute loss for model. If `labels` is None, \n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, n_features]. \n",
    "                input has to be rearranged to [bsz, n_views, n_features] and labels [bsz],\n",
    "            labels: ground truth of shape [bsz].\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "        device = features.device\n",
    "\n",
    "        if len(features.shape) != 3:\n",
    "            raise ValueError('`features` needs to be [bsz, n_views, n_feats],'\n",
    "                             '3 dimensions are required')\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        contrast_count = features.shape[1]\n",
    "\n",
    "        if labels is None:\n",
    "            mask = torch.eye(batch_size, device=device)\n",
    "        \n",
    "        else:\n",
    "            labels = labels.view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError('Num of labels does not match num of features')\n",
    "            \n",
    "            if self.kernel is None:\n",
    "                mask = torch.eq(labels, labels.T)\n",
    "            else:\n",
    "                mask = self.kernel(labels, krnl_sigma = self.krnl_sigma)     \n",
    "        \n",
    "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == 'one':\n",
    "            anchor_feature = features[:, 0]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == 'all':\n",
    "            anchor_feature = contrast_feature\n",
    "            anchor_count = contrast_count\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
    "\n",
    "        # Tile mask\n",
    "        mask = mask.repeat(anchor_count, contrast_count)\n",
    "        # Inverse of torch-eye to remove self-contrast (diagonal)\n",
    "        inv_diagonal = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size*anchor_count, device=device).view(-1, 1),\n",
    "            0\n",
    "        )\n",
    "        # compute similarity\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            self.temperature\n",
    "        )\n",
    "\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        alignment = logits \n",
    "\n",
    "        # base case is:\n",
    "        # - supcon if kernel = none \n",
    "        # - y-aware is kernel != none\n",
    "        uniformity = torch.exp(logits) * inv_diagonal \n",
    "\n",
    "        if self.method == 'threshold':\n",
    "            repeated = mask.unsqueeze(-1).repeat(1, 1, mask.shape[0]) # repeat kernel mask\n",
    "\n",
    "            delta = (mask[:, None].T - repeated.T).transpose(1, 2) # compute the difference w_k - w_j for every k,j\n",
    "            delta = (delta > 0.).float()\n",
    "\n",
    "            # for each z_i, repel only samples j s.t. K(z_i, z_j) < K(z_i, z_k)\n",
    "            uniformity = uniformity.unsqueeze(-1).repeat(1, 1, mask.shape[0])\n",
    "\n",
    "            if self.delta_reduction == 'mean':\n",
    "                uniformity = (uniformity * delta).mean(-1)\n",
    "            else:\n",
    "                uniformity = (uniformity * delta).sum(-1)\n",
    "    \n",
    "        elif self.method == 'expw':\n",
    "            # exp weight e^(s_j(1-w_j))\n",
    "            uniformity = torch.exp(logits * (1 - mask)) * inv_diagonal\n",
    "\n",
    "        uniformity = torch.log(uniformity.sum(1, keepdim=True))\n",
    "\n",
    "\n",
    "        # positive mask contains the anchor-positive pairs\n",
    "        # excluding <self,self> on the diagonal\n",
    "        positive_mask = mask * inv_diagonal\n",
    "\n",
    "        log_prob = alignment - uniformity # log(alignment/uniformity) = log(alignment) - log(uniformity)\n",
    "        log_prob = (positive_mask * log_prob).sum(1) / positive_mask.sum(1) # compute mean of log-likelihood over positive\n",
    " \n",
    "        # loss\n",
    "        loss = - (self.temperature / self.base_temperature) * log_prob\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0f6722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "train_dataset = MatData(\"matrices.npy\", \"participants.csv\", \"age\", train=True, train_size = 100, test_size = 50)\n",
    "test_dataset = MatData(\"matrices.npy\", \"participants.csv\", \"age\", train=False, train_size = 100, test_size = 50)\n",
    "dataset = ConcatDataset([train_dataset, test_dataset])\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle = False)\n",
    "X, y = loader_to_numpy(dataloader)\n",
    "X = X.squeeze(1)\n",
    "\n",
    "X_train, y_train = X[:100], y[:100]\n",
    "X_test, y_test = X[100:], y[100:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4798efc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "78f087cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomNet(\n",
    "    module=MLP,\n",
    "    optimizer=optim.Adam,\n",
    "    lr = 0.01,\n",
    "    estimator = AgeEstimator(),\n",
    "    module__input_dim_feat = 499500,\n",
    "    module__hidden_dim_feat = 1000,\n",
    "    module__input_dim_target = 1,\n",
    "    module__output_dim = 2,\n",
    "    train_split=predefined_split(test_dataset),\n",
    "    n_views = 1,\n",
    "    callbacks=[ProjCallback()],\n",
    "    criterion_pft=KernelizedSupCon,\n",
    "    criterion_ptt=KernelizedSupCon,\n",
    "    criterion=torch.nn.MSELoss(),  # Default criterion, actual loss handled in get_loss\n",
    "    batch_size=20,\n",
    "    max_epochs=10,\n",
    "    device='cuda',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7977798f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "validation_step() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m, in \u001b[0;36mCustomNet.fit\u001b[0;34m(self, X_train, y_train, X_test, y_test, **fit_params)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_test_ \u001b[38;5;241m=\u001b[39m X_test\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_test_ \u001b[38;5;241m=\u001b[39m y_test\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/skorch/net.py:1319\u001b[0m, in \u001b[0;36mNeuralNet.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarm_start \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialized_:\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize()\n\u001b[0;32m-> 1319\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/skorch/net.py:1278\u001b[0m, in \u001b[0;36mNeuralNet.partial_fit\u001b[0;34m(self, X, y, classes, **fit_params)\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_train_begin\u001b[39m\u001b[38;5;124m'\u001b[39m, X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my)\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/skorch/net.py:1193\u001b[0m, in \u001b[0;36mNeuralNet.fit_loop\u001b[0;34m(self, X, y, epochs, **fit_params)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_epoch_begin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mon_epoch_kwargs)\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single_epoch(iterator_train, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1191\u001b[0m                           step_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m-> 1193\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mstep_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mon_epoch_kwargs)\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/skorch/net.py:1226\u001b[0m, in \u001b[0;36mNeuralNet.run_single_epoch\u001b[0;34m(self, iterator, training, prefix, step_fn, **fit_params)\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m   1225\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_batch_begin\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch\u001b[38;5;241m=\u001b[39mbatch, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[0;32m-> 1226\u001b[0m     step \u001b[38;5;241m=\u001b[39m \u001b[43mstep_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mrecord_batch(prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, step[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m   1228\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m (get_len(batch[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m))\n\u001b[1;32m   1229\u001b[0m                   \u001b[38;5;28;01melse\u001b[39;00m get_len(batch))\n",
      "\u001b[0;31mTypeError\u001b[0m: validation_step() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fb665f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers\n",
    "import time\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from contextlib import suppress\n",
    "from functools import partial\n",
    "from numbers import Real\n",
    "from traceback import format_exc\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from joblib import logger\n",
    "\n",
    "from sklearn.base import clone, is_classifier\n",
    "from sklearn.exceptions import FitFailedWarning\n",
    "from sklearn.metrics import check_scoring, get_scorer_names\n",
    "from sklearn.metrics._scorer import _check_multimetric_scoring, _MultimetricScorer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import _safe_indexing, check_random_state, indexable\n",
    "from sklearn.utils._param_validation import (\n",
    "    HasMethods,\n",
    "    Integral,\n",
    "    Interval,\n",
    "    StrOptions,\n",
    "    validate_params,\n",
    ")\n",
    "from sklearn.utils.metaestimators import _safe_split\n",
    "from sklearn.utils.parallel import Parallel, delayed\n",
    "from sklearn.utils.validation import _check_fit_params, _num_samples\n",
    "from sklearn.model_selection._split import check_cv\n",
    "\n",
    "def create_model():\n",
    "    # Initialize a new instance of CustomNet\n",
    "    # Ensure to set up the model with all necessary initializations\n",
    "    return CustomNet(\n",
    "        module=MLP,\n",
    "        optimizer=optim.Adam,\n",
    "        lr = 0.01,\n",
    "        estimator = AgeEstimator(),\n",
    "        module__input_dim_feat = 499500,\n",
    "        module__hidden_dim_feat = 1000,\n",
    "        module__input_dim_target = 1,\n",
    "        module__output_dim = 2,\n",
    "        train_split=predefined_split(test_dataset),\n",
    "        n_views = 1,\n",
    "        callbacks=[ProjCallback()],\n",
    "        criterion_pft=KernelizedSupCon,\n",
    "        criterion_ptt=KernelizedSupCon,\n",
    "        criterion=torch.nn.MSELoss(),  # Default criterion, actual loss handled in get_loss\n",
    "        batch_size=20,\n",
    "        max_epochs=10,\n",
    "        device='cuda')\n",
    "\n",
    "def learning_curve(\n",
    "    estimator,\n",
    "    X,\n",
    "    y,\n",
    "    *,\n",
    "    groups=None,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "    cv=None,\n",
    "    scoring=None,\n",
    "    exploit_incremental_learning=False,\n",
    "    n_jobs=None,\n",
    "    pre_dispatch=\"all\",\n",
    "    verbose=0,\n",
    "    shuffle=False,\n",
    "    random_state=None,\n",
    "    error_score=np.nan,\n",
    "    return_times=False,\n",
    "    fit_params=None,\n",
    "    clone_func=clone\n",
    "):\n",
    "    \"\"\"Learning curve.\n",
    "\n",
    "    Determines cross-validated training and test scores for different training\n",
    "    set sizes.\n",
    "\n",
    "    A cross-validation generator splits the whole dataset k times in training\n",
    "    and test data. Subsets of the training set with varying sizes will be used\n",
    "    to train the estimator and a score for each training subset size and the\n",
    "    test set will be computed. Afterwards, the scores will be averaged over\n",
    "    all k runs for each training subset size.\n",
    "\n",
    "    Read more in the :ref:`User Guide <learning_curve>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" method\n",
    "        An object of that type which is cloned for each validation. It must\n",
    "        also implement \"predict\" unless `scoring` is a callable that doesn't\n",
    "        rely on \"predict\" to compute a score.\n",
    "\n",
    "    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "        Training vector, where `n_samples` is the number of samples and\n",
    "        `n_features` is the number of features.\n",
    "\n",
    "    y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    groups : array-like of shape (n_samples,), default=None\n",
    "        Group labels for the samples used while splitting the dataset into\n",
    "        train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
    "        instance (e.g., :class:`GroupKFold`).\n",
    "\n",
    "    train_sizes : array-like of shape (n_ticks,), \\\n",
    "            default=np.linspace(0.1, 1.0, 5)\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the dtype is float, it is regarded as a\n",
    "        fraction of the maximum size of the training set (that is determined\n",
    "        by the selected validation method), i.e. it has to be within (0, 1].\n",
    "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
    "        Note that for classification the number of samples usually have to\n",
    "        be big enough to contain at least one sample from each class.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, default=None\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "\n",
    "        - None, to use the default 5-fold cross validation,\n",
    "        - int, to specify the number of folds in a `(Stratified)KFold`,\n",
    "        - :term:`CV splitter`,\n",
    "        - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For int/None inputs, if the estimator is a classifier and ``y`` is\n",
    "        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
    "        other cases, :class:`KFold` is used. These splitters are instantiated\n",
    "        with `shuffle=False` so the splits will be the same across calls.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validation strategies that can be used here.\n",
    "\n",
    "        .. versionchanged:: 0.22\n",
    "            ``cv`` default value if None changed from 3-fold to 5-fold.\n",
    "\n",
    "    scoring : str or callable, default=None\n",
    "        A str (see model evaluation documentation) or\n",
    "        a scorer callable object / function with signature\n",
    "        ``scorer(estimator, X, y)``.\n",
    "\n",
    "    exploit_incremental_learning : bool, default=False\n",
    "        If the estimator supports incremental learning, this will be\n",
    "        used to speed up fitting for different training set sizes.\n",
    "\n",
    "    n_jobs : int, default=None\n",
    "        Number of jobs to run in parallel. Training the estimator and computing\n",
    "        the score are parallelized over the different training and test sets.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    pre_dispatch : int or str, default='all'\n",
    "        Number of predispatched jobs for parallel execution (default is\n",
    "        all). The option can reduce the allocated memory. The str can\n",
    "        be an expression like '2*n_jobs'.\n",
    "\n",
    "    verbose : int, default=0\n",
    "        Controls the verbosity: the higher, the more messages.\n",
    "\n",
    "    shuffle : bool, default=False\n",
    "        Whether to shuffle training data before taking prefixes of it\n",
    "        based on``train_sizes``.\n",
    "\n",
    "    random_state : int, RandomState instance or None, default=None\n",
    "        Used when ``shuffle`` is True. Pass an int for reproducible\n",
    "        output across multiple function calls.\n",
    "        See :term:`Glossary <random_state>`.\n",
    "\n",
    "    error_score : 'raise' or numeric, default=np.nan\n",
    "        Value to assign to the score if an error occurs in estimator fitting.\n",
    "        If set to 'raise', the error is raised.\n",
    "        If a numeric value is given, FitFailedWarning is raised.\n",
    "\n",
    "        .. versionadded:: 0.20\n",
    "\n",
    "    return_times : bool, default=False\n",
    "        Whether to return the fit and score times.\n",
    "\n",
    "    fit_params : dict, default=None\n",
    "        Parameters to pass to the fit method of the estimator.\n",
    "\n",
    "        .. versionadded:: 0.24\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_sizes_abs : array of shape (n_unique_ticks,)\n",
    "        Numbers of training examples that has been used to generate the\n",
    "        learning curve. Note that the number of ticks might be less\n",
    "        than n_ticks because duplicate entries will be removed.\n",
    "\n",
    "    train_scores : array of shape (n_ticks, n_cv_folds)\n",
    "        Scores on training sets.\n",
    "\n",
    "    test_scores : array of shape (n_ticks, n_cv_folds)\n",
    "        Scores on test set.\n",
    "\n",
    "    fit_times : array of shape (n_ticks, n_cv_folds)\n",
    "        Times spent for fitting in seconds. Only present if ``return_times``\n",
    "        is True.\n",
    "\n",
    "    score_times : array of shape (n_ticks, n_cv_folds)\n",
    "        Times spent for scoring in seconds. Only present if ``return_times``\n",
    "        is True.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.datasets import make_classification\n",
    "    >>> from sklearn.tree import DecisionTreeClassifier\n",
    "    >>> from sklearn.model_selection import learning_curve\n",
    "    >>> X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n",
    "    >>> tree = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "    >>> train_size_abs, train_scores, test_scores = learning_curve(\n",
    "    ...     tree, X, y, train_sizes=[0.3, 0.6, 0.9]\n",
    "    ... )\n",
    "    >>> for train_size, cv_train_scores, cv_test_scores in zip(\n",
    "    ...     train_size_abs, train_scores, test_scores\n",
    "    ... ):\n",
    "    ...     print(f\"{train_size} samples were used to train the model\")\n",
    "    ...     print(f\"The average train accuracy is {cv_train_scores.mean():.2f}\")\n",
    "    ...     print(f\"The average test accuracy is {cv_test_scores.mean():.2f}\")\n",
    "    24 samples were used to train the model\n",
    "    The average train accuracy is 1.00\n",
    "    The average test accuracy is 0.85\n",
    "    48 samples were used to train the model\n",
    "    The average train accuracy is 1.00\n",
    "    The average test accuracy is 0.90\n",
    "    72 samples were used to train the model\n",
    "    The average train accuracy is 1.00\n",
    "    The average test accuracy is 0.93\n",
    "    \"\"\"\n",
    "    if exploit_incremental_learning and not hasattr(estimator, \"partial_fit\"):\n",
    "        raise ValueError(\n",
    "            \"An estimator must support the partial_fit interface \"\n",
    "            \"to exploit incremental learning\"\n",
    "        )\n",
    "    X, y, groups = indexable(X, y, groups)\n",
    "\n",
    "    cv = check_cv(cv, y, classifier=is_classifier(estimator))\n",
    "    # Store it as list as we will be iterating over the list multiple times\n",
    "    cv_iter = list(cv.split(X, y, groups))\n",
    "\n",
    "    scorer = check_scoring(estimator, scoring=scoring)\n",
    "    print(scorer)\n",
    "\n",
    "    n_max_training_samples = len(cv_iter[0][0])\n",
    "    # Because the lengths of folds can be significantly different, it is\n",
    "    # not guaranteed that we use all of the available training data when we\n",
    "    # use the first 'n_max_training_samples' samples.\n",
    "    train_sizes_abs = _translate_train_sizes(train_sizes, n_max_training_samples)\n",
    "    n_unique_ticks = train_sizes_abs.shape[0]\n",
    "    if verbose > 0:\n",
    "        print(\"[learning_curve] Training set sizes: \" + str(train_sizes_abs))\n",
    "\n",
    "    parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch, verbose=verbose)\n",
    "\n",
    "    if shuffle:\n",
    "        rng = check_random_state(random_state)\n",
    "        cv_iter = ((rng.permutation(train), test) for train, test in cv_iter)\n",
    "\n",
    "    if exploit_incremental_learning:\n",
    "        classes = np.unique(y) if is_classifier(estimator) else None\n",
    "        out = parallel(\n",
    "            delayed(_incremental_fit_estimator)(\n",
    "                create_model(),\n",
    "                X,\n",
    "                y,\n",
    "                classes,\n",
    "                train,\n",
    "                test,\n",
    "                train_sizes_abs,\n",
    "                scorer,\n",
    "                verbose,\n",
    "                return_times,\n",
    "                error_score=error_score,\n",
    "                fit_params=fit_params,\n",
    "            )\n",
    "            for train, test in cv_iter\n",
    "        )\n",
    "        out = np.asarray(out).transpose((2, 1, 0))\n",
    "    else:\n",
    "        train_test_proportions = []\n",
    "        for train, test in cv_iter:\n",
    "            for n_train_samples in train_sizes_abs:\n",
    "                train_test_proportions.append((train[:n_train_samples], test))\n",
    "\n",
    "        results = parallel(\n",
    "            delayed(_fit_and_score)(\n",
    "                create_model(),\n",
    "                X,\n",
    "                y,\n",
    "                scorer,\n",
    "                train,\n",
    "                test,\n",
    "                verbose,\n",
    "                parameters=None,\n",
    "                fit_params=fit_params,\n",
    "                return_train_score=True,\n",
    "                error_score=error_score,\n",
    "                return_times=return_times,\n",
    "            )\n",
    "            for train, test in train_test_proportions\n",
    "        )\n",
    "        results = _aggregate_score_dicts(results)\n",
    "        train_scores = results[\"train_scores\"].reshape(-1, n_unique_ticks).T\n",
    "        test_scores = results[\"test_scores\"].reshape(-1, n_unique_ticks).T\n",
    "        out = [train_scores, test_scores]\n",
    "\n",
    "        if return_times:\n",
    "            fit_times = results[\"fit_time\"].reshape(-1, n_unique_ticks).T\n",
    "            score_times = results[\"score_time\"].reshape(-1, n_unique_ticks).T\n",
    "            out.extend([fit_times, score_times])\n",
    "\n",
    "    ret = train_sizes_abs, out[0], out[1]\n",
    "\n",
    "    if return_times:\n",
    "        ret = ret + (out[2], out[3])\n",
    "\n",
    "    return ret\n",
    "\n",
    "def _translate_train_sizes(train_sizes, n_max_training_samples):\n",
    "    \"\"\"Determine absolute sizes of training subsets and validate 'train_sizes'.\n",
    "\n",
    "    Examples:\n",
    "        _translate_train_sizes([0.5, 1.0], 10) -> [5, 10]\n",
    "        _translate_train_sizes([5, 10], 10) -> [5, 10]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_sizes : array-like of shape (n_ticks,)\n",
    "        Numbers of training examples that will be used to generate the\n",
    "        learning curve. If the dtype is float, it is regarded as a\n",
    "        fraction of 'n_max_training_samples', i.e. it has to be within (0, 1].\n",
    "\n",
    "    n_max_training_samples : int\n",
    "        Maximum number of training samples (upper bound of 'train_sizes').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_sizes_abs : array of shape (n_unique_ticks,)\n",
    "        Numbers of training examples that will be used to generate the\n",
    "        learning curve. Note that the number of ticks might be less\n",
    "        than n_ticks because duplicate entries will be removed.\n",
    "    \"\"\"\n",
    "    train_sizes_abs = np.asarray(train_sizes)\n",
    "    n_ticks = train_sizes_abs.shape[0]\n",
    "    n_min_required_samples = np.min(train_sizes_abs)\n",
    "    n_max_required_samples = np.max(train_sizes_abs)\n",
    "    if np.issubdtype(train_sizes_abs.dtype, np.floating):\n",
    "        if n_min_required_samples <= 0.0 or n_max_required_samples > 1.0:\n",
    "            raise ValueError(\n",
    "                \"train_sizes has been interpreted as fractions \"\n",
    "                \"of the maximum number of training samples and \"\n",
    "                \"must be within (0, 1], but is within [%f, %f].\"\n",
    "                % (n_min_required_samples, n_max_required_samples)\n",
    "            )\n",
    "        train_sizes_abs = (train_sizes_abs * n_max_training_samples).astype(\n",
    "            dtype=int, copy=False\n",
    "        )\n",
    "        train_sizes_abs = np.clip(train_sizes_abs, 1, n_max_training_samples)\n",
    "    else:\n",
    "        if (\n",
    "            n_min_required_samples <= 0\n",
    "            or n_max_required_samples > n_max_training_samples\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"train_sizes has been interpreted as absolute \"\n",
    "                \"numbers of training samples and must be within \"\n",
    "                \"(0, %d], but is within [%d, %d].\"\n",
    "                % (\n",
    "                    n_max_training_samples,\n",
    "                    n_min_required_samples,\n",
    "                    n_max_required_samples,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    train_sizes_abs = np.unique(train_sizes_abs)\n",
    "    if n_ticks > train_sizes_abs.shape[0]:\n",
    "        warnings.warn(\n",
    "            \"Removed duplicate entries from 'train_sizes'. Number \"\n",
    "            \"of ticks will be less than the size of \"\n",
    "            \"'train_sizes': %d instead of %d.\" % (train_sizes_abs.shape[0], n_ticks),\n",
    "            RuntimeWarning,\n",
    "        )\n",
    "\n",
    "    return train_sizes_abs\n",
    "\n",
    "def _fit_and_score(\n",
    "    estimator,\n",
    "    X,\n",
    "    y,\n",
    "    scorer,\n",
    "    train,\n",
    "    test,\n",
    "    verbose,\n",
    "    parameters,\n",
    "    fit_params,\n",
    "    return_train_score=False,\n",
    "    return_parameters=False,\n",
    "    return_n_test_samples=False,\n",
    "    return_times=False,\n",
    "    return_estimator=False,\n",
    "    split_progress=None,\n",
    "    candidate_progress=None,\n",
    "    error_score=np.nan,\n",
    "):\n",
    "    \"\"\"Fit estimator and compute scores for a given dataset split.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator object implementing 'fit'\n",
    "        The object to use to fit the data.\n",
    "\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        The data to fit.\n",
    "\n",
    "    y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None\n",
    "        The target variable to try to predict in the case of\n",
    "        supervised learning.\n",
    "\n",
    "    scorer : A single callable or dict mapping scorer name to the callable\n",
    "        If it is a single callable, the return value for ``train_scores`` and\n",
    "        ``test_scores`` is a single float.\n",
    "\n",
    "        For a dict, it should be one mapping the scorer name to the scorer\n",
    "        callable object / function.\n",
    "\n",
    "        The callable object / fn should have signature\n",
    "        ``scorer(estimator, X, y)``.\n",
    "\n",
    "    train : array-like of shape (n_train_samples,)\n",
    "        Indices of training samples.\n",
    "\n",
    "    test : array-like of shape (n_test_samples,)\n",
    "        Indices of test samples.\n",
    "\n",
    "    verbose : int\n",
    "        The verbosity level.\n",
    "\n",
    "    error_score : 'raise' or numeric, default=np.nan\n",
    "        Value to assign to the score if an error occurs in estimator fitting.\n",
    "        If set to 'raise', the error is raised.\n",
    "        If a numeric value is given, FitFailedWarning is raised.\n",
    "\n",
    "    parameters : dict or None\n",
    "        Parameters to be set on the estimator.\n",
    "\n",
    "    fit_params : dict or None\n",
    "        Parameters that will be passed to ``estimator.fit``.\n",
    "\n",
    "    return_train_score : bool, default=False\n",
    "        Compute and return score on training set.\n",
    "\n",
    "    return_parameters : bool, default=False\n",
    "        Return parameters that has been used for the estimator.\n",
    "\n",
    "    split_progress : {list, tuple} of int, default=None\n",
    "        A list or tuple of format (<current_split_id>, <total_num_of_splits>).\n",
    "\n",
    "    candidate_progress : {list, tuple} of int, default=None\n",
    "        A list or tuple of format\n",
    "        (<current_candidate_id>, <total_number_of_candidates>).\n",
    "\n",
    "    return_n_test_samples : bool, default=False\n",
    "        Whether to return the ``n_test_samples``.\n",
    "\n",
    "    return_times : bool, default=False\n",
    "        Whether to return the fit/score times.\n",
    "\n",
    "    return_estimator : bool, default=False\n",
    "        Whether to return the fitted estimator.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : dict with the following attributes\n",
    "        train_scores : dict of scorer name -> float\n",
    "            Score on training set (for all the scorers),\n",
    "            returned only if `return_train_score` is `True`.\n",
    "        test_scores : dict of scorer name -> float\n",
    "            Score on testing set (for all the scorers).\n",
    "        n_test_samples : int\n",
    "            Number of test samples.\n",
    "        fit_time : float\n",
    "            Time spent for fitting in seconds.\n",
    "        score_time : float\n",
    "            Time spent for scoring in seconds.\n",
    "        parameters : dict or None\n",
    "            The parameters that have been evaluated.\n",
    "        estimator : estimator object\n",
    "            The fitted estimator.\n",
    "        fit_error : str or None\n",
    "            Traceback str if the fit failed, None if the fit succeeded.\n",
    "    \"\"\"\n",
    "    if not isinstance(error_score, numbers.Number) and error_score != \"raise\":\n",
    "        raise ValueError(\n",
    "            \"error_score must be the string 'raise' or a numeric value. \"\n",
    "            \"(Hint: if using 'raise', please make sure that it has been \"\n",
    "            \"spelled correctly.)\"\n",
    "        )\n",
    "\n",
    "    progress_msg = \"\"\n",
    "    if verbose > 2:\n",
    "        if split_progress is not None:\n",
    "            progress_msg = f\" {split_progress[0]+1}/{split_progress[1]}\"\n",
    "        if candidate_progress and verbose > 9:\n",
    "            progress_msg += f\"; {candidate_progress[0]+1}/{candidate_progress[1]}\"\n",
    "\n",
    "    if verbose > 1:\n",
    "        if parameters is None:\n",
    "            params_msg = \"\"\n",
    "        else:\n",
    "            sorted_keys = sorted(parameters)  # Ensure deterministic o/p\n",
    "            params_msg = \", \".join(f\"{k}={parameters[k]}\" for k in sorted_keys)\n",
    "    if verbose > 9:\n",
    "        start_msg = f\"[CV{progress_msg}] START {params_msg}\"\n",
    "        print(f\"{start_msg}{(80 - len(start_msg)) * '.'}\")\n",
    "\n",
    "    # Adjust length of sample weights\n",
    "    fit_params = fit_params if fit_params is not None else {}\n",
    "    fit_params = _check_fit_params(X, fit_params, train)\n",
    "\n",
    "    if parameters is not None:\n",
    "        # here we clone the parameters, since sometimes the parameters\n",
    "        # themselves might be estimators, e.g. when we search over different\n",
    "        # estimators in a pipeline.\n",
    "        # ref: https://github.com/scikit-learn/scikit-learn/pull/26786\n",
    "        estimator = estimator.set_params(**clone(parameters, safe=False))\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    X_train, y_train = _safe_split(estimator, X, y, train)\n",
    "    X_test, y_test = _safe_split(estimator, X, y, test, train)\n",
    "\n",
    "    result = {}\n",
    "    try:\n",
    "        if y_train is None:\n",
    "            estimator.fit(X_train, **fit_params)\n",
    "        else:\n",
    "            estimator.fit(X_train, y_train, **fit_params)\n",
    "            print(X_train, y_train)\n",
    "\n",
    "    except Exception:\n",
    "        # Note fit time as time until error\n",
    "        fit_time = time.time() - start_time\n",
    "        score_time = 0.0\n",
    "        if error_score == \"raise\":\n",
    "            raise\n",
    "        elif isinstance(error_score, numbers.Number):\n",
    "            if isinstance(scorer, dict):\n",
    "                test_scores = {name: error_score for name in scorer}\n",
    "                if return_train_score:\n",
    "                    train_scores = test_scores.copy()\n",
    "            else:\n",
    "                test_scores = error_score\n",
    "                if return_train_score:\n",
    "                    train_scores = error_score\n",
    "        result[\"fit_error\"] = format_exc()\n",
    "    else:\n",
    "        result[\"fit_error\"] = None\n",
    "\n",
    "        fit_time = time.time() - start_time\n",
    "        test_scores = _score(estimator, X_test, y_test, scorer, error_score)\n",
    "        print(test_scores)\n",
    "        score_time = time.time() - start_time - fit_time\n",
    "        if return_train_score:\n",
    "            train_scores = _score(estimator, X_train, y_train, scorer, error_score)\n",
    "\n",
    "    if verbose > 1:\n",
    "        total_time = score_time + fit_time\n",
    "        end_msg = f\"[CV{progress_msg}] END \"\n",
    "        result_msg = params_msg + (\";\" if params_msg else \"\")\n",
    "        if verbose > 2:\n",
    "            if isinstance(test_scores, dict):\n",
    "                for scorer_name in sorted(test_scores):\n",
    "                    result_msg += f\" {scorer_name}: (\"\n",
    "                    if return_train_score:\n",
    "                        scorer_scores = train_scores[scorer_name]\n",
    "                        result_msg += f\"train={scorer_scores:.3f}, \"\n",
    "                    result_msg += f\"test={test_scores[scorer_name]:.3f})\"\n",
    "            else:\n",
    "                result_msg += \", score=\"\n",
    "                if return_train_score:\n",
    "                    result_msg += f\"(train={train_scores:.3f}, test={test_scores:.3f})\"\n",
    "                else:\n",
    "                    result_msg += f\"{test_scores:.3f}\"\n",
    "        result_msg += f\" total time={logger.short_format_time(total_time)}\"\n",
    "\n",
    "        # Right align the result_msg\n",
    "        end_msg += \".\" * (80 - len(end_msg) - len(result_msg))\n",
    "        end_msg += result_msg\n",
    "        print(end_msg)\n",
    "\n",
    "    result[\"test_scores\"] = test_scores\n",
    "    if return_train_score:\n",
    "        result[\"train_scores\"] = train_scores\n",
    "    if return_n_test_samples:\n",
    "        result[\"n_test_samples\"] = _num_samples(X_test)\n",
    "    if return_times:\n",
    "        result[\"fit_time\"] = fit_time\n",
    "        result[\"score_time\"] = score_time\n",
    "    if return_parameters:\n",
    "        result[\"parameters\"] = parameters\n",
    "    if return_estimator:\n",
    "        result[\"estimator\"] = estimator\n",
    "    return result\n",
    "\n",
    "def _aggregate_score_dicts(scores):\n",
    "    \"\"\"Aggregate the list of dict to dict of np ndarray\n",
    "\n",
    "    The aggregated output of _aggregate_score_dicts will be a list of dict\n",
    "    of form [{'prec': 0.1, 'acc':1.0}, {'prec': 0.1, 'acc':1.0}, ...]\n",
    "    Convert it to a dict of array {'prec': np.array([0.1 ...]), ...}\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    scores : list of dict\n",
    "        List of dicts of the scores for all scorers. This is a flat list,\n",
    "        assumed originally to be of row major order.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "\n",
    "    >>> scores = [{'a': 1, 'b':10}, {'a': 2, 'b':2}, {'a': 3, 'b':3},\n",
    "    ...           {'a': 10, 'b': 10}]                         # doctest: +SKIP\n",
    "    >>> _aggregate_score_dicts(scores)                        # doctest: +SKIP\n",
    "    {'a': array([1, 2, 3, 10]),\n",
    "     'b': array([10, 2, 3, 10])}\n",
    "    \"\"\"\n",
    "    return {\n",
    "        key: (\n",
    "            np.asarray([score[key] for score in scores])\n",
    "            if isinstance(scores[0][key], numbers.Number)\n",
    "            else [score[key] for score in scores]\n",
    "        )\n",
    "        for key in scores[0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "27e4f2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_scorer(r2_score)\n",
      "[learning_curve] Training set sizes: [ 12  24  36  48  60  72  84  96 108]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .................................................... total time=   3.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .................................................... total time=   3.7s\n",
      "[CV] END .................................................... total time=   3.7s\n",
      "[CV] END .................................................... total time=   3.8s\n",
      "[CV] END .................................................... total time=   3.9s\n",
      "[CV] END .................................................... total time=   3.9s\n",
      "[CV] END .................................................... total time=   3.9s\n",
      "[CV] END .................................................... total time=   3.7s\n",
      "[CV] END .................................................... total time=   3.7s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.7s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.7s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.7s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.5s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.7s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.7s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.4s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.7s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.5s\n",
      "[CV] END .................................................... total time=   3.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:  2.7min finished\n"
     ]
    }
   ],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    estimator=model,\n",
    "    train_sizes = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    X=X,  # Combined features\n",
    "    y=y,\n",
    "    cv = 5,\n",
    "    scoring='r2',\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c14312f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display = LearningCurveDisplay(train_sizes=train_sizes, train_scores=train_scores, test_scores=test_scores, score_name=\"R2\")\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "90ba46e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f5d96d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
