{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99de795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from nilearn.connectome import sym_matrix_to_vec\n",
    "from scipy.stats import pearsonr\n",
    "from cmath import isinf\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import math\n",
    "from cmath import isinf\n",
    "from sklearn.model_selection import train_test_split, KFold, LearningCurveDisplay, learning_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_percentage_error, r2_score\n",
    "import multiprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from skorch import NeuralNet\n",
    "from skorch.callbacks import Callback\n",
    "from skorch.helper import predefined_split\n",
    "from skorch.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d147112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f7d8635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader_to_numpy(loader):\n",
    "    features, targets = [], []\n",
    "    for feat, targ in loader:\n",
    "        features.append(feat.numpy())\n",
    "        targets.append(targ.numpy())\n",
    "    return np.concatenate(features), np.concatenate(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5aa9e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim_feat = 499500, input_dim_target = 1, hidden_dim_feat = 1000, output_dim = 2, dropout_rate = 0):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # Xavier initialization for feature MLP\n",
    "        self.feat_mlp = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_dim_feat),\n",
    "            nn.Linear(input_dim_feat, hidden_dim_feat),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(hidden_dim_feat, output_dim)\n",
    "        )\n",
    "        self.init_weights(self.feat_mlp)\n",
    "\n",
    "        # Xavier initialization for target MLP\n",
    "        self.target_mlp = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_dim_target),\n",
    "            nn.Linear(input_dim_target, output_dim)\n",
    "        )\n",
    "        self.init_weights(self.target_mlp)\n",
    "        \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        features = self.feat_mlp(x)\n",
    "        targets = self.target_mlp(y)\n",
    "        features = nn.functional.normalize(features, p=2, dim=1)\n",
    "        targets = nn.functional.normalize(targets, p=2, dim=1)\n",
    "        return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93673abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgeEstimator(BaseEstimator):\n",
    "    \"\"\" Define the age estimator on latent space network features.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        n_jobs = multiprocessing.cpu_count()\n",
    "        self.age_estimator = GridSearchCV(\n",
    "            Ridge(), param_grid={\"alpha\": 10.**np.arange(-2, 3)}, cv=5,\n",
    "            scoring=\"r2\", n_jobs=n_jobs)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.age_estimator.fit(X, y)\n",
    "        return self.score(X, y), self.r2(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = self.age_estimator.predict(X)\n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.age_estimator.predict(X)\n",
    "        return mean_absolute_percentage_error(y, y_pred)\n",
    "    \n",
    "    def r2(self, X, y):\n",
    "        y_pred = self.age_estimator.predict(X)\n",
    "        return r2_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5caa4aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatData(Dataset):\n",
    "    def __init__(self, path_feat, path_target, target_name, indices, transform = None, regions = None, threshold_mat = False, threshold_percent = None, random_state=42):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with the capability to handle training and testing splits, \n",
    "        including multiple views for augmented data.\n",
    "        \n",
    "        Args:\n",
    "            path_feat (str): Path to the features file.\n",
    "            path_target (str): Path to the target file.\n",
    "            transform (callable): A transformation function to apply for augmentation.\n",
    "            train (bool): Whether the dataset is used for training. False will load the test set.\n",
    "            test_size (float): Proportion of the dataset to include in the test split.\n",
    "            random_state (int): Random state for reproducible train-test splits.\n",
    "        \"\"\"\n",
    "        \n",
    "        features = np.load(path_feat)[indices]\n",
    "        participant_data = pd.read_csv(path_target)\n",
    "        targets = participant_data[target_name].values[indices]\n",
    "        targets = np.expand_dims(targets, axis = 1)\n",
    "        \n",
    "        if threshold_mat:\n",
    "            thresholded_feat = []\n",
    "            for matrix in features:\n",
    "                threshold = np.percentile(matrix, threshold_percent)\n",
    "                matrix[matrix < threshold] = 0\n",
    "                thresholded_feat.append(matrix)\n",
    "            threshold_feat = np.stack(thresholded_feat)\n",
    "            features = threshold_feat\n",
    "        \n",
    "\n",
    "        self.n_sub = len(features)\n",
    "        self.n_views = 1\n",
    "        self.transform = transform\n",
    "        self.targets = targets\n",
    "        \n",
    "        vectorized_feat = np.array([sym_matrix_to_vec(mat, discard_diagonal=True) for mat in features])\n",
    "        self.n_features = vectorized_feat.shape[-1]\n",
    "        \n",
    "        if transform is not None:\n",
    "            # augmentation only in training mode\n",
    "            if transform != \"copy\":\n",
    "                augmented_features = np.array([self.transform(sample, regions = regions) for sample in features])\n",
    "\n",
    "                self.n_views = self.n_views + augmented_features.shape[1]\n",
    "                self.features = np.zeros((self.n_sub, self.n_views, self.n_features))\n",
    "                for sub in range(self.n_sub):\n",
    "                    self.features[sub, 0, :] = vectorized_feat[sub]\n",
    "                    self.features[sub, 1:, :] = augmented_features[sub]\n",
    "            else:\n",
    "                self.features = np.repeat(np.expand_dims(vectorized_feat, axis = 1), 2, axis=1)\n",
    "        else:\n",
    "            self.features = np.expand_dims(vectorized_feat, axis = 1)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.n_sub\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.features[idx]\n",
    "        targets = self.targets[idx]\n",
    "        features = torch.from_numpy(features).float()\n",
    "        targets = torch.from_numpy(targets).float()\n",
    "        \n",
    "        return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7aeeb48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x, krnl_sigma):\n",
    "    x = x - x.T\n",
    "    return torch.exp(-(x**2) / (2*(krnl_sigma**2))) / (math.sqrt(2*torch.pi)*krnl_sigma)\n",
    "\n",
    "def cauchy(x, krnl_sigma):\n",
    "        x = x - x.T\n",
    "        return  1. / (krnl_sigma*(x**2) + 1)\n",
    "\n",
    "def rbf(x, krnl_sigma):\n",
    "        x = x - x.T\n",
    "        return torch.exp(-(x**2)/(2*(krnl_sigma**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f739858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss from: https://github.com/EIDOSLAB/contrastive-brain-age-prediction/blob/master/src/losses.py\n",
    "# modified to accept input shape [bsz, n_feats]. In the age paper: [bsz, n_views, n_feats].\n",
    "class KernelizedSupCon(nn.Module):\n",
    "    \"\"\"Supervised contrastive loss: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\n",
    "    Based on: https://github.com/HobbitLong/SupContrast\"\"\"\n",
    "    def __init__(self, method: str='expw', temperature: float=0.03, contrast_mode: str='all',\n",
    "                 base_temperature: float=0.07, krnl_sigma: float = 1., kernel: callable=cauchy, delta_reduction: str='sum'):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "        self.method = method\n",
    "        self.kernel = kernel\n",
    "        self.krnl_sigma = krnl_sigma\n",
    "        self.delta_reduction = delta_reduction\n",
    "\n",
    "        if kernel is not None and method == 'supcon':\n",
    "            raise ValueError('Kernel must be none if method=supcon')\n",
    "        \n",
    "        if kernel is None and method != 'supcon':\n",
    "            raise ValueError('Kernel must not be none if method != supcon')\n",
    "\n",
    "        if delta_reduction not in ['mean', 'sum']:\n",
    "            raise ValueError(f\"Invalid reduction {delta_reduction}\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__} ' \\\n",
    "               f'(t={self.temperature}, ' \\\n",
    "               f'method={self.method}, ' \\\n",
    "               f'kernel={self.kernel is not None}, ' \\\n",
    "               f'delta_reduction={self.delta_reduction})'\n",
    "\n",
    "    def forward(self, features, labels=None):\n",
    "        \"\"\"Compute loss for model. If `labels` is None, \n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, n_features]. \n",
    "                input has to be rearranged to [bsz, n_views, n_features] and labels [bsz],\n",
    "            labels: ground truth of shape [bsz].\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "        device = features.device\n",
    "\n",
    "        if len(features.shape) != 3:\n",
    "            raise ValueError('`features` needs to be [bsz, n_views, n_feats],'\n",
    "                             '3 dimensions are required')\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        contrast_count = features.shape[1]\n",
    "\n",
    "        if labels is None:\n",
    "            mask = torch.eye(batch_size, device=device)\n",
    "        \n",
    "        else:\n",
    "            labels = labels.view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError('Num of labels does not match num of features')\n",
    "            \n",
    "            if self.kernel is None:\n",
    "                mask = torch.eq(labels, labels.T)\n",
    "            else:\n",
    "                mask = self.kernel(labels, krnl_sigma = self.krnl_sigma)     \n",
    "        \n",
    "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == 'one':\n",
    "            anchor_feature = features[:, 0]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == 'all':\n",
    "            anchor_feature = contrast_feature\n",
    "            anchor_count = contrast_count\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
    "\n",
    "        # Tile mask\n",
    "        mask = mask.repeat(anchor_count, contrast_count)\n",
    "        # Inverse of torch-eye to remove self-contrast (diagonal)\n",
    "        inv_diagonal = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size*anchor_count, device=device).view(-1, 1),\n",
    "            0\n",
    "        )\n",
    "        # compute similarity\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            self.temperature\n",
    "        )\n",
    "\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        alignment = logits \n",
    "\n",
    "        # base case is:\n",
    "        # - supcon if kernel = none \n",
    "        # - y-aware is kernel != none\n",
    "        uniformity = torch.exp(logits) * inv_diagonal \n",
    "\n",
    "        if self.method == 'threshold':\n",
    "            repeated = mask.unsqueeze(-1).repeat(1, 1, mask.shape[0]) # repeat kernel mask\n",
    "\n",
    "            delta = (mask[:, None].T - repeated.T).transpose(1, 2) # compute the difference w_k - w_j for every k,j\n",
    "            delta = (delta > 0.).float()\n",
    "\n",
    "            # for each z_i, repel only samples j s.t. K(z_i, z_j) < K(z_i, z_k)\n",
    "            uniformity = uniformity.unsqueeze(-1).repeat(1, 1, mask.shape[0])\n",
    "\n",
    "            if self.delta_reduction == 'mean':\n",
    "                uniformity = (uniformity * delta).mean(-1)\n",
    "            else:\n",
    "                uniformity = (uniformity * delta).sum(-1)\n",
    "    \n",
    "        elif self.method == 'expw':\n",
    "            # exp weight e^(s_j(1-w_j))\n",
    "            uniformity = torch.exp(logits * (1 - mask)) * inv_diagonal\n",
    "\n",
    "        uniformity = torch.log(uniformity.sum(1, keepdim=True))\n",
    "\n",
    "\n",
    "        # positive mask contains the anchor-positive pairs\n",
    "        # excluding <self,self> on the diagonal\n",
    "        positive_mask = mask * inv_diagonal\n",
    "\n",
    "        log_prob = alignment - uniformity # log(alignment/uniformity) = log(alignment) - log(uniformity)\n",
    "        log_prob = (positive_mask * log_prob).sum(1) / positive_mask.sum(1) # compute mean of log-likelihood over positive\n",
    " \n",
    "        # loss\n",
    "        loss = - (self.temperature / self.base_temperature) * log_prob\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75fea03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "train_dataset = MatData(\"matrices.npy\", \"participants.csv\", \"age\", train=True, train_size = 100, test_size = 50)\n",
    "test_dataset = MatData(\"matrices.npy\", \"participants.csv\", \"age\", train=False, train_size = 100, test_size = 50)\n",
    "dataset = ConcatDataset([train_dataset, test_dataset])\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle = False)\n",
    "X, y = loader_to_numpy(dataloader)\n",
    "X = X.squeeze(1)\n",
    "\n",
    "X_train, y_train = X[:100], y[:100]\n",
    "X_test, y_test = X[100:], y[100:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "deb2eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "144228a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "973e6c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = pd.read_csv(\"participants.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4683743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion_pft, criterion_ptt, optimizer):\n",
    "    \n",
    "    model.train()\n",
    "    for batch_num, (features, targets) in enumerate(train_loader):\n",
    "        bsz = targets.shape[0]\n",
    "        n_views = features.shape[1]\n",
    "        n_feat = features.shape[-1]\n",
    "        \n",
    "        features = features.view(bsz * n_views, n_feat) # [bsz*2, 499500]\n",
    "        features, targets = features.to(device), targets.to(device) # [bsz, 2, 499500], [bsz, 1]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out_feat, out_target = model(features, torch.cat(n_views*[targets], dim=0)) # ([bsz*5, 1], [bsz*5, 1])\n",
    "        \n",
    "        out_feat = torch.split(out_feat, [bsz]*n_views, dim=0)\n",
    "        out_feat = torch.cat([f.unsqueeze(1) for f in out_feat], dim=1) # [bsz, 5, 2]\n",
    "        \n",
    "        loss = criterion_pft(out_feat, targets) # ([bsz, 5, 2], [bsz, 1])\n",
    "        \n",
    "        out_target = torch.split(out_target, [bsz]*n_views, dim=0)\n",
    "        out_target = torch.cat([f.unsqueeze(1) for f in out_target], dim=1) # [bsz, 2, 2]\n",
    "        loss += criterion_ptt(out_target, targets) # ([bsz, 5, 2], [bsz, 1])\n",
    "        loss += torch.nn.functional.mse_loss(out_feat.view(bsz * n_views, 2), out_target.view(bsz * n_views, 2)) # mse_loss([bsz*2, 2], [bsz*2, 2])\n",
    "    \n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        batch_losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "        \n",
    "        return batch_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "01c87171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_loader, criterion_pft, criterion_ptt, optimizer):\n",
    "    \n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    emb_features = [] # saving the embedded features for each batch\n",
    "    emb_targets = []\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "        for batch_num, (features, targets) in enumerate(v):\n",
    "            bsz = targets.shape[0]\n",
    "            n_views = 1\n",
    "            n_feat = features.shape[-1]\n",
    "\n",
    "            if len(features.shape) > 2:\n",
    "                n_views = features.shape[1]\n",
    "                features = features.view(bsz * n_views, n_feat) # [bsz*2, 499500]\n",
    "            features, targets = features.to(device), targets.to(device) # [bsz, 2, 499500], [bsz, 1]\n",
    "\n",
    "            out_feat, out_target = model(features, torch.cat(n_views*[targets], dim=0))   \n",
    "\n",
    "            out_feat = torch.split(out_feat, [bsz]*n_views, dim=0)\n",
    "            out_feat = torch.cat([f.unsqueeze(1) for f in out_feat], dim=1) # [bsz, 5, 2]\n",
    "\n",
    "            loss = criterion_pft(out_feat, targets) # ([bsz, 5, 2], [bsz, 1])\n",
    "\n",
    "            out_target = torch.split(out_target, [bsz]*n_views, dim=0)\n",
    "            out_target = torch.cat([f.unsqueeze(1) for f in out_target], dim=1) # [bsz, 2, 2]\n",
    "\n",
    "            loss += criterion_ptt(out_target, targets) # ([bsz, 5, 2], [bsz, 1])\n",
    "            loss += torch.nn.functional.mse_loss(out_feat.view(bsz * n_views, 2), out_target.view(bsz * n_views, 2)) # mse_loss([bsz*2, 2], [bsz*2, 2])\n",
    "\n",
    "            emb_features.append(out_feat[:, 0, :])\n",
    "            emb_targets.append(out_target[:, 0, :])\n",
    "\n",
    "            test_losses.append(loss.item())\n",
    "            total_loss += loss.item() * features.size(0)\n",
    "            total_samples += features.size(0)\n",
    "\n",
    "        test_losses =np.array(test_losses)\n",
    "        average_loss = total_loss / total_samples\n",
    "        print('Mean Test Loss: %6.2f' % (average_loss))\n",
    "        \n",
    "        return torch.row_stack(emb_features).cpu(), torch.row_stack(emb_features).cpu()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7f782af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "842\n",
      "94\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m test_indices \u001b[38;5;241m=\u001b[39m folds[i]\n\u001b[1;32m     26\u001b[0m train_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([folds[j] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k) \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;241m!=\u001b[39m i])\n\u001b[0;32m---> 28\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMatData\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmatrices.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparticipants.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m MatData(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatrices.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparticipants.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m, indices \u001b[38;5;241m=\u001b[39m test_indices)\n",
      "Cell \u001b[0;32mIn[52], line 36\u001b[0m, in \u001b[0;36mMatData.__init__\u001b[0;34m(self, path_feat, path_target, target_name, indices, transform, regions, threshold_mat, threshold_percent, random_state)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets \u001b[38;5;241m=\u001b[39m targets\n\u001b[0;32m---> 36\u001b[0m vectorized_feat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([sym_matrix_to_vec(mat, discard_diagonal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m mat \u001b[38;5;129;01min\u001b[39;00m features])\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features \u001b[38;5;241m=\u001b[39m vectorized_feat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# augmentation only in training mode\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[52], line 36\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets \u001b[38;5;241m=\u001b[39m targets\n\u001b[0;32m---> 36\u001b[0m vectorized_feat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43msym_matrix_to_vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscard_diagonal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m mat \u001b[38;5;129;01min\u001b[39;00m features])\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features \u001b[38;5;241m=\u001b[39m vectorized_feat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# augmentation only in training mode\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/nilearn/connectome/connectivity_matrices.py:239\u001b[0m, in \u001b[0;36msym_matrix_to_vec\u001b[0;34m(symmetric, discard_diagonal)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the flattened lower triangular part of an array.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03mIf diagonal is kept, diagonal elements are divided by sqrt(2) to conserve\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    235\u001b[0m \n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m discard_diagonal:\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# No scaling, we directly return the values\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m     tril_mask \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtril\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43msymmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m symmetric[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, tril_mask]\n\u001b[1;32m    241\u001b[0m scaling \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(symmetric\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mtril\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/numpy/lib/twodim_base.py:494\u001b[0m, in \u001b[0;36mtril\u001b[0;34m(m, k)\u001b[0m\n\u001b[1;32m    491\u001b[0m m \u001b[38;5;241m=\u001b[39m asanyarray(m)\n\u001b[1;32m    492\u001b[0m mask \u001b[38;5;241m=\u001b[39m tri(\u001b[38;5;241m*\u001b[39mm\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:], k\u001b[38;5;241m=\u001b[39mk, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m--> 494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k = 2\n",
    "# val_indices = participants.index[participants['dataset'] == \"COBRE\"].values\n",
    "n_subj = len(participants)\n",
    "batch_size = 30\n",
    "\n",
    "input_dim_feat = 499500 # vectorized mat, diagonal discarded\n",
    "# the rest is arbitrary\n",
    "hidden_dim_feat = 1000\n",
    "input_dim_target = 1\n",
    "output_dim = 2\n",
    "\n",
    "lr = 0.01\n",
    "dropout_rate = 0\n",
    "weight_decay = 0\n",
    "kernel = cauchy\n",
    "epochs = 100\n",
    "\n",
    "criterion_pft = KernelizedSupCon(method='expw', temperature=0.03, base_temperature=0.07, kernel=kernel, krnl_sigma = 1, contrast_mode = 'all')\n",
    "criterion_ptt = KernelizedSupCon(method='expw', temperature=0.03, base_temperature=0.07, kernel=kernel, krnl_sigma = 1, contrast_mode = 'all')\n",
    "\n",
    "for train_size in train_sizes:\n",
    "    print(train_size)\n",
    "    all_indices = np.arange(n_subj)\n",
    "    val_size = 1 - train_size\n",
    "    np.random.shuffle(all_indices)\n",
    "    \n",
    "    val_size = int(n_subj * val_size)\n",
    "    print(val_size)\n",
    "    \n",
    "    val_indices = np.random.choice(all_indices, val_size, replace = False)\n",
    "    cv_indices = all_indices[~np.isin(all_indices, val_indices)]\n",
    "    print(len(cv_indices))\n",
    "    \n",
    "    validation_dataset = MatData(\"matrices.npy\", \"participants.csv\", \"age\", indices = val_indices)\n",
    "    val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    folds = np.array_split(cv_indices, k)\n",
    "    \n",
    "    for i in range(k):\n",
    "        test_indices = folds[i]\n",
    "        train_indices = np.concatenate([folds[j] for j in range(k) if j != i])\n",
    "        \n",
    "        train_dataset = MatData(\"matrices.npy\", \"participants.csv\", \"age\", indices = train_indices)\n",
    "        test_dataset = MatData(\"matrices.npy\", \"participants.csv\", \"age\", indices = test_indices)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False) # already shuffled\n",
    "        val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        model = MLP(input_dim_feat, input_dim_target, hidden_dim_feat, output_dim, dropout_rate).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "        \n",
    "        for epoch in epochs:\n",
    "            batch_losses = train(model, train_loader, criterion_pft, criterion_ptt, optimizer)\n",
    "            print(f'Epoch {epoch} | Mean Loss {sum(batch_losses)/len(batch_losses)}')\n",
    "        \n",
    "        X_train, y_train = gather_feats_targets(model, train_loader, device)\n",
    "        estimator = AgeEstimator().fit(X_train, y_train)\n",
    "        mae_train = age_estimator.score(X_train, y_train)\n",
    "        r2_train = age_estimator.r2(X_train, y_train)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "27e4f2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_scorer(r2_score)\n",
      "[learning_curve] Training set sizes: [ 12  24  36  48  60  72  84  96 108]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .................................................... total time=   3.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .................................................... total time=   3.7s\n",
      "[CV] END .................................................... total time=   3.7s\n",
      "[CV] END .................................................... total time=   3.8s\n",
      "[CV] END .................................................... total time=   3.9s\n",
      "[CV] END .................................................... total time=   3.9s\n",
      "[CV] END .................................................... total time=   3.9s\n",
      "[CV] END .................................................... total time=   3.7s\n",
      "[CV] END .................................................... total time=   3.7s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.7s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.7s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.7s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.5s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.7s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.7s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.4s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.7s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.6s\n",
      "[CV] END .................................................... total time=   3.5s\n",
      "[CV] END .................................................... total time=   3.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:  2.7min finished\n"
     ]
    }
   ],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    estimator=model,\n",
    "    train_sizes = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    X=X,  # Combined features\n",
    "    y=y,\n",
    "    cv = 5,\n",
    "    scoring='r2',\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c14312f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display = LearningCurveDisplay(train_sizes=train_sizes, train_scores=train_scores, test_scores=test_scores, score_name=\"R2\")\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "90ba46e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f5d96d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
