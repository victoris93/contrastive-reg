{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs3/well/margulies/users/cpy397/python/neuro/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import yaml\n",
    "import wandb\n",
    "import xarray as xr\n",
    "import asyncio\n",
    "import submitit\n",
    "import pickle\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "from nilearn.connectome import sym_matrix_to_vec, vec_to_sym_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    ")\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, TensorDataset\n",
    "from tqdm.auto import tqdm\n",
    "# from augmentations import augs, aug_args\n",
    "import glob, os, shutil\n",
    "from nilearn.datasets import fetch_atlas_schaefer_2018\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from ContModeling.utils import (\n",
    "    gaussian_kernel,\n",
    "    cauchy,\n",
    "    rbf,\n",
    "    standardize,\n",
    "    save_embeddings,\n",
    "    filter_nans\n",
    ")\n",
    "from ContModeling.losses import LogEuclideanLoss, NormLoss, KernelizedSupCon, OutlierRobustMSE\n",
    "from ContModeling.models import PhenoProj\n",
    "from ContModeling.helper_classes import MatData\n",
    "from ContModeling.viz_func import wandb_plot_acc_vs_baseline, wandb_plot_test_recon_corr, wandb_plot_individual_recon\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "multi_gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_TASKS_DICT = {\n",
    "    \"PicVocab_AgeAdj\": \"Vocabulary (picture matching)\",\n",
    "    \"ReadEng_AgeAdj\": \"Reading (pronunciation)\",\n",
    "    \"WM_Task_2bk_Acc\": \"Working Memory (2-back)\",\n",
    "    \"Relational_Task_Acc\": \"Relational processing\",\n",
    "    \"Language_Task_Story_Acc\": \"Story comprehension\",\n",
    "    \"PMAT24_A_CR\": \"Fluid Intelligence (PMAT)\",\n",
    "    \"VSPLOT_TC\": \"Spatial orientation\",  #???\n",
    "    \"CardSort_AgeAdj\": \"Cognitive flexibility (DCCS)\",\n",
    "    \"ListSort_AgeAdj\": \"Working memory (list sorting)\",\n",
    "    \"Language_Task_Math_Acc\": \"Arithmetic\", # ???\n",
    "    \"ProcSpeed_AgeAdj\": \"Processing speed\",\n",
    "    \"PicSeq_AgeAdj\": \"Visual Episodic Memory\",\n",
    "    \"SCPT_SPEC\": \"Sustained attention (Spec.)\",\n",
    "    \"Flanker_AgeAdj\": \"Inhibition (Flanker Task)\",\n",
    "    \"Emotion_Task_Face_Acc\": \"Emotional Face Matching\",\n",
    "    \"IWRD_TOT\": \"Verbal Episodic Memory\",\n",
    "    \"Dexterity_AgeAdj\": \"Manual dexterity\",\n",
    "    \"ER40_CR\": \"Emotion recognition - Total\",\n",
    "    \"ER40SAD\": \"Emotion recognition - Sad\",\n",
    "    \"ER40ANG\": \"Emotion recognition - Angry\",\n",
    "    ### SOCIAL COGNITION - INTERACTION ??????\n",
    "    \"ER40NOE\": \"Emotion recognition - Neutral\",\n",
    "    \"MMSE_Score\": \"Cognitive status (MMSE)\",\n",
    "    \"ER40HAP\": \"Emotion recognition - Happy\",\n",
    "    \"SCPT_SEN\": \"Sustained attention (Sens.)\",\n",
    "    \"Social_Task_Perc_Random\": \"Social cognition (Random)\",\n",
    "    \"ER40FEAR\": \"Emotion recognition - Fear\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_LOSSES ={\n",
    "    'Norm': NormLoss(),\n",
    "    'LogEuclidean': LogEuclideanLoss(),\n",
    "    'MSE': nn.functional.mse_loss,\n",
    "    'MAE': nn.functional.l1_loss,\n",
    "    'MSERobust': OutlierRobustMSE(),\n",
    "    'Huber': nn.HuberLoss(),\n",
    "    'cosine': nn.functional.cosine_embedding_loss,\n",
    "}\n",
    "\n",
    "SUPCON_KERNELS = {\n",
    "    'cauchy': cauchy,\n",
    "    'gaussian_kernel': gaussian_kernel,\n",
    "    'rbf': rbf,\n",
    "    'None': None\n",
    "    }\n",
    "\n",
    "def train(run, train_ratio, train_dataset, test_dataset, mean, std, B_init_fMRI, cfg, model=None, device=device):\n",
    "    print(\"Start training...\")\n",
    "\n",
    "    # MODEL DIMS\n",
    "    input_dim_feat = cfg.input_dim_feat\n",
    "    input_dim_target = cfg.input_dim_target\n",
    "    hidden_dim = cfg.hidden_dim\n",
    "    output_dim_target = cfg.output_dim_target\n",
    "    output_dim_feat = cfg.output_dim_feat\n",
    "    kernel = SUPCON_KERNELS[cfg.SupCon_kernel]\n",
    "    \n",
    "    # TRAINING PARAMS\n",
    "    lr = cfg.lr\n",
    "    batch_size = cfg.batch_size\n",
    "    dropout_rate = cfg.dropout_rate\n",
    "    weight_decay = cfg.weight_decay\n",
    "    num_epochs = cfg.num_epochs\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    mean= torch.tensor(mean).to(device)\n",
    "    std = torch.tensor(std).to(device)\n",
    "    if model is None:\n",
    "        model = PhenoProj(\n",
    "            input_dim_feat,\n",
    "            input_dim_target,\n",
    "            hidden_dim,\n",
    "            output_dim_target,\n",
    "            output_dim_feat,\n",
    "            dropout_rate,\n",
    "            cfg\n",
    "        ).to(device)\n",
    "\n",
    "    if cfg.full_model_pretrained:\n",
    "        print(f\"Loading pretrained FULL model, train ratio {train_ratio}...\")\n",
    "        state_dict = torch.load(f\"{cfg.output_dir}/{cfg.pretrained_full_model_exp}/saved_models/model_weights_train_ratio{train_ratio}_run0.pth\")\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    else:\n",
    "        if cfg.mat_ae_pretrained:\n",
    "            print(\"Loading pretrained MatrixAutoencoder...\")\n",
    "            state_dict = torch.load(f\"{cfg.output_dir}/{cfg.pretrained_mat_ae_exp}/saved_models/autoencoder_weights_fold{cfg.best_mat_ae_fold}.pth\")\n",
    "            model.matrix_ae.load_state_dict(state_dict)\n",
    "\n",
    "        if cfg.reduced_mat_ae_pretrained:\n",
    "            print(\"Loading pretrained ReducedMatrixAutoencoder...\")\n",
    "            state_dict = torch.load(f\"{cfg.output_dir}/{cfg.pretrained_reduced_mat_ae_exp}/saved_models/autoencoder_weights_fold{cfg.best_reduced_mat_ae_fold}.pth\")\n",
    "            model.reduced_matrix_ae.load_state_dict(state_dict)\n",
    "        \n",
    "    if cfg.mat_ae_enc_freeze:\n",
    "        print(\"Freezing weights for mat encoding...\")\n",
    "        for param in model.matrix_ae.enc_mat1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.matrix_ae.enc_mat2.parameters():\n",
    "            param.requires_grad = False\n",
    "    else:\n",
    "        model.matrix_ae.enc_mat1.weight = torch.nn.Parameter(B_init_fMRI.transpose(0,1))\n",
    "        model.matrix_ae.enc_mat2.weight = torch.nn.Parameter(B_init_fMRI.transpose(0,1))\n",
    "\n",
    "    if cfg.mat_ae_dec_freeze:\n",
    "        print(\"Freezing weights for mat decoding...\")\n",
    "        for param in model.matrix_ae.dec_mat1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.matrix_ae.dec_mat2.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    if cfg.reduced_mat_ae_enc_freeze:\n",
    "        print(\"Freezing weights for reduced mat encoding...\")\n",
    "        for param in model.reduced_matrix_ae.reduced_mat_to_embed.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    if cfg.reduced_mat_ae_dec_freeze:\n",
    "        print(\"Freezing weights for reduced mat decoding...\")\n",
    "        for param in model.reduced_matrix_ae.embed_to_reduced_mat.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    if cfg.target_dec_freeze:\n",
    "        print(\"Freezing TargetDecoder...\")\n",
    "        for param in model.target_dec.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    criterion_pft = KernelizedSupCon(\n",
    "        method=\"yaware\",\n",
    "        temperature=cfg.pft_temperature,\n",
    "        base_temperature= cfg.pft_base_temperature,\n",
    "        reg_term = cfg.pft_reg_term,\n",
    "        kernel=kernel,\n",
    "        krnl_sigma=cfg.pft_sigma,\n",
    "    )\n",
    "\n",
    "    criterion_ptt = KernelizedSupCon(\n",
    "        method=\"yaware\",\n",
    "        temperature=cfg.ptt_temperature,\n",
    "        base_temperature= cfg.ptt_base_temperature,\n",
    "        reg_term = cfg.ptt_reg_term,\n",
    "        kernel=kernel,\n",
    "        krnl_sigma=cfg.ptt_sigma,\n",
    "    )\n",
    "    feature_autoencoder_crit = EMB_LOSSES[cfg.feature_autoencoder_crit]\n",
    "    target_decoding_crit = EMB_LOSSES[cfg.target_decoding_crit]\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=cfg.lr, weight_decay=weight_decay,\n",
    "    #                             momentum=0.9)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.1, patience = cfg.scheduler_patience)\n",
    "\n",
    "    loss_terms = []\n",
    "    validation = []\n",
    "    autoencoder_features = []\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    wandb.init(project=cfg.project,\n",
    "        mode = \"offline\",\n",
    "        name=f\"{cfg.experiment_name}_run{run}_train_ratio_{train_ratio}\",\n",
    "        dir = cfg.output_dir,\n",
    "        config = OmegaConf.to_container(cfg, resolve=True))\n",
    "    \n",
    "    with tqdm(range(num_epochs), desc=\"Epochs\", leave=False) as pbar:\n",
    "        for epoch in pbar:\n",
    "            model.train()\n",
    "\n",
    "            if cfg.reduced_mat_ae_pretrained:\n",
    "                model.reduced_matrix_ae.eval()\n",
    "            if cfg.reduced_mat_ae_enc_freeze:\n",
    "                model.reduced_matrix_ae.reduced_mat_to_embed.eval()\n",
    "            if cfg.reduced_mat_ae_dec_freeze:\n",
    "                model.reduced_matrix_ae.embed_to_reduced_mat.eval()\n",
    "            if cfg.target_dec_freeze:\n",
    "                model.target_dec.eval()\n",
    "                \n",
    "            loss_terms_batch = defaultdict(lambda:0)\n",
    "            for features, targets, inter_network_conn, _ in train_loader:\n",
    "                loss = 0\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                features, targets, _, inter_network_conn = filter_nans(features, targets, _z=inter_network_conn)\n",
    "\n",
    "                features = features.to(device)\n",
    "                targets = targets.to(device)\n",
    "                inter_network_conn = inter_network_conn.to(device)\n",
    "\n",
    "                ## FEATURE ENCODING == MATRIX REDUCTION\n",
    "                embedded_feat = model.encode_features(features)\n",
    "                \n",
    "                ## VECTORIZE REDUCED MATRIX\n",
    "                embedded_feat_vectorized = sym_matrix_to_vec(embedded_feat.detach().cpu().numpy())\n",
    "                embedded_feat_vectorized = torch.tensor(embedded_feat_vectorized).to(torch.float32).to(device)\n",
    "\n",
    "                ## EMBEDDING OF THE REDUCED MATRIX\n",
    "                reduced_mat_embedding, reduced_mat_embedding_norm = model.encode_reduced_mat(embedded_feat_vectorized)\n",
    "                out_target_decoded = model.decode_targets(reduced_mat_embedding_norm)\n",
    "\n",
    "                ## RECONSTRUCT REDUCED MATRIX FROM EMBEDDING AND THE FULL MATRIX FROM REDUCED\n",
    "                recon_reduced_mat = model.decode_reduced_mat(reduced_mat_embedding)\n",
    "\n",
    "                if not cfg.reduced_mat_ae_dec_freeze:\n",
    "                    reduced_mat_recon_loss = feature_autoencoder_crit(embedded_feat_vectorized, recon_reduced_mat) / 1000\n",
    "                    loss += reduced_mat_recon_loss\n",
    "\n",
    "                recon_reduced_mat = vec_to_sym_matrix(recon_reduced_mat.detach().cpu().numpy())\n",
    "                recon_reduced_mat = torch.tensor(recon_reduced_mat).to(torch.float32).to(device)\n",
    "\n",
    "                reconstructed_feat = model.decode_features(recon_reduced_mat)\n",
    "\n",
    "                ## LOSS: TARGET DECODING FROM TARGET EMBEDDING\n",
    "                if cfg.target_decoding_crit == 'Huber' and cfg.huber_delta != 'None':\n",
    "                    target_decoding_crit = nn.HuberLoss(delta = cfg.huber_delta)\n",
    "\n",
    "                if not cfg.reduced_mat_ae_enc_freeze:\n",
    "                    ## KERNLIZED LOSS: MAT embedding vs targets\n",
    "                    kernel_embedded_target_loss, _ = criterion_ptt(reduced_mat_embedding_norm.unsqueeze(1), targets)\n",
    "                    kernel_embedded_network_loss, _ = criterion_pft(reduced_mat_embedding_norm.unsqueeze(1), inter_network_conn)\n",
    "                    loss += (kernel_embedded_target_loss + kernel_embedded_network_loss)\n",
    "\n",
    "                if not cfg.mat_ae_enc_freeze or not cfg.mat_ae_dec_freeze:\n",
    "                    feature_autoencoder_loss = feature_autoencoder_crit(features, reconstructed_feat) / 1000\n",
    "                    loss += feature_autoencoder_loss\n",
    "\n",
    "                if not cfg.target_dec_freeze:\n",
    "                    target_decoding_from_reduced_emb_loss = target_decoding_crit(targets, out_target_decoded)\n",
    "                    loss += target_decoding_from_reduced_emb_loss\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                if cfg.clip_grad:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    \n",
    "                if cfg.log_gradients:\n",
    "                    for name, param in model.named_parameters():\n",
    "                        if param.grad is not None:\n",
    "                            wandb.log({\n",
    "                                \"Epoch\": epoch,\n",
    "                                'Run': run,\n",
    "                                \"Train ratio\": train_ratio,\n",
    "                                f\"Gradient Norm/{name}\": param.grad.norm().item()\n",
    "                            })  \n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                loss_terms_batch['loss'] = loss.item() / len(features)\n",
    "\n",
    "                if not cfg.reduced_mat_ae_enc_freeze:\n",
    "                    loss_terms_batch['kernel_embedded_target_loss'] = kernel_embedded_target_loss.item() / len(features)\n",
    "                    loss_terms_batch['kernel_embedded_network_loss'] = kernel_embedded_network_loss.item() / len(features)\n",
    "\n",
    "                    wandb.log({\n",
    "                        'Epoch': epoch,\n",
    "                        'Run': run,\n",
    "                        \"Train ratio\": train_ratio,\n",
    "                        'kernel_embedded_target_loss': loss_terms_batch['kernel_embedded_target_loss'],\n",
    "                        'kernel_embedded_network_loss': loss_terms_batch['kernel_embedded_network_loss'],\n",
    "                    })\n",
    "\n",
    "                if not cfg.reduced_mat_ae_dec_freeze:\n",
    "                    loss_terms_batch['reduced_mat_recon_loss'] = reduced_mat_recon_loss.item() / len(features)\n",
    "                    wandb.log({\n",
    "                        'Epoch': epoch,\n",
    "                        'Run': run,\n",
    "                        \"Train ratio\": train_ratio,\n",
    "                        'reduced_mat_recon_loss': loss_terms_batch['reduced_mat_recon_loss'],\n",
    "                    })\n",
    "                \n",
    "                if not cfg.target_dec_freeze:\n",
    "                    loss_terms_batch['target_decoding_loss'] = target_decoding_from_reduced_emb_loss.item() / len(features)\n",
    "                    wandb.log({\n",
    "                        'Epoch': epoch,\n",
    "                        'Run': run,\n",
    "                        \"Train ratio\": train_ratio,\n",
    "                        'target_decoding_loss': loss_terms_batch['target_decoding_loss'],\n",
    "                    })\n",
    "\n",
    "                # loss_terms_batch['direction_reg_target_loss'] = direction_reg_target.item() / len(features)\n",
    "                \n",
    "                if not cfg.mat_ae_enc_freeze or not cfg.mat_ae_dec_freeze:\n",
    "                    loss_terms_batch['feature_autoencoder_loss'] = feature_autoencoder_loss.item() / len(features)\n",
    "                    wandb.log({\n",
    "                        'Epoch': epoch,\n",
    "                        'Run': run,\n",
    "                        \"Train ratio\": train_ratio,\n",
    "                        'feature_autoencoder_loss': loss_terms_batch['feature_autoencoder_loss'],\n",
    "                    })\n",
    "                \n",
    "                wandb.log({\n",
    "                    'Epoch': epoch,\n",
    "                    'Run': run,\n",
    "                    \"Train ratio\": train_ratio,\n",
    "                    'total_loss': loss_terms_batch['loss'],\n",
    "                })\n",
    "\n",
    "            loss_terms_batch['epoch'] = epoch\n",
    "            loss_terms.append(loss_terms_batch)\n",
    "\n",
    "            model.eval()\n",
    "            mape_batch = 0\n",
    "            corr_batch = 0\n",
    "            with torch.no_grad():\n",
    "                for features, targets, _, _ in test_loader:\n",
    "\n",
    "                    features, targets, _, _ = filter_nans(features, targets)\n",
    "                    \n",
    "                    features, targets = features.to(device), targets.to(device)                    \n",
    "                    reduced_mat = model.encode_features(features)\n",
    "                    \n",
    "                    reduced_mat = torch.tensor(sym_matrix_to_vec(reduced_mat.detach().cpu().numpy())).to(torch.float32).to(device)\n",
    "                    embedding, embedding_norm = model.encode_reduced_mat(reduced_mat)\n",
    "                    out_target_decoded = model.decode_targets(embedding_norm)\n",
    "                    \n",
    "                    epsilon = 1e-8\n",
    "\n",
    "                    mape =  torch.mean(torch.abs((targets - out_target_decoded)) / torch.abs((targets + epsilon))) * 100\n",
    "                    if torch.isnan(mape):\n",
    "                        mape = torch.tensor(0.0)\n",
    "                    \n",
    "                    corr =  spearmanr(targets.cpu().numpy().flatten(), out_target_decoded.cpu().numpy().flatten())[0]\n",
    "                    if np.isnan(corr):\n",
    "                        corr = 0.0\n",
    "                        \n",
    "                    mape_batch += mape.item()\n",
    "                    corr_batch += corr\n",
    "\n",
    "                mape_batch = mape_batch/len(test_loader)\n",
    "                corr_batch = corr_batch/len(test_loader)\n",
    "                validation.append(mape_batch)\n",
    "\n",
    "            wandb.log({\n",
    "                'Run': run,\n",
    "                \"Train ratio\": train_ratio,\n",
    "                'Target MAPE/val' : mape_batch,\n",
    "                'Target Corr/val': corr_batch,\n",
    "                })\n",
    "            \n",
    "            scheduler.step(mape_batch)\n",
    "            if np.log10(scheduler._last_lr[0]) < -7:\n",
    "                break\n",
    "\n",
    "            pbar.set_postfix_str(\n",
    "                f\"Epoch {epoch} \"\n",
    "                f\"| Loss {loss_terms[-1]['loss']:.02f} \"\n",
    "                f\"| val Target MAPE {mape_batch:.02f}\"\n",
    "                f\"| val Target Corr {corr_batch:.02f} \"\n",
    "                f\"| log10 lr {np.log10(scheduler._last_lr[0])}\"\n",
    "            )\n",
    "    wandb.finish()\n",
    "    loss_terms = pd.DataFrame(loss_terms)\n",
    "    return loss_terms, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRun(submitit.helpers.Checkpointable):\n",
    "    def __init__(self):\n",
    "        self.results = None\n",
    "        self.embeddings = None\n",
    "\n",
    "    def __call__(self, train, test_size, indices, train_ratio, run_size, run, dataset, cfg, random_state=None, device=None, save_model = True, path: Path = None):\n",
    "        if self.results is None:\n",
    "            if device is None:\n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                print(f\"Device {device}, ratio {train_ratio}\", flush=True)\n",
    "            if not isinstance(random_state, np.random.RandomState):\n",
    "                random_state = np.random.RandomState(random_state)\n",
    "            \n",
    "            recon_mat_dir = os.path.join(cfg.output_dir, cfg.experiment_name, cfg.reconstructed_dir)\n",
    "            os.makedirs(recon_mat_dir, exist_ok=True)\n",
    "    \n",
    "            predictions = {}\n",
    "            autoencoder_features = {}\n",
    "            losses = []\n",
    "            self.embeddings = {'train': [], 'test': []}\n",
    "            self.run = run\n",
    "\n",
    "            if cfg.mat_ae_pretrained:\n",
    "                print(\"Loading test indices from the pretraining experiment...\")\n",
    "                test_indices = np.load(f\"{cfg.output_dir}/{cfg.pretrained_mat_ae_exp}/test_idx.npy\")\n",
    "                train_indices = np.setdiff1d(indices, test_indices)\n",
    "                if train_ratio < 1.0:\n",
    "                    train_size = int(len(train_indices) * train_ratio)\n",
    "                    train_indices = random_state.choice(train_indices, train_size, replace=False)\n",
    "\n",
    "            elif cfg.external_test_mode:\n",
    "                test_scanners = list(cfg.test_scanners)\n",
    "                xr_dataset = xr.open_dataset(cfg.dataset_path)\n",
    "                scanner_mask = np.sum([xr_dataset.isin(scanner).scanner.values for scanner in test_scanners],\n",
    "                                    axis = 0).astype(bool)\n",
    "                test_indices = indices[scanner_mask]\n",
    "                train_indices = indices[~scanner_mask]\n",
    "                if train_ratio < 1.0:\n",
    "                    train_size = int(len(train_indices) * train_ratio)\n",
    "                    train_indices = random_state.choice(train_indices, train_size, replace=False)\n",
    "                del xr_dataset\n",
    "\n",
    "            else:\n",
    "                run_indices = random_state.choice(indices, run_size, replace=False)\n",
    "                train_indices, test_indices = train_test_split(run_indices, test_size=test_size, random_state=random_state)\n",
    "                \n",
    "            train_dataset = Subset(dataset, train_indices)\n",
    "            test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "            train_features = train_dataset.dataset.matrices[train_dataset.indices]\n",
    "            train_targets = train_dataset.dataset.targets[train_dataset.indices].numpy()\n",
    "            train_inter_network_conn = train_dataset.dataset.inter_network_conn[train_dataset.indices]\n",
    "            train_intra_network_conn = train_dataset.dataset.intra_network_conn[train_dataset.indices]\n",
    "\n",
    "            std_train_targets, mean, std = standardize(train_targets)\n",
    "            # scaler = MinMaxScaler().fit(train_targets)\n",
    "            # train_targets = scaler.transform(train_targets)\n",
    "\n",
    "            input_dim_feat =cfg.input_dim_feat\n",
    "            output_dim_feat = cfg.output_dim_feat\n",
    "\n",
    "            ## Weight initialization for bilinear layer\n",
    "            mean_f = torch.mean(train_features, dim=0).to(device)\n",
    "            [D,V] = torch.linalg.eigh(mean_f,UPLO = \"U\")\n",
    "            B_init_fMRI = V[:,input_dim_feat-output_dim_feat:]\n",
    "\n",
    "            test_features= test_dataset.dataset.matrices[test_dataset.indices].numpy()\n",
    "            test_targets = test_dataset.dataset.targets[test_dataset.indices].numpy()\n",
    "            test_inter_network_conn = test_dataset.dataset.inter_network_conn[test_dataset.indices]\n",
    "            test_intra_network_conn = test_dataset.dataset.intra_network_conn[test_dataset.indices]\n",
    "\n",
    "            train_dataset = TensorDataset(train_features,\n",
    "                                          torch.from_numpy(train_targets).to(torch.float32),\n",
    "                                          train_inter_network_conn,\n",
    "                                          train_intra_network_conn)\n",
    "            \n",
    "            test_dataset = TensorDataset(torch.from_numpy(test_features).to(torch.float32),\n",
    "                                         torch.from_numpy(test_targets).to(torch.float32),\n",
    "                                         test_inter_network_conn,\n",
    "                                         test_intra_network_conn)\n",
    "\n",
    "            loss_terms, model = train(run, train_ratio, train_dataset, test_dataset, mean, std, B_init_fMRI, cfg, device=device)\n",
    "            losses.append(loss_terms.eval(\"train_ratio = @train_ratio\").eval(\"run = @run\"))\n",
    "\n",
    "            mean = torch.tensor(mean).to(device) #do we need this?\n",
    "            std  = torch.tensor(std).to(device)\n",
    "\n",
    "            wandb.init(project=cfg.project,\n",
    "                mode = \"offline\",\n",
    "                name=f\"TEST_{cfg.experiment_name}_run{run}_train_ratio_{train_ratio}\",\n",
    "                dir = cfg.output_dir,\n",
    "                config = OmegaConf.to_container(cfg, resolve=True))\n",
    "            \n",
    "            embedding_dir = os.path.join(cfg.output_dir, cfg.experiment_name, cfg.embedding_dir)\n",
    "            os.makedirs(embedding_dir, exist_ok=True)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for label, d, d_indices in (('train', train_dataset, train_indices), ('test', test_dataset, test_indices)):\n",
    "\n",
    "                    is_test = True\n",
    "                    if label == 'train':\n",
    "                        is_test = False\n",
    "                    \n",
    "                    X, y, _, _ = zip(*d)\n",
    "                    X = torch.stack(X)\n",
    "                    y = torch.stack(y)\n",
    "                    X, y, d_indices, _ = filter_nans(X, y, d_indices)\n",
    "                    X = X.to(device)\n",
    "                    y = y.to(device)\n",
    "\n",
    "                    X_embedded = model.encode_features(X)\n",
    "                    X_embedded = X_embedded.cpu().numpy()\n",
    "                    X_embedded = torch.tensor(sym_matrix_to_vec(X_embedded)).to(torch.float32).to(device)\n",
    "                    X_emb_reduced, X_emb_reduced_norm = model.encode_reduced_mat(X_embedded)\n",
    "                    \n",
    "                    if label == 'test' and train_ratio == 1.0:\n",
    "                        np.save(f'{recon_mat_dir}/test_idx_run{run}',d_indices)\n",
    "                        inv_feat_embedding = model.decode_reduced_mat(X_emb_reduced).detach().cpu().numpy()\n",
    "                        inv_feat_embedding = vec_to_sym_matrix(inv_feat_embedding)\n",
    "                        inv_feat_embedding = torch.tensor(inv_feat_embedding).to(torch.float32).to(device)\n",
    "                        recon_mat = model.decode_features(inv_feat_embedding)\n",
    "                        mape_mat = torch.abs((X - recon_mat) / (X + 1e-10)) * 100\n",
    "                        \n",
    "                        wandb_plot_test_recon_corr(wandb, cfg.experiment_name, cfg.work_dir, recon_mat.cpu().numpy(), X.cpu().numpy(), mape_mat.cpu().numpy(), True, run)\n",
    "                        wandb_plot_individual_recon(wandb, cfg.experiment_name, cfg.work_dir, d_indices, recon_mat.cpu().numpy(), X.cpu().numpy(), mape_mat.cpu().numpy(), 0, True, run)\n",
    "\n",
    "                        np.save(f'{recon_mat_dir}/recon_mat_run{run}', recon_mat.cpu().numpy())\n",
    "                        np.save(f'{recon_mat_dir}/mape_mat_run{run}', mape_mat.cpu().numpy())\n",
    "\n",
    "                    y_pred = model.decode_targets(X_emb_reduced_norm)\n",
    "\n",
    "                    save_embeddings(X_embedded, \"mat\", cfg, is_test, run)\n",
    "                    save_embeddings(X_emb_reduced_norm, \"joint\", cfg, is_test, run)\n",
    "\n",
    "                    if label == 'test':\n",
    "                        epsilon = 1e-8\n",
    "                        mape =  100 * torch.mean(torch.abs((y - y_pred)) / torch.abs((y + epsilon))).item()\n",
    "                        corr =  spearmanr(y.cpu().numpy().flatten(), y_pred.cpu().numpy().flatten())[0]\n",
    "\n",
    "                        wandb.log({\n",
    "                            'Run': run,\n",
    "                            \"Train ratio\": train_ratio,\n",
    "                            'Test | Target MAPE/val' : mape,\n",
    "                            'Test | Target Corr/val': corr,\n",
    "                            'Test | Train ratio' : train_ratio\n",
    "                            })\n",
    "            \n",
    "                    predictions[(train_ratio, run, label)] = (y.cpu().numpy(), y_pred.cpu().numpy(), d_indices)\n",
    "                    for i, idx in enumerate(d_indices):\n",
    "                        self.embeddings[label].append({\n",
    "                            'index': idx,\n",
    "                            'joint_embedding': X_emb_reduced[i].cpu().numpy()\n",
    "                        })\n",
    "            wandb.finish()\n",
    "            \n",
    "            self.results = (losses, predictions, self.embeddings)\n",
    "\n",
    "        if save_model:\n",
    "            saved_models_dir = os.path.join(cfg.output_dir, cfg.experiment_name, cfg.model_weight_dir)\n",
    "            os.makedirs(saved_models_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"{saved_models_dir}/model_weights_train_ratio{train_ratio}_run{run}.pth\")\n",
    "\n",
    "        return self.results\n",
    "\n",
    "    def checkpoint(self, *args, **kwargs):\n",
    "        print(\"Checkpointing\", flush=True)\n",
    "        return super().checkpoint(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project: HCP_CL_REDMAT_ENC_MULTIVAR\n",
      "experiment_name: 1run_train_redmat_enc_hcp_multivar\n",
      "hypothesis: '-'\n",
      "input_dim_feat: 400\n",
      "output_dim_feat: 100\n",
      "hidden_dim: 128\n",
      "input_dim_target: 21\n",
      "output_dim_target: 64\n",
      "seed: 42\n",
      "skip_conn: false\n",
      "skip_enc1: false\n",
      "ReEig: false\n",
      "full_model_pretrained: false\n",
      "mat_ae_pretrained: true\n",
      "reduced_mat_ae_pretrained: false\n",
      "pretrained_mat_ae_exp: pretrain_mat_ae_hcp\n",
      "pretrained_reduced_mat_ae_exp: ext_sup_reduced_mat_ae_abcd\n",
      "pretrained_full_model_exp: 1run_train_redmat_enc_hcp_sup_net_latest\n",
      "mat_ae_enc_freeze: true\n",
      "mat_ae_dec_freeze: true\n",
      "reduced_mat_ae_enc_freeze: false\n",
      "reduced_mat_ae_dec_freeze: true\n",
      "target_dec_freeze: true\n",
      "best_mat_ae_fold: 2\n",
      "best_reduced_mat_ae_fold: 4\n",
      "synth_exp: false\n",
      "multi_gpu: true\n",
      "num_epochs: 500\n",
      "batch_size: 256\n",
      "n_runs: 1\n",
      "lr: 0.0001\n",
      "weight_decay: 0.0001\n",
      "dropout_rate: 0\n",
      "scheduler_patience: 20\n",
      "test_ratio: 0.3\n",
      "train_ratio:\n",
      "- 1.0\n",
      "log_gradients: true\n",
      "clip_grad: true\n",
      "external_test_mode: false\n",
      "test_scanners:\n",
      "- GE MEDICAL SYSTEMS_DISCOVERY MR750\n",
      "- Philips Medical Systems_Achieva dStream\n",
      "- Philips Medical Systems_Ingenia\n",
      "SupCon_kernel: rbf\n",
      "SupConLoss_on_mat: false\n",
      "pft_base_temperature: 0.07\n",
      "pft_temperature: 0.07\n",
      "pft_sigma: 0.2\n",
      "pft_reg_term: 0.01\n",
      "ptt_base_temperature: 0.07\n",
      "ptt_temperature: 0.07\n",
      "ptt_sigma: 0.2\n",
      "ptt_reg_term: 0.01\n",
      "feature_autoencoder_crit: Norm\n",
      "joint_embedding_crit: cosine\n",
      "target_decoding_crit: MSE\n",
      "huber_delta: 10\n",
      "augmentation: None\n",
      "mat_threshold: 0\n",
      "dataset_path: /well/margulies/users/cpy397/contrastive-learning/data/hcp_kong_400parcels.nc\n",
      "targets:\n",
      "- PicVocab_AgeAdj\n",
      "- LifeSatisf_Unadj\n",
      "- ReadEng_AgeAdj\n",
      "- WM_Task_2bk_Acc\n",
      "- Relational_Task_Acc\n",
      "- Language_Task_Story_Acc\n",
      "- PMAT24_A_CR\n",
      "- VSPLOT_TC\n",
      "- CardSort_AgeAdj\n",
      "- ListSort_AgeAdj\n",
      "- Language_Task_Math_Acc\n",
      "- ProcSpeed_AgeAdj\n",
      "- PicSeq_AgeAdj\n",
      "- SCPT_SPEC\n",
      "- Flanker_AgeAdj\n",
      "- Emotion_Task_Face_Acc\n",
      "- ER40ANG\n",
      "- ER40NOE\n",
      "- MMSE_Score\n",
      "- ER40HAP\n",
      "- ER40FEAR\n",
      "standardize_target: true\n",
      "work_dir: /well/margulies/users/cpy397/contrastive-learning\n",
      "reconstructed_dir: recon_mat\n",
      "embedding_dir: embeddings\n",
      "model_weight_dir: saved_models\n",
      "output_dir: /well/margulies/users/cpy397/contrastive-learning/results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with initialize(version_base=None, config_path=\".\"):\n",
    "    cfg = compose(config_name='jupyter_main_model_config.yaml')\n",
    "    print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing target PicVocab_AgeAdj (min: 63.69, max: 153.0889) to [0, 1]\n",
      "Standardizing target LifeSatisf_Unadj (min: 23.7, max: 74.6) to [0, 1]\n",
      "Standardizing target ReadEng_AgeAdj (min: 60.11, max: 138.0873) to [0, 1]\n",
      "Standardizing target WM_Task_2bk_Acc (min: 37.153, max: 100.0) to [0, 1]\n",
      "Standardizing target Relational_Task_Acc (min: 35.417, max: 100.0) to [0, 1]\n",
      "Standardizing target Language_Task_Story_Acc (min: 50.0, max: 100.0) to [0, 1]\n",
      "Standardizing target PMAT24_A_CR (min: 4.0, max: 24.0) to [0, 1]\n",
      "Standardizing target VSPLOT_TC (min: 1.0, max: 26.0) to [0, 1]\n",
      "Standardizing target CardSort_AgeAdj (min: 57.79, max: 122.65) to [0, 1]\n",
      "Standardizing target ListSort_AgeAdj (min: 60.09, max: 140.86) to [0, 1]\n",
      "Standardizing target Language_Task_Math_Acc (min: 43.182, max: 100.0) to [0, 1]\n",
      "Standardizing target ProcSpeed_AgeAdj (min: 46.07, max: 149.3) to [0, 1]\n",
      "Standardizing target PicSeq_AgeAdj (min: 56.48, max: 134.81) to [0, 1]\n",
      "Standardizing target SCPT_SPEC (min: 0.7667, max: 1.0) to [0, 1]\n",
      "Standardizing target Flanker_AgeAdj (min: 71.24, max: 123.56) to [0, 1]\n",
      "Standardizing target Emotion_Task_Face_Acc (min: 43.333, max: 100.0) to [0, 1]\n",
      "Standardizing target ER40ANG (min: 2.0, max: 8.0) to [0, 1]\n",
      "Standardizing target ER40NOE (min: 0.0, max: 8.0) to [0, 1]\n",
      "Standardizing target MMSE_Score (min: 23.0, max: 30.0) to [0, 1]\n",
      "Standardizing target ER40HAP (min: 5.0, max: 8.0) to [0, 1]\n",
      "Standardizing target ER40FEAR (min: 0.0, max: 8.0) to [0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Size:   0%|                                                                                    | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[Ael Run:   0%|                                                                                        | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda, ratio 1.0\n",
      "Loading test indices from the pretraining experiment...\n",
      "Start training...\n",
      "Loading pretrained MatrixAutoencoder...\n",
      "Freezing weights for mat encoding...\n",
      "Freezing weights for mat decoding...\n",
      "Freezing weights for reduced mat decoding...\n",
      "Freezing TargetDecoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/slurm-30701279/ipykernel_464322/2881619367.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f\"{cfg.output_dir}/{cfg.pretrained_mat_ae_exp}/saved_models/autoencoder_weights_fold{cfg.best_mat_ae_fold}.pth\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A:   0%|                                                                                         | 0/500 [00:00<?, ?it/s]\n",
      "\n",
      "Model Run:   0%|                                                                                        | 0/1 [00:20<?, ?it/s]\n",
      "Training Size:   0%|                                                                                    | 0/1 [00:20<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 126\u001b[0m\n\u001b[1;32m    123\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/mape.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 72\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n_runs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Run\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     71\u001b[0m             run_model \u001b[38;5;241m=\u001b[39m ModelRun()\n\u001b[0;32m---> 72\u001b[0m             job \u001b[38;5;241m=\u001b[39m \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m             run_results\u001b[38;5;241m.\u001b[39mappend(job)\n\u001b[1;32m     75\u001b[0m losses, predictions, embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mrun_results)\n",
      "Cell \u001b[0;32mIn[5], line 82\u001b[0m, in \u001b[0;36mModelRun.__call__\u001b[0;34m(self, train, test_size, indices, train_ratio, run_size, run, dataset, cfg, random_state, device, save_model, path)\u001b[0m\n\u001b[1;32m     72\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(train_features,\n\u001b[1;32m     73\u001b[0m                               torch\u001b[38;5;241m.\u001b[39mfrom_numpy(train_targets)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m     74\u001b[0m                               train_inter_network_conn,\n\u001b[1;32m     75\u001b[0m                               train_intra_network_conn)\n\u001b[1;32m     77\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(test_features)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m     78\u001b[0m                              torch\u001b[38;5;241m.\u001b[39mfrom_numpy(test_targets)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m     79\u001b[0m                              test_inter_network_conn,\n\u001b[1;32m     80\u001b[0m                              test_intra_network_conn)\n\u001b[0;32m---> 82\u001b[0m loss_terms, model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB_init_fMRI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss_terms\u001b[38;5;241m.\u001b[39meval(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_ratio = @train_ratio\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39meval(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun = @run\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     85\u001b[0m mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(mean)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m#do we need this?\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 170\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(run, train_ratio, train_dataset, test_dataset, mean, std, B_init_fMRI, cfg, model, device)\u001b[0m\n\u001b[1;32m    167\u001b[0m embedded_feat_vectorized \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(embedded_feat_vectorized)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m## EMBEDDING OF THE REDUCED MATRIX\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m reduced_mat_embedding, reduced_mat_embedding_norm \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_reduced_mat\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded_feat_vectorized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m out_target_decoded \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecode_targets(reduced_mat_embedding_norm)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m## RECONSTRUCT REDUCED MATRIX FROM EMBEDDING AND THE FULL MATRIX FROM REDUCED\u001b[39;00m\n",
      "File \u001b[0;32m~/contrastive-learning/contrastive_phenotypes/src/ContModeling/models.py:62\u001b[0m, in \u001b[0;36mPhenoProj.encode_reduced_mat\u001b[0;34m(self, feat_embedding)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_reduced_mat\u001b[39m(\u001b[38;5;28mself\u001b[39m, feat_embedding): \u001b[38;5;66;03m# note that the feat embedding was vectorized\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     embedding, embedding_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduced_matrix_ae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_reduced_mat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat_embedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embedding, embedding_norm\n",
      "File \u001b[0;32m~/contrastive-learning/contrastive_phenotypes/src/ContModeling/models.py:190\u001b[0m, in \u001b[0;36mReducedMatAutoEncoder.embed_reduced_mat\u001b[0;34m(self, reduced_mat)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_conn:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_reduced_mat \u001b[38;5;241m=\u001b[39m reduced_mat\n\u001b[0;32m--> 190\u001b[0m feat_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduced_mat_to_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduced_mat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m feat_embedding_norm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize(feat_embedding, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m feat_embedding, feat_embedding_norm\n",
      "File \u001b[0;32m~/python/neuro/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/python/neuro/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/python/neuro/lib/python3.11/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/python/neuro/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/python/neuro/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/python/neuro/lib/python3.11/site-packages/torch/nn/modules/activation.py:532\u001b[0m, in \u001b[0;36mELU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/python/neuro/lib/python3.11/site-packages/torch/nn/functional.py:1593\u001b[0m, in \u001b[0;36melu\u001b[0;34m(input, alpha, inplace)\u001b[0m\n\u001b[1;32m   1591\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39melu_(\u001b[38;5;28minput\u001b[39m, alpha)\n\u001b[1;32m   1592\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1593\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main(cfg=cfg):\n",
    "\n",
    "    results_dir = os.path.join(cfg.output_dir, cfg.experiment_name)\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    random_state = np.random.RandomState(seed=42)\n",
    "\n",
    "    dataset_path = cfg.dataset_path\n",
    "\n",
    "    if isinstance(cfg.targets, str):\n",
    "        \n",
    "        targets =[cfg.targets]\n",
    "    else:\n",
    "        targets = list(cfg.targets)\n",
    "        \n",
    "    test_ratio = cfg.test_ratio\n",
    "\n",
    "    dataset = MatData(dataset_path, targets, synth_exp = cfg.synth_exp, standardize_target=cfg.standardize_target, threshold=cfg.mat_threshold)\n",
    "    n_sub = len(dataset)\n",
    "    test_size = int(test_ratio * n_sub)\n",
    "    indices = np.arange(n_sub)\n",
    "    n_runs = cfg.n_runs\n",
    "    multi_gpu = False\n",
    "    train_ratios = list(cfg.train_ratio)\n",
    "    \n",
    "    multi_gpu = False\n",
    "    if multi_gpu:\n",
    "        print(\"Using multi-gpu\")\n",
    "        log_folder = Path(\"logs\")\n",
    "        executor = submitit.AutoExecutor(folder=str(log_folder / \"%j\"))\n",
    "        executor.update_parameters(\n",
    "            timeout_min=120,\n",
    "            slurm_partition=\"gpu_short\",\n",
    "            gpus_per_node=1,\n",
    "            tasks_per_node=1,\n",
    "            nodes=1\n",
    "            #slurm_constraint=\"v100-32g\",\n",
    "        )\n",
    "        run_jobs = []\n",
    "\n",
    "        with executor.batch():\n",
    "            for train_ratio in tqdm(train_ratios, desc=\"Training Size\"):\n",
    "                train_size = int(n_sub * (1 - test_ratio) * train_ratio)\n",
    "                run_size = test_size + train_size\n",
    "                for run in tqdm(range(n_runs)):\n",
    "                    run_model = ModelRun()\n",
    "                    job = executor.submit(run_model,\n",
    "                                          train, test_size,\n",
    "                                          indices,\n",
    "                                          train_ratio,\n",
    "                                          run_size, run,\n",
    "                                          dataset,\n",
    "                                          cfg,\n",
    "                                          random_state=random_state, device=None)\n",
    "                    run_jobs.append(job)\n",
    "\n",
    "        async def get_result(run_jobs):\n",
    "            run_results = []\n",
    "            for aws in tqdm(asyncio.as_completed([j.awaitable().result() for j in run_jobs]), total=len(run_jobs)):\n",
    "                res = await aws\n",
    "                run_results.append(res)\n",
    "            return run_results\n",
    "        run_results = asyncio.run(get_result(run_jobs))\n",
    "\n",
    "    else:\n",
    "        run_results = []\n",
    "        for train_ratio in tqdm(train_ratios, desc=\"Training Size\"):\n",
    "            train_size = int(n_sub * (1 - test_ratio) * train_ratio)\n",
    "            run_size = test_size + train_size\n",
    "            for run in tqdm(range(n_runs), desc=\"Model Run\"):\n",
    "                run_model = ModelRun()\n",
    "                job = run_model(train, test_size, indices, train_ratio, run_size, run, dataset, cfg, random_state=random_state, device=None)\n",
    "                run_results.append(job)\n",
    "\n",
    "    losses, predictions, embeddings = zip(*run_results)\n",
    "\n",
    "    prediction_metrics = predictions[0]\n",
    "    for prediction in predictions[1:]:\n",
    "        prediction_metrics.update(prediction)\n",
    "\n",
    "    pred_results = []\n",
    "    for k, v in prediction_metrics.items():\n",
    "        true_targets, predicted_targets, indices = v\n",
    "        \n",
    "        true_targets_dict = {\"train_ratio\": [k[0]] * len(true_targets),\n",
    "                             \"model_run\":[k[1]] * len(true_targets),\n",
    "                             \"dataset\":[k[2]] * len(true_targets)\n",
    "                            }\n",
    "        predicted_targets_dict = {\"indices\": indices}\n",
    "        \n",
    "        for i, target in enumerate(targets):\n",
    "            true_targets_dict[target] = true_targets[:, i]\n",
    "            predicted_targets_dict[f\"{target}_pred\"] = predicted_targets[:, i]\n",
    "            \n",
    "            \n",
    "        true_targets = pd.DataFrame(true_targets_dict)\n",
    "        predicted_targets = pd.DataFrame(predicted_targets_dict)\n",
    "        \n",
    "        pred_results.append(pd.concat([true_targets, predicted_targets], axis = 1))\n",
    "    pred_results = pd.concat(pred_results)\n",
    "    pred_results.to_csv(f\"{results_dir}/pred_results.csv\", index=False)\n",
    "\n",
    "    prediction_mape_by_element = []\n",
    "    for k, v in prediction_metrics.items():\n",
    "        true_targets, predicted_targets, indices = v\n",
    "        \n",
    "        mape_by_element = np.abs(true_targets - predicted_targets) / (np.abs(true_targets)+1e-10)\n",
    "        \n",
    "        for i, mape in enumerate(mape_by_element):\n",
    "            prediction_mape_by_element.append(\n",
    "                {\n",
    "                    'train_ratio': k[0],\n",
    "                    'model_run': k[1],\n",
    "                    'dataset': k[2],\n",
    "                    'mape': mape\n",
    "                }\n",
    "            )\n",
    "\n",
    "    df = pd.DataFrame(prediction_mape_by_element)\n",
    "    df = pd.concat([df.drop('mape', axis=1), df['mape'].apply(pd.Series)], axis=1)\n",
    "    df.columns = ['train_ratio', 'model_run', 'dataset'] + targets\n",
    "    df= df.groupby(['train_ratio', 'model_run', 'dataset']).agg('mean').reset_index()\n",
    "    df.to_csv(f\"{results_dir}/mape.csv\", index = False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
