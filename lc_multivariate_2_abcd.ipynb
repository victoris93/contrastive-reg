{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs3/well/margulies/users/cpy397/python/neuro/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import yaml\n",
    "import wandb\n",
    "import xarray as xr\n",
    "import asyncio\n",
    "import submitit\n",
    "import pickle\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "from nilearn.connectome import sym_matrix_to_vec, vec_to_sym_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    ")\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, TensorDataset\n",
    "from tqdm.auto import tqdm\n",
    "# from augmentations import augs, aug_args\n",
    "import glob, os, shutil\n",
    "from nilearn.datasets import fetch_atlas_schaefer_2018\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from ContModeling.utils import (\n",
    "    gaussian_kernel,\n",
    "    cauchy,\n",
    "    rbf,\n",
    "    standardize,\n",
    "    save_embeddings,\n",
    "    filter_nans\n",
    ")\n",
    "from ContModeling.losses import LogEuclideanLoss, NormLoss, KernelizedSupCon, OutlierRobustMSE\n",
    "from ContModeling.models import PhenoProj\n",
    "from ContModeling.helper_classes import MatData\n",
    "from ContModeling.viz_func import wandb_plot_acc_vs_baseline, wandb_plot_test_recon_corr, wandb_plot_individual_recon\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "multi_gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_LOSSES ={\n",
    "    'Norm': NormLoss(),\n",
    "    'LogEuclidean': LogEuclideanLoss(),\n",
    "    'MSE': nn.functional.mse_loss,\n",
    "    'MAE': nn.functional.l1_loss,\n",
    "    'MSERobust': OutlierRobustMSE(),\n",
    "    'Huber': nn.HuberLoss(),\n",
    "    'cosine': nn.functional.cosine_embedding_loss,\n",
    "}\n",
    "\n",
    "SUPCON_KERNELS = {\n",
    "    'cauchy': cauchy,\n",
    "    'gaussian_kernel': gaussian_kernel,\n",
    "    'rbf': rbf,\n",
    "    'None': None\n",
    "    }\n",
    "\n",
    "def train(run, train_ratio, train_dataset, test_dataset, mean, std, B_init_fMRI, cfg, model=None, device=device):\n",
    "    print(\"Start training...\")\n",
    "\n",
    "    # MODEL DIMS\n",
    "    input_dim_feat = cfg.input_dim_feat\n",
    "    input_dim_target = cfg.input_dim_target\n",
    "    hidden_dim = cfg.hidden_dim\n",
    "    output_dim_target = cfg.output_dim_target\n",
    "    output_dim_feat = cfg.output_dim_feat\n",
    "    kernel = SUPCON_KERNELS[cfg.SupCon_kernel]\n",
    "    \n",
    "    # TRAINING PARAMS\n",
    "    lr = cfg.lr\n",
    "    batch_size = cfg.batch_size\n",
    "    dropout_rate = cfg.dropout_rate\n",
    "    weight_decay = cfg.weight_decay\n",
    "    num_epochs = cfg.num_epochs\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    mean= torch.tensor(mean).to(device)\n",
    "    std = torch.tensor(std).to(device)\n",
    "    if model is None:\n",
    "        model = PhenoProj(\n",
    "            input_dim_feat,\n",
    "            input_dim_target,\n",
    "            hidden_dim,\n",
    "            output_dim_target,\n",
    "            output_dim_feat,\n",
    "            dropout_rate,\n",
    "            cfg\n",
    "        ).to(device)\n",
    "\n",
    "    if cfg.full_model_pretrained:\n",
    "        print(f\"Loading pretrained FULL model, train ratio {train_ratio}...\")\n",
    "        state_dict = torch.load(f\"{cfg.output_dir}/{cfg.pretrained_full_model_exp}/saved_models/model_weights_train_ratio{train_ratio}_run0.pth\")\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    else:\n",
    "        if cfg.mat_ae_pretrained:\n",
    "            print(\"Loading pretrained MatrixAutoencoder...\")\n",
    "            state_dict = torch.load(f\"{cfg.output_dir}/{cfg.pretrained_mat_ae_exp}/saved_models/autoencoder_weights_fold{cfg.best_mat_ae_fold}.pth\")\n",
    "            model.matrix_ae.load_state_dict(state_dict)\n",
    "\n",
    "        if cfg.reduced_mat_ae_pretrained:\n",
    "            print(\"Loading pretrained ReducedMatrixAutoencoder...\")\n",
    "            state_dict = torch.load(f\"{cfg.output_dir}/{cfg.pretrained_reduced_mat_ae_exp}/saved_models/autoencoder_weights_fold{cfg.best_reduced_mat_ae_fold}.pth\")\n",
    "            model.reduced_matrix_ae.load_state_dict(state_dict)\n",
    "        \n",
    "    if cfg.mat_ae_enc_freeze:\n",
    "        print(\"Freezing weights for mat encoding...\")\n",
    "        for param in model.matrix_ae.enc_mat1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.matrix_ae.enc_mat2.parameters():\n",
    "            param.requires_grad = False\n",
    "    else:\n",
    "        model.matrix_ae.enc_mat1.weight = torch.nn.Parameter(B_init_fMRI.transpose(0,1))\n",
    "        model.matrix_ae.enc_mat2.weight = torch.nn.Parameter(B_init_fMRI.transpose(0,1))\n",
    "\n",
    "    if cfg.mat_ae_dec_freeze:\n",
    "        print(\"Freezing weights for mat decoding...\")\n",
    "        for param in model.matrix_ae.dec_mat1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.matrix_ae.dec_mat2.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    if cfg.reduced_mat_ae_enc_freeze:\n",
    "        print(\"Freezing weights for reduced mat encoding...\")\n",
    "        for param in model.reduced_matrix_ae.reduced_mat_to_embed.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    if cfg.reduced_mat_ae_dec_freeze:\n",
    "        print(\"Freezing weights for reduced mat decoding...\")\n",
    "        for param in model.reduced_matrix_ae.embed_to_reduced_mat.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    if cfg.target_dec_freeze:\n",
    "        print(\"Freezing TargetDecoder...\")\n",
    "        for param in model.target_dec.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    criterion_pft = KernelizedSupCon(\n",
    "        method=\"expw\",\n",
    "        temperature=cfg.pft_temperature,\n",
    "        base_temperature= cfg.pft_base_temperature,\n",
    "        reg_term = cfg.pft_reg_term,\n",
    "        kernel=kernel,\n",
    "        krnl_sigma=cfg.pft_sigma,\n",
    "    )\n",
    "\n",
    "    criterion_ptt = KernelizedSupCon(\n",
    "        method=\"expw\",\n",
    "        temperature=cfg.ptt_temperature,\n",
    "        base_temperature= cfg.ptt_base_temperature,\n",
    "        reg_term = cfg.ptt_reg_term,\n",
    "        kernel=kernel,\n",
    "        krnl_sigma=cfg.ptt_sigma,\n",
    "    )\n",
    "    \n",
    "    feature_autoencoder_crit = EMB_LOSSES[cfg.feature_autoencoder_crit]\n",
    "    target_decoding_crit = EMB_LOSSES[cfg.target_decoding_crit]\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=cfg.lr, weight_decay=weight_decay,\n",
    "    #                             momentum=0.9)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.1, patience = cfg.scheduler_patience)\n",
    "\n",
    "    loss_terms = []\n",
    "    validation = []\n",
    "    autoencoder_features = []\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    wandb.init(project=cfg.project,\n",
    "        mode = \"offline\",\n",
    "        name=f\"{cfg.experiment_name}_run{run}_train_ratio_{train_ratio}\",\n",
    "        dir = cfg.output_dir,\n",
    "        config = OmegaConf.to_container(cfg, resolve=True))\n",
    "\n",
    "    with tqdm(range(num_epochs), desc=\"Epochs\", leave=False) as pbar:\n",
    "        for epoch in pbar:\n",
    "            model.train()\n",
    "\n",
    "            if cfg.reduced_mat_ae_pretrained:\n",
    "                model.reduced_matrix_ae.eval()\n",
    "            if cfg.reduced_mat_ae_enc_freeze:\n",
    "                model.reduced_matrix_ae.reduced_mat_to_embed.eval()\n",
    "            if cfg.reduced_mat_ae_dec_freeze:\n",
    "                model.reduced_matrix_ae.embed_to_reduced_mat.eval()\n",
    "            if cfg.target_dec_freeze:\n",
    "                model.target_dec.eval()\n",
    "                \n",
    "            loss_terms_batch = defaultdict(lambda:0)\n",
    "            for features, targets, inter_network_conn, _ in train_loader:\n",
    "\n",
    "                loss = 0\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                features, targets, _, inter_network_conn = filter_nans(features, targets, _z=inter_network_conn)\n",
    "\n",
    "                features = features.to(device)\n",
    "                targets = targets.to(device)\n",
    "                inter_network_conn = inter_network_conn.to(device)\n",
    "\n",
    "                ## FEATURE ENCODING == MATRIX REDUCTION\n",
    "                embedded_feat = model.encode_features(features)\n",
    "                \n",
    "                ## VECTORIZE REDUCED MATRIX\n",
    "                embedded_feat_vectorized = sym_matrix_to_vec(embedded_feat.detach().cpu().numpy())\n",
    "                embedded_feat_vectorized = torch.tensor(embedded_feat_vectorized).to(torch.float32).to(device)\n",
    "\n",
    "                ## EMBEDDING OF THE REDUCED MATRIX\n",
    "                reduced_mat_embedding, reduced_mat_embedding_norm = model.encode_reduced_mat(embedded_feat_vectorized)\n",
    "                out_target_decoded = model.decode_targets(reduced_mat_embedding_norm)\n",
    "\n",
    "                ## RECONSTRUCT REDUCED MATRIX FROM EMBEDDING AND THE FULL MATRIX FROM REDUCED\n",
    "                recon_reduced_mat = model.decode_reduced_mat(reduced_mat_embedding)\n",
    "\n",
    "                if not cfg.reduced_mat_ae_dec_freeze:\n",
    "                    reduced_mat_recon_loss = feature_autoencoder_crit(embedded_feat_vectorized, recon_reduced_mat) / 1000\n",
    "                    loss += reduced_mat_recon_loss\n",
    "\n",
    "                recon_reduced_mat = vec_to_sym_matrix(recon_reduced_mat.detach().cpu().numpy())\n",
    "                recon_reduced_mat = torch.tensor(recon_reduced_mat).to(torch.float32).to(device)\n",
    "\n",
    "                reconstructed_feat = model.decode_features(recon_reduced_mat)\n",
    "\n",
    "                ## LOSS: TARGET DECODING FROM TARGET EMBEDDING\n",
    "                if cfg.target_decoding_crit == 'Huber' and cfg.huber_delta != 'None':\n",
    "                    target_decoding_crit = nn.HuberLoss(delta = cfg.huber_delta)\n",
    "\n",
    "                if not cfg.reduced_mat_ae_enc_freeze:\n",
    "                    ## KERNLIZED LOSS: MAT embedding vs targets\n",
    "                    kernel_embedded_target_loss, _ = criterion_ptt(reduced_mat_embedding_norm.unsqueeze(1), targets)\n",
    "                    kernel_embedded_network_loss, _ = criterion_pft(reduced_mat_embedding_norm.unsqueeze(1), inter_network_conn)\n",
    "                    loss += (kernel_embedded_target_loss + kernel_embedded_network_loss)\n",
    "\n",
    "                if not cfg.mat_ae_enc_freeze or not cfg.mat_ae_dec_freeze:\n",
    "                    feature_autoencoder_loss = feature_autoencoder_crit(features, reconstructed_feat) / 1000\n",
    "                    loss += feature_autoencoder_loss\n",
    "\n",
    "                if not cfg.target_dec_freeze:\n",
    "                    target_decoding_from_reduced_emb_loss = target_decoding_crit(targets, out_target_decoded)\n",
    "                    loss += target_decoding_from_reduced_emb_loss\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                if cfg.clip_grad:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    \n",
    "                if cfg.log_gradients:\n",
    "                    for name, param in model.named_parameters():\n",
    "                        if param.grad is not None:\n",
    "                            wandb.log({\n",
    "                                \"Epoch\": epoch,\n",
    "                                'Run': run,\n",
    "                                \"Train ratio\": train_ratio,\n",
    "                                f\"Gradient Norm/{name}\": param.grad.norm().item()\n",
    "                            })  \n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                loss_terms_batch['loss'] = loss.item() / len(features)\n",
    "\n",
    "                if not cfg.reduced_mat_ae_enc_freeze:\n",
    "                    loss_terms_batch['kernel_embedded_target_loss'] = kernel_embedded_target_loss.item() / len(features)\n",
    "                    loss_terms_batch['kernel_embedded_network_loss'] = kernel_embedded_network_loss.item() / len(features)\n",
    "\n",
    "                    wandb.log({\n",
    "                        'Epoch': epoch,\n",
    "                        'Run': run,\n",
    "                        \"Train ratio\": train_ratio,\n",
    "                        'kernel_embedded_target_loss': loss_terms_batch['kernel_embedded_target_loss'],\n",
    "                        'kernel_embedded_network_loss': loss_terms_batch['kernel_embedded_network_loss'],\n",
    "                    })\n",
    "\n",
    "                if not cfg.reduced_mat_ae_dec_freeze:\n",
    "                    loss_terms_batch['reduced_mat_recon_loss'] = reduced_mat_recon_loss.item() / len(features)\n",
    "                    wandb.log({\n",
    "                        'Epoch': epoch,\n",
    "                        'Run': run,\n",
    "                        \"Train ratio\": train_ratio,\n",
    "                        'reduced_mat_recon_loss': loss_terms_batch['reduced_mat_recon_loss'],\n",
    "                    })\n",
    "                \n",
    "                if not cfg.target_dec_freeze:\n",
    "                    loss_terms_batch['target_decoding_loss'] = target_decoding_from_reduced_emb_loss.item() / len(features)\n",
    "                    wandb.log({\n",
    "                        'Epoch': epoch,\n",
    "                        'Run': run,\n",
    "                        \"Train ratio\": train_ratio,\n",
    "                        'target_decoding_loss': loss_terms_batch['target_decoding_loss'],\n",
    "                    })\n",
    "\n",
    "                # loss_terms_batch['direction_reg_target_loss'] = direction_reg_target.item() / len(features)\n",
    "                \n",
    "                if not cfg.mat_ae_enc_freeze or not cfg.mat_ae_dec_freeze:\n",
    "                    loss_terms_batch['feature_autoencoder_loss'] = feature_autoencoder_loss.item() / len(features)\n",
    "                    wandb.log({\n",
    "                        'Epoch': epoch,\n",
    "                        'Run': run,\n",
    "                        \"Train ratio\": train_ratio,\n",
    "                        'feature_autoencoder_loss': loss_terms_batch['feature_autoencoder_loss'],\n",
    "                    })\n",
    "                \n",
    "                wandb.log({\n",
    "                    'Epoch': epoch,\n",
    "                    'Run': run,\n",
    "                    \"Train ratio\": train_ratio,\n",
    "                    'total_loss': loss_terms_batch['loss'],\n",
    "                })\n",
    "\n",
    "            loss_terms_batch['epoch'] = epoch\n",
    "            loss_terms.append(loss_terms_batch)\n",
    "\n",
    "            model.eval()\n",
    "            mape_batch = 0\n",
    "            corr_batch = 0\n",
    "            with torch.no_grad():\n",
    "                for features, targets, _, _ in test_loader:\n",
    "\n",
    "                    features, targets, _, _ = filter_nans(features, targets)\n",
    "                    \n",
    "                    features, targets = features.to(device), targets.to(device)                    \n",
    "                    reduced_mat = model.encode_features(features)\n",
    "                    \n",
    "                    reduced_mat = torch.tensor(sym_matrix_to_vec(reduced_mat.detach().cpu().numpy())).to(torch.float32).to(device)\n",
    "                    embedding, embedding_norm = model.encode_reduced_mat(reduced_mat)\n",
    "                    out_target_decoded = model.decode_targets(embedding_norm)\n",
    "                    \n",
    "                    epsilon = 1e-8\n",
    "\n",
    "                    mape =  torch.mean(torch.abs((targets - out_target_decoded)) / torch.abs((targets + epsilon))) * 100\n",
    "                    if torch.isnan(mape):\n",
    "                        mape = torch.tensor(0.0)\n",
    "\n",
    "                    corr =  spearmanr(targets.cpu().numpy().flatten(), out_target_decoded.cpu().numpy().flatten())[0]\n",
    "                    if np.isnan(corr):\n",
    "                        corr = 0.0\n",
    "                        \n",
    "                    mape_batch+=mape.item()\n",
    "                    corr_batch += corr\n",
    "\n",
    "                mape_batch = mape_batch/len(test_loader)\n",
    "                corr_batch = corr_batch/len(test_loader)\n",
    "                validation.append(mape_batch)\n",
    "\n",
    "            wandb.log({\n",
    "                'Run': run,\n",
    "                \"Train ratio\": train_ratio,\n",
    "                'Target MAPE/val' : mape_batch,\n",
    "                'Target Corr/val': corr_batch,\n",
    "                })\n",
    "            \n",
    "            scheduler.step(mape_batch)\n",
    "            if np.log10(scheduler._last_lr[0]) < -5:\n",
    "                break\n",
    "\n",
    "            pbar.set_postfix_str(\n",
    "                f\"Epoch {epoch} \"\n",
    "                f\"| Loss {loss_terms[-1]['loss']:.02f} \"\n",
    "                f\"| val Target MAPE {mape_batch:.02f}\"\n",
    "                f\"| val Target Corr {corr_batch:.02f} \"\n",
    "                f\"| log10 lr {np.log10(scheduler._last_lr[0])}\"\n",
    "            )\n",
    "    wandb.finish()\n",
    "    loss_terms = pd.DataFrame(loss_terms)\n",
    "    return loss_terms, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRun(submitit.helpers.Checkpointable):\n",
    "    def __init__(self):\n",
    "        self.results = None\n",
    "        self.embeddings = None\n",
    "\n",
    "    def __call__(self, train, test_size, indices, train_ratio, run_size, run, dataset, cfg, random_state=None, device=None, save_model = True, path: Path = None):\n",
    "        if self.results is None:\n",
    "            if device is None:\n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                print(f\"Device {device}, ratio {train_ratio}\", flush=True)\n",
    "            if not isinstance(random_state, np.random.RandomState):\n",
    "                random_state = np.random.RandomState(random_state)\n",
    "            \n",
    "            recon_mat_dir = os.path.join(cfg.output_dir, cfg.experiment_name, cfg.reconstructed_dir)\n",
    "            os.makedirs(recon_mat_dir, exist_ok=True)\n",
    "    \n",
    "            predictions = {}\n",
    "            autoencoder_features = {}\n",
    "            losses = []\n",
    "            self.embeddings = {'train': [], 'test': []}\n",
    "            self.run = run\n",
    "\n",
    "            if cfg.mat_ae_pretrained:\n",
    "                print(\"Loading test indices from the pretraining experiment...\")\n",
    "                test_indices = np.load(f\"{cfg.output_dir}/{cfg.pretrained_mat_ae_exp}/test_idx.npy\")\n",
    "                train_indices = np.setdiff1d(indices, test_indices)\n",
    "                if train_ratio < 1.0:\n",
    "                    train_size = int(len(train_indices) * train_ratio)\n",
    "                    train_indices = random_state.choice(train_indices, train_size, replace=False)\n",
    "\n",
    "            elif cfg.external_test_mode:\n",
    "                test_scanners = list(cfg.test_scanners)\n",
    "                xr_dataset = xr.open_dataset(cfg.dataset_path)\n",
    "                scanner_mask = np.sum([xr_dataset.isin(scanner).scanner.values for scanner in test_scanners],\n",
    "                                    axis = 0).astype(bool)\n",
    "                test_indices = indices[scanner_mask]\n",
    "                train_indices = indices[~scanner_mask]\n",
    "                if train_ratio < 1.0:\n",
    "                    train_size = int(len(train_indices) * train_ratio)\n",
    "                    train_indices = random_state.choice(train_indices, train_size, replace=False)\n",
    "                del xr_dataset\n",
    "\n",
    "            else:\n",
    "                run_indices = random_state.choice(indices, run_size, replace=False)\n",
    "                train_indices, test_indices = train_test_split(run_indices, test_size=test_size, random_state=random_state)\n",
    "                \n",
    "            train_dataset = Subset(dataset, train_indices)\n",
    "            test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "            train_features = train_dataset.dataset.matrices[train_dataset.indices]\n",
    "            train_targets = train_dataset.dataset.targets[train_dataset.indices].numpy()\n",
    "            train_inter_network_conn = train_dataset.dataset.inter_network_conn[train_dataset.indices]\n",
    "            train_intra_network_conn = train_dataset.dataset.intra_network_conn[train_dataset.indices]\n",
    "\n",
    "            std_train_targets, mean, std = standardize(train_targets)\n",
    "            # scaler = MinMaxScaler().fit(train_targets)\n",
    "            # train_targets = scaler.transform(train_targets)\n",
    "\n",
    "            input_dim_feat =cfg.input_dim_feat\n",
    "            output_dim_feat = cfg.output_dim_feat\n",
    "\n",
    "            ## Weight initialization for bilinear layer\n",
    "            mean_f = torch.mean(train_features, dim=0).to(device)\n",
    "            [D,V] = torch.linalg.eigh(mean_f,UPLO = \"U\")\n",
    "            B_init_fMRI = V[:,input_dim_feat-output_dim_feat:]\n",
    "\n",
    "            test_features= test_dataset.dataset.matrices[test_dataset.indices].numpy()\n",
    "            test_targets = test_dataset.dataset.targets[test_dataset.indices].numpy()\n",
    "            test_inter_network_conn = test_dataset.dataset.inter_network_conn[test_dataset.indices]\n",
    "            test_intra_network_conn = test_dataset.dataset.intra_network_conn[test_dataset.indices]\n",
    "\n",
    "            train_dataset = TensorDataset(train_features,\n",
    "                                          torch.from_numpy(train_targets).to(torch.float32),\n",
    "                                          train_inter_network_conn,\n",
    "                                          train_intra_network_conn)\n",
    "            \n",
    "            test_dataset = TensorDataset(torch.from_numpy(test_features).to(torch.float32),\n",
    "                                         torch.from_numpy(test_targets).to(torch.float32),\n",
    "                                         test_inter_network_conn,\n",
    "                                         test_intra_network_conn)\n",
    "\n",
    "            loss_terms, model = train(run, train_ratio, train_dataset, test_dataset, mean, std, B_init_fMRI, cfg, device=device)\n",
    "            losses.append(loss_terms.eval(\"train_ratio = @train_ratio\").eval(\"run = @run\"))\n",
    "\n",
    "            mean = torch.tensor(mean).to(device) #do we need this?\n",
    "            std  = torch.tensor(std).to(device)\n",
    "\n",
    "            wandb.init(project=cfg.project,\n",
    "                mode = \"offline\",\n",
    "                name=f\"TEST_{cfg.experiment_name}_run{run}_train_ratio_{train_ratio}\",\n",
    "                dir = cfg.output_dir,\n",
    "                config = OmegaConf.to_container(cfg, resolve=True))\n",
    "            \n",
    "            embedding_dir = os.path.join(cfg.output_dir, cfg.experiment_name, cfg.embedding_dir)\n",
    "            os.makedirs(embedding_dir, exist_ok=True)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for label, d, d_indices in (('train', train_dataset, train_indices), ('test', test_dataset, test_indices)):\n",
    "\n",
    "                    is_test = True\n",
    "                    if label == 'train':\n",
    "                        is_test = False\n",
    "                    \n",
    "                    X, y, _, _ = zip(*d)\n",
    "                    X = torch.stack(X)\n",
    "                    y = torch.stack(y)\n",
    "                    X, y, d_indices, _ = filter_nans(X, y, d_indices)\n",
    "                    X = X.to(device)\n",
    "                    y = y.to(device)\n",
    "\n",
    "                    X_embedded = model.encode_features(X)\n",
    "                    X_embedded = X_embedded.cpu().numpy()\n",
    "                    X_embedded = torch.tensor(sym_matrix_to_vec(X_embedded)).to(torch.float32).to(device)\n",
    "                    X_emb_reduced, X_emb_reduced_norm = model.encode_reduced_mat(X_embedded)\n",
    "                    \n",
    "                    if label == 'test' and train_ratio == 1.0:\n",
    "                        np.save(f'{recon_mat_dir}/test_idx_run{run}',d_indices)\n",
    "                        inv_feat_embedding = model.decode_reduced_mat(X_emb_reduced).detach().cpu().numpy()\n",
    "                        inv_feat_embedding = vec_to_sym_matrix(inv_feat_embedding)\n",
    "                        inv_feat_embedding = torch.tensor(inv_feat_embedding).to(torch.float32).to(device)\n",
    "                        recon_mat = model.decode_features(inv_feat_embedding)\n",
    "                        mape_mat = torch.abs((X - recon_mat) / (X + 1e-10)) * 100\n",
    "                        \n",
    "                        wandb_plot_test_recon_corr(wandb, cfg.experiment_name, cfg.work_dir, recon_mat.cpu().numpy(), X.cpu().numpy(), mape_mat.cpu().numpy(), True, run)\n",
    "                        wandb_plot_individual_recon(wandb, cfg.experiment_name, cfg.work_dir, d_indices, recon_mat.cpu().numpy(), X.cpu().numpy(), mape_mat.cpu().numpy(), 0, True, run)\n",
    "\n",
    "                        np.save(f'{recon_mat_dir}/recon_mat_run{run}', recon_mat.cpu().numpy())\n",
    "                        np.save(f'{recon_mat_dir}/mape_mat_run{run}', mape_mat.cpu().numpy())\n",
    "                    y_pred = model.decode_targets(X_emb_reduced_norm)\n",
    "\n",
    "                    save_embeddings(X_embedded, \"mat\", cfg, is_test, run)\n",
    "                    save_embeddings(X_emb_reduced_norm, \"joint\", cfg, is_test, run)\n",
    "\n",
    "                    if label == 'test':\n",
    "                        epsilon = 1e-8\n",
    "                        mape =  100 * torch.mean(torch.abs((y - y_pred)) / torch.abs((y + epsilon))).item()\n",
    "                        corr =  spearmanr(y.cpu().numpy().flatten(), y_pred.cpu().numpy().flatten())[0]\n",
    "\n",
    "                        wandb.log({\n",
    "                            'Run': run,\n",
    "                            \"Train ratio\": train_ratio,\n",
    "                            'Test | Target MAPE/val' : mape,\n",
    "                            'Test | Target Corr/val': corr,\n",
    "                            'Test | Train ratio' : train_ratio\n",
    "                            })\n",
    "            \n",
    "                    predictions[(train_ratio, run, label)] = (y.cpu().numpy(), y_pred.cpu().numpy(), d_indices)\n",
    "                    for i, idx in enumerate(d_indices):\n",
    "                        self.embeddings[label].append({\n",
    "                            'index': idx,\n",
    "                            'joint_embedding': X_emb_reduced[i].cpu().numpy()\n",
    "                        })\n",
    "            wandb.finish()\n",
    "            \n",
    "            self.results = (losses, predictions, self.embeddings)\n",
    "\n",
    "        if save_model:\n",
    "            saved_models_dir = os.path.join(cfg.output_dir, cfg.experiment_name, cfg.model_weight_dir)\n",
    "            os.makedirs(saved_models_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"{saved_models_dir}/model_weights_train_ratio{train_ratio}_run{run}.pth\")\n",
    "\n",
    "        return self.results\n",
    "\n",
    "    def checkpoint(self, *args, **kwargs):\n",
    "        print(\"Checkpointing\", flush=True)\n",
    "        return super().checkpoint(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project: HCP_CL_REDMAT_ENC\n",
      "experiment_name: 1run_train_redmat_enc_hcp_sup_net_latest\n",
      "hypothesis: '-'\n",
      "input_dim_feat: 400\n",
      "output_dim_feat: 100\n",
      "hidden_dim: 128\n",
      "input_dim_target: 1\n",
      "output_dim_target: 64\n",
      "seed: 42\n",
      "skip_conn: false\n",
      "skip_enc1: false\n",
      "ReEig: false\n",
      "full_model_pretrained: false\n",
      "mat_ae_pretrained: true\n",
      "reduced_mat_ae_pretrained: false\n",
      "pretrained_mat_ae_exp: pretrain_mat_ae_hcp\n",
      "pretrained_reduced_mat_ae_exp: ext_sup_reduced_mat_ae_abcd\n",
      "pretrained_full_model_exp: 1run_train_target_dec_hcp_sup_net_latest\n",
      "mat_ae_enc_freeze: true\n",
      "mat_ae_dec_freeze: true\n",
      "reduced_mat_ae_enc_freeze: false\n",
      "reduced_mat_ae_dec_freeze: true\n",
      "target_dec_freeze: true\n",
      "best_mat_ae_fold: 2\n",
      "best_reduced_mat_ae_fold: 4\n",
      "synth_exp: false\n",
      "multi_gpu: true\n",
      "num_epochs: 500\n",
      "batch_size: 256\n",
      "n_runs: 1\n",
      "lr: 0.001\n",
      "weight_decay: 0.0001\n",
      "dropout_rate: 0\n",
      "scheduler_patience: 20\n",
      "test_ratio: 0.3\n",
      "train_ratio:\n",
      "- 1.0\n",
      "log_gradients: true\n",
      "clip_grad: true\n",
      "external_test_mode: false\n",
      "test_scanners:\n",
      "- GE MEDICAL SYSTEMS_DISCOVERY MR750\n",
      "- Philips Medical Systems_Achieva dStream\n",
      "- Philips Medical Systems_Ingenia\n",
      "SupCon_kernel: rbf\n",
      "SupConLoss_on_mat: false\n",
      "pft_base_temperature: 0.07\n",
      "pft_temperature: 0.07\n",
      "pft_sigma: 0.2\n",
      "pft_reg_term: 0.01\n",
      "ptt_base_temperature: 0.07\n",
      "ptt_temperature: 0.07\n",
      "ptt_sigma: 10\n",
      "ptt_reg_term: 0.01\n",
      "feature_autoencoder_crit: Norm\n",
      "joint_embedding_crit: cosine\n",
      "target_decoding_crit: MSE\n",
      "huber_delta: 10\n",
      "augmentation: None\n",
      "mat_threshold: 0\n",
      "dataset_path: /well/margulies/users/cpy397/contrastive-learning/data/hcp_kong_400parcels.nc\n",
      "targets:\n",
      "- PicVocab_AgeAdj\n",
      "standardize_targets: false\n",
      "work_dir: /well/margulies/users/cpy397/contrastive-learning\n",
      "reconstructed_dir: recon_mat\n",
      "embedding_dir: embeddings\n",
      "model_weight_dir: saved_models\n",
      "output_dir: /well/margulies/users/cpy397/contrastive-learning/results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with initialize(version_base=None, config_path=\".\"):\n",
    "    cfg = compose(config_name='jupyter_main_model_config.yaml')\n",
    "    print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Size:   0%|                                      | 0/1 [00:00<?, ?it/s]\n",
      "Model Run:   0%|                                          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda, ratio 1.0\n",
      "Loading test indices from the pretraining experiment...\n",
      "Start training...\n",
      "Loading pretrained MatrixAutoencoder...\n",
      "Freezing weights for mat encoding...\n",
      "Freezing weights for mat decoding...\n",
      "Freezing weights for reduced mat decoding...\n",
      "Freezing TargetDecoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/slurm-30202976/ipykernel_1254606/2307275210.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f\"{cfg.output_dir}/{cfg.pretrained_mat_ae_exp}/saved_models/autoencoder_weights_fold{cfg.best_mat_ae_fold}.pth\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:   0%|                                           | 0/500 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   0%| | 0/500 [00:01<?, ?it/s, Epoch 0 | Loss 0.11 | val Target MAPE 99.\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   0%| | 1/500 [00:01<11:35,  1.39s/it, Epoch 0 | Loss 0.11 | val Target \u001b[A\u001b[A\n",
      "\n",
      "Epochs:   0%| | 1/500 [00:02<11:35,  1.39s/it, Epoch 1 | Loss 0.10 | val Target \u001b[A\u001b[A\n",
      "\n",
      "Epochs:   0%| | 2/500 [00:02<10:14,  1.23s/it, Epoch 1 | Loss 0.10 | val Target \u001b[A\u001b[A\n",
      "\n",
      "Epochs:   0%| | 2/500 [00:03<10:14,  1.23s/it, Epoch 2 | Loss 0.08 | val Target \u001b[A\u001b[A\n",
      "\n",
      "Epochs:   1%| | 3/500 [00:03<09:45,  1.18s/it, Epoch 2 | Loss 0.08 | val Target \u001b[A\u001b[A\n",
      "\n",
      "Epochs:   1%| | 3/500 [00:04<09:45,  1.18s/it, Epoch 3 | Loss 0.06 | val Target \u001b[A\u001b[A\n",
      "\n",
      "Epochs:   1%| | 4/500 [00:04<09:30,  1.15s/it, Epoch 3 | Loss 0.06 | val Target \u001b[A\u001b[A\n",
      "\n",
      "Epochs:   1%| | 4/500 [00:05<09:30,  1.15s/it, Epoch 4 | Loss 0.05 | val Target \u001b[A\u001b[A\n",
      "\n",
      "Epochs:   1%| | 5/500 [00:05<09:21,  1.13s/it, Epoch 4 | Loss 0.05 | val Target \u001b[A\u001b[A\n",
      "\n",
      "Epochs:   1%| | 5/500 [00:06<09:21,  1.13s/it, Epoch 5 | Loss 0.05 | val Target \u001b[A\u001b[A\n",
      "\n",
      "Epochs:   1%| | 6/500 [00:06<09:19,  1.13s/it, Epoch 5 | Loss 0.05 | val Target \u001b[A\u001b[A\n",
      "\n",
      "Epochs:   1%| | 6/500 [00:08<09:19,  1.13s/it, Epoch 6 | Loss 0.05 | val Target \u001b[A\u001b[A\n",
      "\n",
      "Epochs:   1%| | 7/500 [00:08<09:19,  1.14s/it, Epoch 6 | Loss 0.05 | val Target \u001b[A\u001b[A\n",
      "\n",
      "Epochs:   1%| | 7/500 [00:09<09:19,  1.14s/it, Epoch 7 | Loss 0.05 | val Target \u001b[A\u001b[A\n",
      "\n",
      "Epochs:   2%| | 8/500 [00:09<09:14,  1.13s/it, Epoch 7 | Loss 0.05 | val Target \u001b[A\u001b[A\n",
      "\n",
      "Epochs:   2%| | 8/500 [00:10<09:14,  1.13s/it, Epoch 8 | Loss 0.05 | val Target \u001b[A\u001b[A\n",
      "\n",
      "Epochs:   2%| | 9/500 [00:10<09:09,  1.12s/it, Epoch 8 | Loss 0.05 | val Target \u001b[A\u001b[A\n",
      "\n",
      "Epochs:   2%| | 9/500 [00:11<09:09,  1.12s/it, Epoch 9 | Loss 0.05 | val Target \u001b[A\u001b[A\n",
      "\n",
      "Epochs:   2%| | 10/500 [00:11<09:05,  1.11s/it, Epoch 9 | Loss 0.05 | val Target\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   2%| | 10/500 [00:12<09:05,  1.11s/it, Epoch 10 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   2%| | 11/500 [00:12<09:04,  1.11s/it, Epoch 10 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   2%| | 11/500 [00:13<09:04,  1.11s/it, Epoch 11 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   2%| | 12/500 [00:13<09:02,  1.11s/it, Epoch 11 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   2%| | 12/500 [00:14<09:02,  1.11s/it, Epoch 12 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   3%| | 13/500 [00:14<09:03,  1.12s/it, Epoch 12 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   3%| | 13/500 [00:15<09:03,  1.12s/it, Epoch 13 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   3%| | 14/500 [00:15<09:03,  1.12s/it, Epoch 13 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   3%| | 14/500 [00:17<09:03,  1.12s/it, Epoch 14 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   3%| | 15/500 [00:17<09:03,  1.12s/it, Epoch 14 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   3%| | 15/500 [00:18<09:03,  1.12s/it, Epoch 15 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   3%| | 16/500 [00:18<09:00,  1.12s/it, Epoch 15 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   3%| | 16/500 [00:19<09:00,  1.12s/it, Epoch 16 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   3%| | 17/500 [00:19<08:56,  1.11s/it, Epoch 16 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   3%| | 17/500 [00:20<08:56,  1.11s/it, Epoch 17 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   4%| | 18/500 [00:20<08:54,  1.11s/it, Epoch 17 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   4%| | 18/500 [00:21<08:54,  1.11s/it, Epoch 18 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   4%| | 19/500 [00:21<08:53,  1.11s/it, Epoch 18 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   4%| | 19/500 [00:22<08:53,  1.11s/it, Epoch 19 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   4%| | 20/500 [00:22<08:53,  1.11s/it, Epoch 19 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   4%| | 20/500 [00:23<08:53,  1.11s/it, Epoch 20 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   4%| | 21/500 [00:23<08:56,  1.12s/it, Epoch 20 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   4%| | 21/500 [00:24<08:56,  1.12s/it, Epoch 21 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   4%| | 22/500 [00:24<08:49,  1.11s/it, Epoch 21 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   4%| | 22/500 [00:25<08:49,  1.11s/it, Epoch 22 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   5%| | 23/500 [00:25<08:54,  1.12s/it, Epoch 22 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   5%| | 23/500 [00:27<08:54,  1.12s/it, Epoch 23 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   5%| | 24/500 [00:27<08:55,  1.12s/it, Epoch 23 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   5%| | 24/500 [00:28<08:55,  1.12s/it, Epoch 24 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   5%| | 25/500 [00:28<08:50,  1.12s/it, Epoch 24 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   5%| | 25/500 [00:29<08:50,  1.12s/it, Epoch 25 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   5%| | 26/500 [00:29<08:44,  1.11s/it, Epoch 25 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   5%| | 26/500 [00:30<08:44,  1.11s/it, Epoch 26 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   5%| | 27/500 [00:30<08:45,  1.11s/it, Epoch 26 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   5%| | 27/500 [00:31<08:45,  1.11s/it, Epoch 27 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   6%| | 28/500 [00:31<08:43,  1.11s/it, Epoch 27 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   6%| | 28/500 [00:32<08:43,  1.11s/it, Epoch 28 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   6%| | 29/500 [00:32<08:41,  1.11s/it, Epoch 28 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   6%| | 29/500 [00:33<08:41,  1.11s/it, Epoch 29 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   6%| | 30/500 [00:33<08:39,  1.10s/it, Epoch 29 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   6%| | 30/500 [00:34<08:39,  1.10s/it, Epoch 30 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   6%| | 31/500 [00:34<08:36,  1.10s/it, Epoch 30 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   6%| | 31/500 [00:35<08:36,  1.10s/it, Epoch 31 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   6%| | 32/500 [00:35<08:36,  1.10s/it, Epoch 31 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   6%| | 32/500 [00:36<08:36,  1.10s/it, Epoch 32 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   7%| | 33/500 [00:36<08:34,  1.10s/it, Epoch 32 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   7%| | 33/500 [00:38<08:34,  1.10s/it, Epoch 33 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   7%| | 34/500 [00:38<08:32,  1.10s/it, Epoch 33 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   7%| | 34/500 [00:39<08:32,  1.10s/it, Epoch 34 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   7%| | 35/500 [00:39<08:29,  1.09s/it, Epoch 34 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   7%| | 35/500 [00:40<08:29,  1.09s/it, Epoch 35 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   7%| | 36/500 [00:40<08:29,  1.10s/it, Epoch 35 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   7%| | 36/500 [00:41<08:29,  1.10s/it, Epoch 36 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   7%| | 37/500 [00:41<08:27,  1.10s/it, Epoch 36 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   7%| | 37/500 [00:42<08:27,  1.10s/it, Epoch 37 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   8%| | 38/500 [00:42<08:25,  1.09s/it, Epoch 37 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   8%| | 38/500 [00:43<08:25,  1.09s/it, Epoch 38 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   8%| | 39/500 [00:43<08:24,  1.09s/it, Epoch 38 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   8%| | 39/500 [00:44<08:24,  1.09s/it, Epoch 39 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   8%| | 40/500 [00:44<08:27,  1.10s/it, Epoch 39 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   8%| | 40/500 [00:45<08:27,  1.10s/it, Epoch 40 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   8%| | 41/500 [00:45<08:26,  1.10s/it, Epoch 40 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   8%| | 41/500 [00:46<08:26,  1.10s/it, Epoch 41 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   8%| | 42/500 [00:46<08:28,  1.11s/it, Epoch 41 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   8%| | 42/500 [00:48<08:28,  1.11s/it, Epoch 42 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   9%| | 43/500 [00:48<08:28,  1.11s/it, Epoch 42 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   9%| | 43/500 [00:49<08:28,  1.11s/it, Epoch 43 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   9%| | 44/500 [00:49<08:25,  1.11s/it, Epoch 43 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   9%| | 44/500 [00:50<08:25,  1.11s/it, Epoch 44 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   9%| | 45/500 [00:50<08:25,  1.11s/it, Epoch 44 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   9%| | 45/500 [00:51<08:25,  1.11s/it, Epoch 45 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   9%| | 46/500 [00:51<08:23,  1.11s/it, Epoch 45 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   9%| | 46/500 [00:52<08:23,  1.11s/it, Epoch 46 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   9%| | 47/500 [00:52<08:21,  1.11s/it, Epoch 46 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   9%| | 47/500 [00:53<08:21,  1.11s/it, Epoch 47 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  10%| | 48/500 [00:53<08:18,  1.10s/it, Epoch 47 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  10%| | 48/500 [00:54<08:18,  1.10s/it, Epoch 48 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  10%| | 49/500 [00:54<08:16,  1.10s/it, Epoch 48 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  10%| | 49/500 [00:55<08:16,  1.10s/it, Epoch 49 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  10%| | 50/500 [00:55<08:15,  1.10s/it, Epoch 49 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  10%| | 50/500 [00:56<08:15,  1.10s/it, Epoch 50 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  10%| | 51/500 [00:56<08:16,  1.11s/it, Epoch 50 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  10%| | 51/500 [00:57<08:16,  1.11s/it, Epoch 51 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  10%| | 52/500 [00:57<08:17,  1.11s/it, Epoch 51 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  10%| | 52/500 [00:59<08:17,  1.11s/it, Epoch 52 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  11%| | 53/500 [00:59<08:17,  1.11s/it, Epoch 52 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  11%| | 53/500 [01:00<08:17,  1.11s/it, Epoch 53 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  11%| | 54/500 [01:00<08:18,  1.12s/it, Epoch 53 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  11%| | 54/500 [01:01<08:18,  1.12s/it, Epoch 54 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  11%| | 55/500 [01:01<08:16,  1.12s/it, Epoch 54 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  11%| | 55/500 [01:02<08:16,  1.12s/it, Epoch 55 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  11%| | 56/500 [01:02<08:13,  1.11s/it, Epoch 55 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  11%| | 56/500 [01:03<08:13,  1.11s/it, Epoch 56 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  11%| | 57/500 [01:03<08:09,  1.11s/it, Epoch 56 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  11%| | 57/500 [01:04<08:09,  1.11s/it, Epoch 57 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  12%| | 58/500 [01:04<08:07,  1.10s/it, Epoch 57 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  12%| | 58/500 [01:05<08:07,  1.10s/it, Epoch 58 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  12%| | 59/500 [01:05<08:04,  1.10s/it, Epoch 58 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  12%| | 59/500 [01:06<08:04,  1.10s/it, Epoch 59 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  12%| | 60/500 [01:06<08:02,  1.10s/it, Epoch 59 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  12%| | 60/500 [01:07<08:02,  1.10s/it, Epoch 60 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  12%| | 61/500 [01:07<08:03,  1.10s/it, Epoch 60 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  12%| | 61/500 [01:08<08:03,  1.10s/it, Epoch 61 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  12%| | 62/500 [01:09<08:03,  1.10s/it, Epoch 61 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  12%| | 62/500 [01:10<08:03,  1.10s/it, Epoch 62 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "Epochs:  13%|▏| 63/500 [01:10<08:01,  1.10s/it, Epoch 62 | Loss 0.05 | val Targe\u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇██</td></tr><tr><td>Gradient Norm/reduced_matrix_ae.reduced_mat_to_embed.0.bias</td><td>█▃▃▁▃▁▁▂▁▁▁▁▂▂▁▁▁▁▁▂▁▁▂▃▁▁▁▁▁▁▃▁▁▁▁▁▃▁▁▂</td></tr><tr><td>Gradient Norm/reduced_matrix_ae.reduced_mat_to_embed.0.weight</td><td>█▃▃▁▂▂▁▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁</td></tr><tr><td>Gradient Norm/reduced_matrix_ae.reduced_mat_to_embed.1.bias</td><td>▆▇▆██▅▃▂▃▂▂▂▁▄▃▂▃▂▃▂▃▁▃▂▂▃▃▂▂▃▂▂▅▃▃▆▂▅▂▁</td></tr><tr><td>Gradient Norm/reduced_matrix_ae.reduced_mat_to_embed.1.weight</td><td>▅▇██▄▅▅▂▂▂▄▃▃▃▃▃▂▁▃▃▃▁▁▃▃▃▂▃▄▁▃▂▃▄▂▃▂▃▁▁</td></tr><tr><td>Gradient Norm/reduced_matrix_ae.reduced_mat_to_embed.12.bias</td><td>▄████▅▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Gradient Norm/reduced_matrix_ae.reduced_mat_to_embed.12.weight</td><td>▆█▇█▅▄▄▅▅▃▃▃▃▅▅▃▅▄▁▂▃▄▂▃▂▂▃▃▂▄▃▂▃▁▂▂▆▃▁▁</td></tr><tr><td>Gradient Norm/reduced_matrix_ae.reduced_mat_to_embed.4.bias</td><td>▇██▇▇▃▄▅▃▃▃▅▃▂▃▂▃▃▄▂▂▂▃▃▂▃▂▁▃▂▃▂▁▃▂▂▁▂▂▃</td></tr><tr><td>Gradient Norm/reduced_matrix_ae.reduced_mat_to_embed.4.weight</td><td>▅▆█▇▇▅▂▂▄▂▂▂▅▁▁▃▂▂▃▂▂▂▁▂▂▂▃▂▁▂▁▁▂▂▂▁▂▂▂▂</td></tr><tr><td>Gradient Norm/reduced_matrix_ae.reduced_mat_to_embed.5.bias</td><td>▄██▆▇▄▄▄▃▄▆▄▄▃▃▄▂▂▄▄▁▃▃▁▃▂▂▃▂▅▄▃▁▄▂▁▇▆▂▁</td></tr><tr><td>Gradient Norm/reduced_matrix_ae.reduced_mat_to_embed.5.weight</td><td>▂▃▃▄▆▇█▆▃▃▃▁▄▅▂▅▃▁▂▇▄▃▃▂▃▂▄▃▅▂▂▃▂▃▄▄▁▂▂▄</td></tr><tr><td>Gradient Norm/reduced_matrix_ae.reduced_mat_to_embed.8.bias</td><td>▇▆█▆▇▅▃▂▂▂▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▂▁▁</td></tr><tr><td>Gradient Norm/reduced_matrix_ae.reduced_mat_to_embed.8.weight</td><td>▅█▇▇▆▃▅▄▆▂▃▄▂▃▅▅▃▆▄▂▃▃▇▄▂▂▄▃▅▂▄▂▄▁▄▃▂▃▁▇</td></tr><tr><td>Gradient Norm/reduced_matrix_ae.reduced_mat_to_embed.9.bias</td><td>██▇▆▄▁▁▂▁▂▁▁▁▁▁▁▁▁▁▂▁▂▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Gradient Norm/reduced_matrix_ae.reduced_mat_to_embed.9.weight</td><td>███▇▇▇▃▄▇▂▃▂▁▂▄▁▄▂▃▁▂▃▃▄▃▆▄▆▃▃▅▄▂▃▂▃▃▁▂▃</td></tr><tr><td>Run</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Target Corr/val</td><td>▅▁▂▄▇▅▆▂▅▇▇▇▄▆▆█▄▆█▅▄▆█▅▆▇▇▄█▆▅▆▇█▇▄▆█▇▇</td></tr><tr><td>Target MAPE/val</td><td>▅█▇▃▃▁▃▂▂▂▃▃▂▃▂▃▂▃▃▂▃▃▂▁▃▂▂▃▂▃▃▃▂▂▂▂▃▃▂▂</td></tr><tr><td>Train ratio</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>kernel_embedded_network_loss</td><td>█▂▃▂▁▁▁▁▃▃▃▁▃▁▁▁▁▁▂▁▁▁▂▁▁▁▁▂▁▁▂▁▂▁▁▁▂▁▁▃</td></tr><tr><td>kernel_embedded_target_loss</td><td>▇█▄▂▂▁▂▁▂▁▁▂▂▁▁▁▁▂▁▁▂▁▂▁▂▁▂▁▂▁▂▂▁▁▁▂▁▁▁▂</td></tr><tr><td>total_loss</td><td>█▄▁▂▁▁▂▁▁▁▂▁▂▁▁▁▁▁▂▁▁▂▁▁▁▁▁▂▂▁▁▁▂▁▁▂▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>63</td></tr><tr><td>Gradient Norm/reduced_matrix_ae.reduced_mat_to_embed.0.bias</td><td>0.0</td></tr><tr><td>Gradient Norm/reduced_matrix_ae.reduced_mat_to_embed.0.weight</td><td>0.13305</td></tr><tr><td>Gradient Norm/reduced_matrix_ae.reduced_mat_to_embed.1.bias</td><td>0.00256</td></tr><tr><td>Gradient Norm/reduced_matrix_ae.reduced_mat_to_embed.1.weight</td><td>0.00637</td></tr><tr><td>Gradient Norm/reduced_matrix_ae.reduced_mat_to_embed.12.bias</td><td>0.01131</td></tr><tr><td>Gradient Norm/reduced_matrix_ae.reduced_mat_to_embed.12.weight</td><td>0.24327</td></tr><tr><td>Gradient Norm/reduced_matrix_ae.reduced_mat_to_embed.4.bias</td><td>0.0</td></tr><tr><td>Gradient Norm/reduced_matrix_ae.reduced_mat_to_embed.4.weight</td><td>0.10758</td></tr><tr><td>Gradient Norm/reduced_matrix_ae.reduced_mat_to_embed.5.bias</td><td>0.00494</td></tr><tr><td>Gradient Norm/reduced_matrix_ae.reduced_mat_to_embed.5.weight</td><td>0.0141</td></tr><tr><td>Gradient Norm/reduced_matrix_ae.reduced_mat_to_embed.8.bias</td><td>0.0</td></tr><tr><td>Gradient Norm/reduced_matrix_ae.reduced_mat_to_embed.8.weight</td><td>0.21355</td></tr><tr><td>Gradient Norm/reduced_matrix_ae.reduced_mat_to_embed.9.bias</td><td>0.01422</td></tr><tr><td>Gradient Norm/reduced_matrix_ae.reduced_mat_to_embed.9.weight</td><td>0.0195</td></tr><tr><td>Run</td><td>0</td></tr><tr><td>Target Corr/val</td><td>0.30053</td></tr><tr><td>Target MAPE/val</td><td>99.88849</td></tr><tr><td>Train ratio</td><td>1</td></tr><tr><td>kernel_embedded_network_loss</td><td>0.02346</td></tr><tr><td>kernel_embedded_target_loss</td><td>0.02655</td></tr><tr><td>total_loss</td><td>0.05001</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync /well/margulies/users/cpy397/contrastive-learning/results/wandb/offline-run-20250220_074411-085s6mkn<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/well/margulies/users/cpy397/contrastive-learning/results/wandb/offline-run-20250220_074411-085s6mkn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs3/well/margulies/users/cpy397/contrastive-learning/contrastive_phenotypes/src/ContModeling/viz_func.py:108: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  correlations[i // cols, i % cols] = spearmanr(flat_true[:, i], flat_recon[:, i])[0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Run</td><td>▁</td></tr><tr><td>Test | Target Corr/val</td><td>▁</td></tr><tr><td>Test | Target MAPE/val</td><td>▁</td></tr><tr><td>Test | Train ratio</td><td>▁</td></tr><tr><td>Train ratio</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Run</td><td>0</td></tr><tr><td>Test | Target Corr/val</td><td>0.30394</td></tr><tr><td>Test | Target MAPE/val</td><td>99.88891</td></tr><tr><td>Test | Train ratio</td><td>1</td></tr><tr><td>Train ratio</td><td>1</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync /well/margulies/users/cpy397/contrastive-learning/results/wandb/offline-run-20250220_074526-z8jxfuuf<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/well/margulies/users/cpy397/contrastive-learning/results/wandb/offline-run-20250220_074526-z8jxfuuf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Run: 100%|█████████████████████████████████| 1/1 [04:07<00:00, 247.01s/it]\u001b[A\n",
      "Training Size: 100%|█████████████████████████████| 1/1 [04:07<00:00, 247.01s/it]\n"
     ]
    }
   ],
   "source": [
    "def main(cfg=cfg):\n",
    "\n",
    "    results_dir = os.path.join(cfg.output_dir, cfg.experiment_name)\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    random_state = np.random.RandomState(seed=42)\n",
    "\n",
    "    dataset_path = cfg.dataset_path\n",
    "\n",
    "    if isinstance(cfg.targets, str):\n",
    "        \n",
    "        targets =[cfg.targets]\n",
    "    else:\n",
    "        targets = list(cfg.targets)\n",
    "        \n",
    "    test_ratio = cfg.test_ratio\n",
    "\n",
    "    dataset = MatData(dataset_path, targets, synth_exp = cfg.synth_exp, threshold=cfg.mat_threshold)\n",
    "    n_sub = len(dataset)\n",
    "    test_size = int(test_ratio * n_sub)\n",
    "    indices = np.arange(n_sub)\n",
    "    n_runs = cfg.n_runs\n",
    "    multi_gpu = False\n",
    "    train_ratios = list(cfg.train_ratio)\n",
    "    \n",
    "    multi_gpu = False\n",
    "    if multi_gpu:\n",
    "        print(\"Using multi-gpu\")\n",
    "        log_folder = Path(\"logs\")\n",
    "        executor = submitit.AutoExecutor(folder=str(log_folder / \"%j\"))\n",
    "        executor.update_parameters(\n",
    "            timeout_min=120,\n",
    "            slurm_partition=\"gpu_short\",\n",
    "            gpus_per_node=1,\n",
    "            tasks_per_node=1,\n",
    "            nodes=1\n",
    "            #slurm_constraint=\"v100-32g\",\n",
    "        )\n",
    "        run_jobs = []\n",
    "\n",
    "        with executor.batch():\n",
    "            for train_ratio in tqdm(train_ratios, desc=\"Training Size\"):\n",
    "                train_size = int(n_sub * (1 - test_ratio) * train_ratio)\n",
    "                run_size = test_size + train_size\n",
    "                for run in tqdm(range(n_runs)):\n",
    "                    run_model = ModelRun()\n",
    "                    job = executor.submit(run_model, train, test_size, indices, train_ratio, run_size, run, dataset, cfg, random_state=random_state, device=None)\n",
    "                    run_jobs.append(job)\n",
    "\n",
    "        async def get_result(run_jobs):\n",
    "            run_results = []\n",
    "            for aws in tqdm(asyncio.as_completed([j.awaitable().result() for j in run_jobs]), total=len(run_jobs)):\n",
    "                res = await aws\n",
    "                run_results.append(res)\n",
    "            return run_results\n",
    "        run_results = asyncio.run(get_result(run_jobs))\n",
    "\n",
    "    else:\n",
    "        run_results = []\n",
    "        for train_ratio in tqdm(train_ratios, desc=\"Training Size\"):\n",
    "            train_size = int(n_sub * (1 - test_ratio) * train_ratio)\n",
    "            run_size = test_size + train_size\n",
    "            for run in tqdm(range(n_runs), desc=\"Model Run\"):\n",
    "                run_model = ModelRun()\n",
    "                job = run_model(train, test_size, indices, train_ratio, run_size, run, dataset, cfg, random_state=random_state, device=None)\n",
    "                run_results.append(job)\n",
    "\n",
    "    losses, predictions, embeddings = zip(*run_results)\n",
    "\n",
    "    prediction_metrics = predictions[0]\n",
    "    for prediction in predictions[1:]:\n",
    "        prediction_metrics.update(prediction)\n",
    "\n",
    "    pred_results = []\n",
    "    for k, v in prediction_metrics.items():\n",
    "        true_targets, predicted_targets, indices = v\n",
    "        \n",
    "        true_targets_dict = {\"train_ratio\": [k[0]] * len(true_targets),\n",
    "                             \"model_run\":[k[1]] * len(true_targets),\n",
    "                             \"dataset\":[k[2]] * len(true_targets)\n",
    "                            }\n",
    "        predicted_targets_dict = {\"indices\": indices}\n",
    "        \n",
    "        for i, target in enumerate(targets):\n",
    "            true_targets_dict[target] = true_targets[:, i]\n",
    "            predicted_targets_dict[f\"{target}_pred\"] = predicted_targets[:, i]\n",
    "            \n",
    "            \n",
    "        true_targets = pd.DataFrame(true_targets_dict)\n",
    "        predicted_targets = pd.DataFrame(predicted_targets_dict)\n",
    "        \n",
    "        pred_results.append(pd.concat([true_targets, predicted_targets], axis = 1))\n",
    "    pred_results = pd.concat(pred_results)\n",
    "    pred_results.to_csv(f\"{results_dir}/pred_results.csv\", index=False)\n",
    "\n",
    "    prediction_mape_by_element = []\n",
    "    for k, v in prediction_metrics.items():\n",
    "        true_targets, predicted_targets, indices = v\n",
    "        \n",
    "        mape_by_element = np.abs(true_targets - predicted_targets) / (np.abs(true_targets)+1e-10)\n",
    "        \n",
    "        for i, mape in enumerate(mape_by_element):\n",
    "            prediction_mape_by_element.append(\n",
    "                {\n",
    "                    'train_ratio': k[0],\n",
    "                    'model_run': k[1],\n",
    "                    'dataset': k[2],\n",
    "                    'mape': mape\n",
    "                }\n",
    "            )\n",
    "\n",
    "    df = pd.DataFrame(prediction_mape_by_element)\n",
    "    df = pd.concat([df.drop('mape', axis=1), df['mape'].apply(pd.Series)], axis=1)\n",
    "    df.columns = ['train_ratio', 'model_run', 'dataset'] + targets\n",
    "    df= df.groupby(['train_ratio', 'model_run', 'dataset']).agg('mean').reset_index()\n",
    "    df.to_csv(f\"{results_dir}/mape.csv\", index = False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
