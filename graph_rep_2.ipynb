{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mind/mrenaudi/.local/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import asyncio\n",
    "import submitit\n",
    "import pickle\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "from nilearn.connectome import sym_matrix_to_vec\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    ")\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, TensorDataset\n",
    "from tqdm.auto import tqdm\n",
    "from augmentations import augs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "torch.cuda.empty_cache()\n",
    "multi_gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0\n",
    "AUGMENTATION = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_feat = \"/data/parietal/store/work/dwassermann/data/victoria_mat_age/matrices.npy\"\n",
    "path_target = \"/data/parietal/store/work/dwassermann/data/victoria_mat_age/data_mat_age_demian/participants.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim_feat, input_dim_target, hidden_dim_feats, output_dim, dropout_rate, lr, weight_decay):\n",
    "        \n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        self.feat_gcn = Seq_g('x, edge_index', [\n",
    "            (GCNConv(input_dim_feat, hidden_dim_feats), 'x, edge_index -> x'),\n",
    "            nn.ReLU(),\n",
    "            (GCNConv(hidden_dim_feats, hidden_dim_feats), 'x, edge_index -> x'),\n",
    "            nn.ReLU(),\n",
    "            (GCNConv(hidden_dim_feats, output_dim), 'x, edge_index -> x')\n",
    "        ])\n",
    "        #self.init_weights(self.feat_gcn)\n",
    "        \n",
    "        self.target_mlp = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_dim_target),\n",
    "            nn.Linear(input_dim_target, hidden_dim_feats),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(hidden_dim_feats, output_dim)\n",
    "        )\n",
    "        #self.init_weights(self.target_mlp)\n",
    "        \n",
    "        self.decode_target = nn.Sequential(\n",
    "            nn.Linear(output_dim, hidden_dim_feats),\n",
    "            nn.BatchNorm1d(hidden_dim_feats),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim_feats, input_dim_target)\n",
    "        )\n",
    "        #self.init_weights(self.decode_target)\n",
    "    \n",
    "    \n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "            elif isinstance(m, GCNConv):\n",
    "            # GCNConv initializes its weights during initialization\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "                    \n",
    "\n",
    "    \n",
    "    def transform_feat(self, x, edge_index):\n",
    "        features = self.feat_gcn(x, edge_index)\n",
    "        pooling = global_mean_pool(features, batch=None)\n",
    "        pooling = nn.functional.normalize(features, p=2, dim=1)\n",
    "        return pooling\n",
    "    \n",
    "    def transform_targets(self, y):\n",
    "        targets = self.target_mlp(y)\n",
    "        targets = nn.functional.normalize(targets, p=2, dim=1)\n",
    "        return targets\n",
    " \n",
    "    def decode_targets(self, embedding):\n",
    "        return self.decode_target(embedding)\n",
    "\n",
    "        \n",
    "    def forward(self, x, y, edge_index):\n",
    "        x_embedding = self.transform_feat(x, edge_index)\n",
    "        y_embedding = self.transform_targets(y)\n",
    "        return x_embedding, y_embedding\n",
    "    \n",
    "    def initialize_optimizer(self, lr, weight_decay):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data.dataset import Dataset as graph_dataset\n",
    "\n",
    "class GraphData(graph_dataset):\n",
    "    def __init__(self, matrices, target, target_name):\n",
    "        self.features = np.load(path_feat).astype(np.float32)[:15]\n",
    "        self.targets = torch.tensor(\n",
    "            np.expand_dims(\n",
    "                pd.read_csv(path_target)[target_name].values, axis=1\n",
    "            )[:15],\n",
    "            dtype=torch.float32\n",
    "        )        \n",
    "        \n",
    "        self._indices = range(len(self.features))\n",
    "        #self.edge_indices = torch.tensor(np.argwhere(self.features != 0), dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features = self.features[idx]\n",
    "        targets = self.targets[idx]   \n",
    "        edge_indices = torch.tensor(np.argwhere(self.features != 0), dtype=torch.long)\n",
    "        \n",
    "        \n",
    "        return features, targets, edge_indices\n",
    "    \n",
    "    def indices(self):\n",
    "        return self._indices\n",
    "    \n",
    "    #def index_select(self, idx):\n",
    "        #return data.index_select(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelizedSupCon(nn.Module):\n",
    "    \"\"\"Supervised contrastive loss: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\n",
    "    Based on: https://github.com/HobbitLong/SupContrast\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        method: str,\n",
    "        temperature: float = 0.07,\n",
    "        contrast_mode: str = \"all\",\n",
    "        base_temperature: float = 0.07,\n",
    "        krnl_sigma: float = 1.0,\n",
    "        kernel: callable = None,\n",
    "        delta_reduction: str = \"sum\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "        self.method = method\n",
    "        self.kernel = kernel\n",
    "        self.krnl_sigma = krnl_sigma\n",
    "        self.delta_reduction = delta_reduction\n",
    "\n",
    "        if kernel is not None and method == \"supcon\":\n",
    "            raise ValueError(\"Kernel must be none if method=supcon\")\n",
    "\n",
    "        if kernel is None and method != \"supcon\":\n",
    "            raise ValueError(\"Kernel must not be none if method != supcon\")\n",
    "\n",
    "        if delta_reduction not in [\"mean\", \"sum\"]:\n",
    "            raise ValueError(f\"Invalid reduction {delta_reduction}\")\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"{self.__class__.__name__} \"\n",
    "            f\"(t={self.temperature}, \"\n",
    "            f\"method={self.method}, \"\n",
    "            f\"kernel={self.kernel is not None}, \"\n",
    "            f\"delta_reduction={self.delta_reduction})\"\n",
    "        )\n",
    "\n",
    "    def forward(self, features, labels=None):\n",
    "        \"\"\"Compute loss for model. If `labels` is None,\n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, n_features].\n",
    "                input has to be rearranged to [bsz, n_views, n_features] and labels [bsz],\n",
    "            labels: ground truth of shape [bsz].\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "        device = features.device\n",
    "\n",
    "        if len(features.shape) != 3:\n",
    "            raise ValueError(\n",
    "                \"`features` needs to be [bsz, n_views, n_feats],\"\n",
    "                \"3 dimensions are required\"\n",
    "            )\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        n_views = features.shape[1]\n",
    "        if labels is None:\n",
    "            mask = torch.eye(batch_size, device=device)\n",
    "\n",
    "        else:\n",
    "            labels = labels.view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError(\"Num of labels does not match num of features\")\n",
    "\n",
    "            if self.kernel is None:\n",
    "                mask = torch.eq(labels, labels.T)\n",
    "            else:\n",
    "                mask = self.kernel(labels, krnl_sigma=self.krnl_sigma)\n",
    "\n",
    "        view_count = features.shape[1]\n",
    "        features = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == \"one\":\n",
    "            features = features[:, 0]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == \"all\":\n",
    "            features = features\n",
    "            anchor_count = view_count\n",
    "        else:\n",
    "            raise ValueError(\"Unknown mode: {}\".format(self.contrast_mode))\n",
    "\n",
    "        # Tile mask\n",
    "        mask = mask.repeat(anchor_count, view_count)\n",
    "        # Inverse of torch-eye to remove self-contrast (diagonal)\n",
    "        inv_diagonal = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * n_views, device=device).view(-1, 1),\n",
    "            0,\n",
    "        )\n",
    "\n",
    "        # compute similarity\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(features, features.T), self.temperature\n",
    "        )\n",
    "\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        alignment = logits\n",
    "\n",
    "        # base case is:\n",
    "        # - supcon if kernel = none\n",
    "        # - y-aware is kernel != none\n",
    "        uniformity = torch.exp(logits) * inv_diagonal\n",
    "\n",
    "        if self.method == \"threshold\":\n",
    "            repeated = mask.unsqueeze(-1).repeat(\n",
    "                1, 1, mask.shape[0]\n",
    "            )  # repeat kernel mask\n",
    "\n",
    "            delta = (mask[:, None].T - repeated.T).transpose(\n",
    "                1, 2\n",
    "            )  # compute the difference w_k - w_j for every k,j\n",
    "            delta = (delta > 0.0).float()\n",
    "\n",
    "            # for each z_i, repel only samples j s.t. K(z_i, z_j) < K(z_i, z_k)\n",
    "            uniformity = uniformity.unsqueeze(-1).repeat(1, 1, mask.shape[0])\n",
    "\n",
    "            if self.delta_reduction == \"mean\":\n",
    "                uniformity = (uniformity * delta).mean(-1)\n",
    "            else:\n",
    "                uniformity = (uniformity * delta).sum(-1)\n",
    "\n",
    "        elif self.method == \"expw\":\n",
    "            # exp weight e^(s_j(1-w_j))\n",
    "            uniformity = torch.exp(logits * (1 - mask)) * inv_diagonal\n",
    "\n",
    "        uniformity = torch.log(uniformity.sum(1, keepdim=True))\n",
    "\n",
    "        # positive mask contains the anchor-positive pairs\n",
    "        # excluding <self,self> on the diagonal\n",
    "        positive_mask = mask * inv_diagonal\n",
    "\n",
    "        log_prob = (\n",
    "            alignment - uniformity\n",
    "        )  # log(alignment/uniformity) = log(alignment) - log(uniformity)\n",
    "        log_prob = (positive_mask * log_prob).sum(1) / positive_mask.sum(\n",
    "            1\n",
    "        )  # compute mean of log-likelihood over positive\n",
    "\n",
    "        # loss\n",
    "        loss = -(self.temperature / self.base_temperature) * log_prob\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x, krnl_sigma):\n",
    "    x = x - x.T\n",
    "    return torch.exp(-(x**2) / (2 * (krnl_sigma**2))) / (math.sqrt(2 * torch.pi))\n",
    "\n",
    "def gaussian_kernel_on_similarity_matrix(x, krnl_sigma):\n",
    "    return torch.exp(-(x**2) / (2 * (krnl_sigma**2))) / (math.sqrt(2 * torch.pi))\n",
    "\n",
    "\n",
    "\n",
    "def gaussian_kernel_original(x, krnl_sigma):\n",
    "    x = x - x.T\n",
    "    return torch.exp(-(x**2) / (2 * (krnl_sigma**2))) / (\n",
    "        math.sqrt(2 * torch.pi) * krnl_sigma\n",
    "    )\n",
    "\n",
    "\n",
    "def cauchy(x, krnl_sigma):\n",
    "    x = x - x.T\n",
    "    return 1.0 / (krnl_sigma * (x**2) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader as graph_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataset, test_dataset, model=None, device=device, kernel=cauchy, num_epochs=100, batch_size=32):\n",
    "    input_dim_feat = 499500\n",
    "    # the rest is arbitrary\n",
    "    hidden_dim_feat = 1000\n",
    "    input_dim_target = 1\n",
    "    output_dim = 2\n",
    "\n",
    "    num_epochs = 100\n",
    "\n",
    "    lr = 0.1  # too low values return nan loss\n",
    "    kernel = cauchy\n",
    "    batch_size = 32  # too low values return nan loss\n",
    "    dropout_rate = 0\n",
    "    weight_decay = 0\n",
    "\n",
    "    train_loader = graph_dataloader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = graph_dataloader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    if model is None:\n",
    "        model = GCN(\n",
    "            input_dim_feat,\n",
    "            input_dim_target,\n",
    "            hidden_dim_feat,\n",
    "            output_dim,\n",
    "            dropout_rate=dropout_rate,\n",
    "            lr = lr,\n",
    "            weight_decay = weight_decay\n",
    "        ).to(device)\n",
    "    \n",
    "    model.init_weights()   \n",
    "    criterion_pft = KernelizedSupCon(\n",
    "    method=\"expw\", temperature=0.03, base_temperature=0.03, kernel=kernel, krnl_sigma=1\n",
    "    )\n",
    "    \n",
    "    criterion_ptt = KernelizedSupCon(\n",
    "        method=\"expw\", temperature=0.03, base_temperature=0.03, kernel=kernel, krnl_sigma=1\n",
    "    )\n",
    "    \n",
    "    optimizer = model.initialize_optimizer(lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.1)\n",
    "\n",
    "    loss_terms = []\n",
    "    validation = []\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    with tqdm(range(num_epochs), desc=\"Epochs\", leave=False) as pbar:\n",
    "        for epoch in pbar:\n",
    "            model.train()\n",
    "            loss_terms_batch = defaultdict(lambda:0)\n",
    "            for features, targets, edge_indices in train_loader:\n",
    "                \n",
    "                bsz = targets.shape[0]\n",
    "                n_views = features.shape[1]\n",
    "                n_feat = features.shape[-1]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                features = features.view(bsz * n_views, n_feat)\n",
    "                features = features.to(device)              \n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                out_feat, out_target = model(features, torch.cat(n_views*[targets], dim=0), edge_indices)\n",
    "                \n",
    "                joint_embedding = nn.functional.mse_loss(out_feat, out_target)\n",
    "                \n",
    "                out_feat = torch.split(out_feat, [bsz]*n_views, dim=0)\n",
    "                out_feat = torch.cat([f.unsqueeze(1) for f in out_feat], dim=1)\n",
    "                kernel_feature = criterion_pft(out_feat, targets)\n",
    "\n",
    "                out_target_decoded = model.decode_target(out_target)\n",
    "                #cosine_target = torch.ones(len(out_target), device=device)                \n",
    "                out_target = torch.split(out_target, [bsz]*n_views, dim=0)\n",
    "                out_target = torch.cat([f.unsqueeze(1) for f in out_target], dim=1)\n",
    "        \n",
    "                kernel_target = criterion_ptt(out_target, targets)\n",
    "                #joint_embedding = 1000 * nn.functional.cosine_embedding_loss(out_feat, out_target, cosine_target)\n",
    "                target_decoding = .1 * nn.functional.mse_loss(torch.cat(n_views*[targets], dim=0), out_target_decoded)\n",
    "\n",
    "                loss = kernel_feature + kernel_target + joint_embedding + target_decoding\n",
    "                loss.backward()\n",
    "                                # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                loss_terms_batch['loss'] += loss.item() / len(train_loader)\n",
    "                loss_terms_batch['kernel_feature'] += kernel_feature.item() / len(train_loader)\n",
    "                loss_terms_batch['kernel_target'] += kernel_target.item() / len(train_loader)\n",
    "                loss_terms_batch['joint_embedding'] += joint_embedding.item() / len(train_loader)\n",
    "                loss_terms_batch['target_decoding'] += target_decoding.item() / len(train_loader)\n",
    "            loss_terms_batch['epoch'] = epoch\n",
    "            loss_terms.append(loss_terms_batch)\n",
    "\n",
    "            model.eval()\n",
    "            mae_batch = 0\n",
    "            with torch.no_grad():\n",
    "                for (features, targets) in test_loader:\n",
    "                    bsz = targets.shape[0]\n",
    "                    n_views = 1\n",
    "                    n_feat = features.shape[-1]\n",
    "                    \n",
    "                    if len(features.shape) > 2:\n",
    "                        n_views = features.shape[1]\n",
    "                        features = features.view(bsz * n_views, n_feat)\n",
    "                        \n",
    "                    features, targets = features.to(device), targets.to(device)\n",
    "                    \n",
    "                    out_feat = model.transform_feat(features)\n",
    "                    out_target_decoded = model.decode_target(out_feat)\n",
    "                    \n",
    "                    mae_batch += (targets - out_target_decoded).abs().mean() / len(test_loader)\n",
    "                validation.append(mae_batch.item())\n",
    "            scheduler.step(mae_batch)\n",
    "            if np.log10(scheduler._last_lr[0]) < -4:\n",
    "                break\n",
    "            \n",
    "            pbar.set_postfix_str(\n",
    "                f\"Epoch {epoch} \"\n",
    "                f\"| Loss {loss_terms[-1]['loss']:.02f} \"\n",
    "                f\"| val MAE {validation[-1]:.02f}\"\n",
    "                f\"| log10 lr {np.log10(scheduler._last_lr[0])}\"\n",
    "            )\n",
    "            \n",
    "    loss_terms = pd.DataFrame(loss_terms)\n",
    "    return loss_terms, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment(submitit.helpers.Checkpointable):\n",
    "    def __init__(self):\n",
    "        self.results = None\n",
    "\n",
    "    def __call__(self, train, test_size, indices, train_ratio, experiment_size, experiment, dataset, random_state=None, device=None, path: Path = None):\n",
    "        if self.results is None:\n",
    "            if device is None:\n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                print(f\"Device {device}, ratio {train_ratio}\", flush=True)\n",
    "            if not isinstance(random_state, np.random.RandomState):\n",
    "                random_state = np.random.RandomState(random_state)\n",
    "\n",
    "            # if dataset is None:\n",
    "            #     print(\"Loading data\", flush=True)\n",
    "            #     dataset = MatData(\n",
    "            #         data_path / \"vectorized_matrices.npy\",\n",
    "            #         data_path / \"participants.csv\",\n",
    "            #         \"age\",\n",
    "            #         threshold=threshold\n",
    "            #     )\n",
    "\n",
    "            # print(\"Data loaded\", flush=True)\n",
    "            predictions = {}\n",
    "            losses = []\n",
    "            experiment_indices = random_state.choice(indices, experiment_size, replace=False)\n",
    "            train_indices, test_indices = train_test_split(experiment_indices, test_size=test_size, random_state=random_state)\n",
    "            #train_dataset = Subset(dataset, train_indices)\n",
    "            train_dataset = dataset.index_select(train_indices)\n",
    "            \n",
    "            #test_dataset = Subset(dataset, test_indices)\n",
    "            test_dataset = dataset.index_select(test_indices)\n",
    "            ### Augmentation\n",
    "            n_views = 1\n",
    "            \n",
    "            train_features = train_dataset.features#.numpy()\n",
    "            #train_features = train_dataset.matrices.numpy()\n",
    "            #train_features = np.array([data.x for data in train_dataset])\n",
    "           \n",
    "\n",
    "            train_targets = train_dataset.targets.numpy()\n",
    "            #train_targets = [data.y.numpy() for data in train_dataset]\n",
    "\n",
    "            #train_edge_indices = [data.edge_index for data in train_dataset]\n",
    "            train_edge_indices = train_dataset.edge_indices.numpy()\n",
    "\n",
    "            #test_features= train_dataset.dataset.matrices[test_dataset.indices]#.numpy()\n",
    "            test_features= test_dataset.features#.numpy()\n",
    "\n",
    "            #test_targets = train_dataset.dataset.target[test_dataset.indices].numpy()\n",
    "            #test_targets = [data.y.numpy() for data in test_dataset]#.numpy()\n",
    "            test_targets = test_dataset.targets.numpy()\n",
    "\n",
    "            test_edge_indices = test_dataset.edge_indices.numpy()\n",
    "\n",
    "            if AUGMENTATION is not None:\n",
    "                transform = augs[AUGMENTATION]\n",
    "                aug_features = np.array([transform(sample) for sample in train_features])\n",
    "\n",
    "                train_features = sym_matrix_to_vec(train_features, discard_diagonal=True)\n",
    "                aug_features = sym_matrix_to_vec(train_features, discard_diagonal=True)\n",
    "\n",
    "                n_views = n_views + aug_features.shape[1]\n",
    "                n_features = train_features.shape[-1]\n",
    "                n_samples = len(train_dataset)\n",
    "\n",
    "                new_train_features = np.zeros((n_samples, n_views, n_features))\n",
    "                new_train_features[:, 0, :] = train_features\n",
    "                new_train_features[:, 1:, :] = aug_features\n",
    "            else:\n",
    "                train_features = sym_matrix_to_vec(train_features, discard_diagonal=True)\n",
    "                train_features = np.expand_dims(train_features, axis = 1)\n",
    "                #train_features = train_features\n",
    "            \n",
    "            train_dataset = TensorDataset(torch.from_numpy(train_features).to(torch.float32), torch.from_numpy(train_targets).to(torch.float32))\n",
    "            #train_dataset = TensorDataset(train_features.to(torch.float32), train_targets.to(torch.float32))\n",
    "            test_features = sym_matrix_to_vec(test_features, discard_diagonal=True)\n",
    "            #test_features = test_features\n",
    "            test_dataset = TensorDataset(torch.from_numpy(test_features).to(torch.float32), torch.from_numpy(test_targets).to(torch.float32))\n",
    "            #test_dataset = TensorDataset(test_features.to(torch.float32), test_targets.to(torch.float32))\n",
    "            loss_terms, model = train(train_dataset, test_dataset, model = None, device=device)\n",
    "            losses.append(loss_terms.eval(\"train_ratio = @train_ratio\").eval(\"experiment = @experiment\"))\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                #train_dataset = Subset(dataset, train_indices)\n",
    "                train_dataset = dataset.index_select(train_indices)\n",
    "\n",
    "                #train_features = train_dataset.dataset.matrices[train_dataset.indices].numpy()\n",
    "                train_features = train_dataset.features.numpy()\n",
    "                #train_targets = train_dataset.dataset.target[train_dataset.indices].numpy()\n",
    "                train_targets = train_dataset.targets.numpy()\n",
    "                #train_features = np.array([sym_matrix_to_vec(i, discard_diagonal=True) for i in train_features])\n",
    "                train_edge_indices = train_dataset.edge_indices.numpy()\n",
    "\n",
    "                #train_features = preprocess_features(train_features)\n",
    "                train_dataset = TensorDataset(torch.from_numpy(train_features).to(torch.float32), torch.from_numpy(train_targets).to(torch.float32))\n",
    "                for label, d, d_indices in (('train', train_dataset, train_indices), ('test', test_dataset, test_indices)):\n",
    "                    X, y = zip(*d)\n",
    "                    X = torch.stack(X).to(device)\n",
    "                    y = torch.stack(y).to(device)\n",
    "                    y_pred = model.decode_target(model.transform_feat(X))\n",
    "                    predictions[(train_ratio, experiment, label)] = (y.cpu().numpy(), y_pred.cpu().numpy(), d_indices)\n",
    "\n",
    "            self.results = (losses, predictions)\n",
    "\n",
    "        if path:\n",
    "            self.save(path)\n",
    "        \n",
    "        return self.results\n",
    "\n",
    "    def checkpoint(self, *args, **kwargs):\n",
    "        print(\"Checkpointing\", flush=True)\n",
    "        return super().checkpoint(*args, **kwargs)\n",
    "\n",
    "    def save(self, path: Path):\n",
    "        with open(path, \"wb\") as o:\n",
    "            pickle.dump(self.results, o, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.RandomState(seed=42)\n",
    "dataset = GraphData(path_feat, path_target, \"age\")\n",
    "n_sub = len(dataset)\n",
    "test_ratio = .2\n",
    "test_size = int(test_ratio * n_sub)\n",
    "indices = np.arange(n_sub)\n",
    "experiments = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Size:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cpu, ratio 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Training Size:   0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/data/parietal/store2/work/mrenaudi/contrastive-reg-2/graph_rep_2.ipynb Cell 15\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmargaret/data/parietal/store2/work/mrenaudi/contrastive-reg-2/graph_rep_2.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mfor\u001b[39;00m experiment \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(experiments), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExperiment\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmargaret/data/parietal/store2/work/mrenaudi/contrastive-reg-2/graph_rep_2.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m     run_experiment \u001b[39m=\u001b[39m Experiment()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmargaret/data/parietal/store2/work/mrenaudi/contrastive-reg-2/graph_rep_2.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m     job \u001b[39m=\u001b[39m run_experiment(train,  test_size, indices, train_ratio, experiment_size, experiment, dataset, random_state\u001b[39m=\u001b[39;49mrandom_state, device\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmargaret/data/parietal/store2/work/mrenaudi/contrastive-reg-2/graph_rep_2.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m     experiment_results\u001b[39m.\u001b[39mappend(job)\n",
      "\u001b[1;32m/data/parietal/store2/work/mrenaudi/contrastive-reg-2/graph_rep_2.ipynb Cell 15\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmargaret/data/parietal/store2/work/mrenaudi/contrastive-reg-2/graph_rep_2.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m     train_features \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(train_features, axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmargaret/data/parietal/store2/work/mrenaudi/contrastive-reg-2/graph_rep_2.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m     \u001b[39m#train_features = train_features\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmargaret/data/parietal/store2/work/mrenaudi/contrastive-reg-2/graph_rep_2.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m TensorDataset(torch\u001b[39m.\u001b[39mfrom_numpy(train_features)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32), train_targets\u001b[39m.\u001b[39;49mto(torch\u001b[39m.\u001b[39mfloat32))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmargaret/data/parietal/store2/work/mrenaudi/contrastive-reg-2/graph_rep_2.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m#train_dataset = TensorDataset(train_features.to(torch.float32), train_targets.to(torch.float32))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmargaret/data/parietal/store2/work/mrenaudi/contrastive-reg-2/graph_rep_2.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m test_features \u001b[39m=\u001b[39m sym_matrix_to_vec(test_features, discard_diagonal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# %% ## Training\n",
    "if multi_gpu:\n",
    "    log_folder = Path(\"log_folder\")\n",
    "    executor = submitit.AutoExecutor(folder=str(log_folder / \"%j\"))\n",
    "    executor.update_parameters(\n",
    "        #timeout_min=120,\n",
    "        slurm_partition=\"gpu-best\",\n",
    "        gpus_per_node=1,\n",
    "        tasks_per_node=1,\n",
    "        nodes=1,\n",
    "        cpus_per_task=8\n",
    "        #slurm_qos=\"qos_gpu-t3\",\n",
    "        #slurm_constraint=\"v100-32g\",\n",
    "        #slurm_mem=\"10G\",\n",
    "        #slurm_additional_parameters={\"requeue\": True}\n",
    "    )\n",
    "    # srun -n 1  --verbose -A hjt@v100 -c 10 -C v100-32g   --gres=gpu:1 --time 5  python\n",
    "    experiment_jobs = []\n",
    "    # module_purge = submitit.helpers.CommandFunction(\"module purge\".split())\n",
    "    # module_load = submitit.helpers.CommandFunction(\"module load pytorch-gpu/py3/2.0.1\".split())\n",
    "    with executor.batch():\n",
    "        for train_ratio in tqdm(np.linspace(.1, 1., 5)):\n",
    "            train_size = int(n_sub * (1 - test_ratio) * train_ratio)\n",
    "            experiment_size = test_size + train_size\n",
    "            for experiment in tqdm(range(experiments)):\n",
    "                run_experiment = Experiment()\n",
    "                job = executor.submit(run_experiment, train, test_size, indices, train_ratio, experiment_size, experiment, dataset, random_state=random_state, device=None)\n",
    "                experiment_jobs.append(job)\n",
    "\n",
    "    async def get_result(experiment_jobs):\n",
    "        experiment_results = []\n",
    "        for aws in tqdm(asyncio.as_completed([j.awaitable().result() for j in experiment_jobs]), total=len(experiment_jobs)):\n",
    "            res = await aws\n",
    "            experiment_results.append(res)\n",
    "        return experiment_results\n",
    "    experiment_results = asyncio.run(get_result(experiment_jobs))\n",
    "\n",
    "else:\n",
    "    experiment_results = []\n",
    "    for train_ratio in tqdm(np.linspace(.1, 1., 5), desc=\"Training Size\"):\n",
    "        train_size = int(n_sub * (1 - test_ratio) * train_ratio)\n",
    "        experiment_size = test_size + train_size\n",
    "        for experiment in tqdm(range(experiments), desc=\"Experiment\"):\n",
    "            run_experiment = Experiment()\n",
    "            job = run_experiment(train,  test_size, indices, train_ratio, experiment_size, experiment, dataset, random_state=random_state, device=None)\n",
    "            experiment_results.append(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_indices = random_state.choice(indices, experiment_size, replace=False)\n",
    "train_indices, test_indices = train_test_split(experiment_indices, test_size=test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9, 11,  0])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.index_select(train_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dataset.index_select(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = train_dataset.targets.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_edge_indices = train_dataset[2]#.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x7f5c65c9e8d0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TensorDataset(torch.from_numpy(train_features).to(torch.float32), torch.from_numpy(train_target).to(torch.float32))#, torch.from_numpy(train_edge_indices).to(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30.],\n",
       "       [32.],\n",
       "       [34.],\n",
       "       [20.],\n",
       "       [56.],\n",
       "       [55.],\n",
       "       [43.],\n",
       "       [21.],\n",
       "       [51.],\n",
       "       [20.],\n",
       "       [53.],\n",
       "       [22.],\n",
       "       [22.],\n",
       "       [24.],\n",
       "       [37.]], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.00000000e+00,  3.16017956e-01,  3.98602784e-01, ...,\n",
       "          2.32582450e-01,  3.34294766e-01,  2.70993829e-01],\n",
       "        [ 3.16017956e-01,  1.00000000e+00,  5.59164822e-01, ...,\n",
       "         -6.32898300e-04, -6.93037501e-03, -4.22096774e-02],\n",
       "        [ 3.98602784e-01,  5.59164822e-01,  1.00000000e+00, ...,\n",
       "          2.82288760e-01,  3.78347576e-01,  4.23787028e-01],\n",
       "        ...,\n",
       "        [ 2.32582450e-01, -6.32898300e-04,  2.82288760e-01, ...,\n",
       "          1.00000000e+00,  9.57663536e-01,  9.71337020e-01],\n",
       "        [ 3.34294766e-01, -6.93037501e-03,  3.78347576e-01, ...,\n",
       "          9.57663536e-01,  1.83684006e+01,  1.06736660e+00],\n",
       "        [ 2.70993829e-01, -4.22096774e-02,  4.23787028e-01, ...,\n",
       "          9.71337020e-01,  1.06736660e+00,  1.00000000e+00]],\n",
       "\n",
       "       [[ 1.00000000e+00,  5.09892225e-01,  4.56054688e-01, ...,\n",
       "          3.98789436e-01,  4.27864224e-01,  4.00093228e-01],\n",
       "        [ 5.09892225e-01,  1.83684006e+01,  6.87724590e-01, ...,\n",
       "          1.37597620e-01,  1.79800436e-01,  6.03553988e-02],\n",
       "        [ 4.56054688e-01,  6.87724590e-01,  1.00000000e+00, ...,\n",
       "          2.07275361e-01,  2.18966275e-01,  9.63982102e-03],\n",
       "        ...,\n",
       "        [ 3.98789436e-01,  1.37597620e-01,  2.07275361e-01, ...,\n",
       "          1.00000000e+00,  1.11582100e+00,  1.11949968e+00],\n",
       "        [ 4.27864224e-01,  1.79800436e-01,  2.18966275e-01, ...,\n",
       "          1.11582100e+00,  1.00000000e+00,  1.67706728e+00],\n",
       "        [ 4.00093228e-01,  6.03553988e-02,  9.63982102e-03, ...,\n",
       "          1.11949968e+00,  1.67706728e+00,  1.00000000e+00]],\n",
       "\n",
       "       [[ 1.00000000e+00,  7.24815011e-01,  8.16604257e-01, ...,\n",
       "          2.46250913e-01,  8.02583098e-02,  2.53019363e-01],\n",
       "        [ 7.24815011e-01,  1.00000000e+00,  1.11292481e+00, ...,\n",
       "          7.31878877e-02, -2.27084570e-03,  1.51197612e-01],\n",
       "        [ 8.16604257e-01,  1.11292481e+00,  1.00000000e+00, ...,\n",
       "          6.49773553e-02,  1.44070357e-01,  3.57482255e-01],\n",
       "        ...,\n",
       "        [ 2.46250913e-01,  7.31878877e-02,  6.49773553e-02, ...,\n",
       "          1.00000000e+00,  1.39255285e-01,  2.57460922e-01],\n",
       "        [ 8.02583098e-02, -2.27084570e-03,  1.44070357e-01, ...,\n",
       "          1.39255285e-01,  1.00000000e+00,  1.10825109e+00],\n",
       "        [ 2.53019363e-01,  1.51197612e-01,  3.57482255e-01, ...,\n",
       "          2.57460922e-01,  1.10825109e+00,  1.00000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.00000000e+00,  6.94963098e-01,  9.11594987e-01, ...,\n",
       "          7.21947253e-01,  2.53180116e-01,  3.68505031e-01],\n",
       "        [ 6.94963098e-01,  1.00000000e+00,  7.84369588e-01, ...,\n",
       "          6.31049037e-01,  1.46641731e-01,  2.96900421e-01],\n",
       "        [ 9.11594987e-01,  7.84369588e-01,  1.00000000e+00, ...,\n",
       "          8.32409203e-01,  2.96507567e-01,  4.93098915e-01],\n",
       "        ...,\n",
       "        [ 7.21947253e-01,  6.31049037e-01,  8.32409203e-01, ...,\n",
       "          1.00000000e+00,  4.26701605e-01,  7.86132991e-01],\n",
       "        [ 2.53180116e-01,  1.46641731e-01,  2.96507567e-01, ...,\n",
       "          4.26701605e-01,  1.00000000e+00,  9.13797975e-01],\n",
       "        [ 3.68505031e-01,  2.96900421e-01,  4.93098915e-01, ...,\n",
       "          7.86132991e-01,  9.13797975e-01,  1.00000000e+00]],\n",
       "\n",
       "       [[ 1.00000000e+00,  7.74209201e-01,  1.00477779e+00, ...,\n",
       "          2.88796633e-01,  2.46898532e-01,  2.21849695e-01],\n",
       "        [ 7.74209201e-01,  1.00000000e+00,  1.17544353e+00, ...,\n",
       "          1.60269998e-02, -1.20085776e-01,  5.71978837e-02],\n",
       "        [ 1.00477779e+00,  1.17544353e+00,  1.83684006e+01, ...,\n",
       "          2.78684556e-01,  2.45234564e-01,  3.24964106e-01],\n",
       "        ...,\n",
       "        [ 2.88796633e-01,  1.60269998e-02,  2.78684556e-01, ...,\n",
       "          1.00000000e+00,  6.55645490e-01,  1.01298130e+00],\n",
       "        [ 2.46898532e-01, -1.20085776e-01,  2.45234564e-01, ...,\n",
       "          6.55645490e-01,  1.00000000e+00,  9.11400616e-01],\n",
       "        [ 2.21849695e-01,  5.71978837e-02,  3.24964106e-01, ...,\n",
       "          1.01298130e+00,  9.11400616e-01,  1.00000000e+00]],\n",
       "\n",
       "       [[ 1.00000000e+00,  3.54514480e-01,  4.23652798e-01, ...,\n",
       "          2.61441410e-01,  5.49922064e-02,  4.21047568e-01],\n",
       "        [ 3.54514480e-01,  1.00000000e+00,  9.13146913e-01, ...,\n",
       "         -4.39248174e-01,  8.34473073e-02,  8.93260315e-02],\n",
       "        [ 4.23652798e-01,  9.13146913e-01,  1.00000000e+00, ...,\n",
       "         -4.11839485e-01,  4.01009843e-02,  3.08297575e-01],\n",
       "        ...,\n",
       "        [ 2.61441410e-01, -4.39248174e-01, -4.11839485e-01, ...,\n",
       "          1.83684006e+01,  3.31872672e-01,  4.26161945e-01],\n",
       "        [ 5.49922064e-02,  8.34473073e-02,  4.01009843e-02, ...,\n",
       "          3.31872672e-01,  1.00000000e+00,  6.75896049e-01],\n",
       "        [ 4.21047568e-01,  8.93260315e-02,  3.08297575e-01, ...,\n",
       "          4.26161945e-01,  6.75896049e-01,  1.00000000e+00]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset._indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_edge_indices = [train_dataset[i][2] for i in range(len(train_dataset))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  0,   0,   0],\n",
       "         [  0,   0,   1],\n",
       "         [  0,   0,   2],\n",
       "         ...,\n",
       "         [ 14, 999, 997],\n",
       "         [ 14, 999, 998],\n",
       "         [ 14, 999, 999]]),\n",
       " tensor([[  0,   0,   0],\n",
       "         [  0,   0,   1],\n",
       "         [  0,   0,   2],\n",
       "         ...,\n",
       "         [ 14, 999, 997],\n",
       "         [ 14, 999, 998],\n",
       "         [ 14, 999, 999]]),\n",
       " tensor([[  0,   0,   0],\n",
       "         [  0,   0,   1],\n",
       "         [  0,   0,   2],\n",
       "         ...,\n",
       "         [ 14, 999, 997],\n",
       "         [ 14, 999, 998],\n",
       "         [ 14, 999, 999]]),\n",
       " tensor([[  0,   0,   0],\n",
       "         [  0,   0,   1],\n",
       "         [  0,   0,   2],\n",
       "         ...,\n",
       "         [ 14, 999, 997],\n",
       "         [ 14, 999, 998],\n",
       "         [ 14, 999, 999]]),\n",
       " tensor([[  0,   0,   0],\n",
       "         [  0,   0,   1],\n",
       "         [  0,   0,   2],\n",
       "         ...,\n",
       "         [ 14, 999, 997],\n",
       "         [ 14, 999, 998],\n",
       "         [ 14, 999, 999]]),\n",
       " tensor([[  0,   0,   0],\n",
       "         [  0,   0,   1],\n",
       "         [  0,   0,   2],\n",
       "         ...,\n",
       "         [ 14, 999, 997],\n",
       "         [ 14, 999, 998],\n",
       "         [ 14, 999, 999]]),\n",
       " tensor([[  0,   0,   0],\n",
       "         [  0,   0,   1],\n",
       "         [  0,   0,   2],\n",
       "         ...,\n",
       "         [ 14, 999, 997],\n",
       "         [ 14, 999, 998],\n",
       "         [ 14, 999, 999]]),\n",
       " tensor([[  0,   0,   0],\n",
       "         [  0,   0,   1],\n",
       "         [  0,   0,   2],\n",
       "         ...,\n",
       "         [ 14, 999, 997],\n",
       "         [ 14, 999, 998],\n",
       "         [ 14, 999, 999]]),\n",
       " tensor([[  0,   0,   0],\n",
       "         [  0,   0,   1],\n",
       "         [  0,   0,   2],\n",
       "         ...,\n",
       "         [ 14, 999, 997],\n",
       "         [ 14, 999, 998],\n",
       "         [ 14, 999, 999]]),\n",
       " tensor([[  0,   0,   0],\n",
       "         [  0,   0,   1],\n",
       "         [  0,   0,   2],\n",
       "         ...,\n",
       "         [ 14, 999, 997],\n",
       "         [ 14, 999, 998],\n",
       "         [ 14, 999, 999]]),\n",
       " tensor([[  0,   0,   0],\n",
       "         [  0,   0,   1],\n",
       "         [  0,   0,   2],\n",
       "         ...,\n",
       "         [ 14, 999, 997],\n",
       "         [ 14, 999, 998],\n",
       "         [ 14, 999, 999]]),\n",
       " tensor([[  0,   0,   0],\n",
       "         [  0,   0,   1],\n",
       "         [  0,   0,   2],\n",
       "         ...,\n",
       "         [ 14, 999, 997],\n",
       "         [ 14, 999, 998],\n",
       "         [ 14, 999, 999]]),\n",
       " tensor([[  0,   0,   0],\n",
       "         [  0,   0,   1],\n",
       "         [  0,   0,   2],\n",
       "         ...,\n",
       "         [ 14, 999, 997],\n",
       "         [ 14, 999, 998],\n",
       "         [ 14, 999, 999]]),\n",
       " tensor([[  0,   0,   0],\n",
       "         [  0,   0,   1],\n",
       "         [  0,   0,   2],\n",
       "         ...,\n",
       "         [ 14, 999, 997],\n",
       "         [ 14, 999, 998],\n",
       "         [ 14, 999, 999]]),\n",
       " tensor([[  0,   0,   0],\n",
       "         [  0,   0,   1],\n",
       "         [  0,   0,   2],\n",
       "         ...,\n",
       "         [ 14, 999, 997],\n",
       "         [ 14, 999, 998],\n",
       "         [ 14, 999, 999]])]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_edge_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
