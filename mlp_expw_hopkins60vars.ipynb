{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from cmath import isinf\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import math\n",
    "from utils_v import compute_target_score, estimate_target, save_model, standardize_dataset\n",
    "from cmath import isinf\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split, KFold, LearningCurveDisplay, learning_curve\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_percentage_error, r2_score\n",
    "from helper_classes import MatData, MLP\n",
    "from dev_losses import cauchy, rbf, gaussian_kernel, CustomSupCon, CustomContrastiveLoss\n",
    "from losses import KernelizedSupCon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x, krnl_sigma = 0.1):\n",
    "    kernelized = []\n",
    "    for var in x.T:\n",
    "        krnl_var = var.unsqueeze(-1).T - var.unsqueeze(-1)\n",
    "        kernelized.append(krnl_var)\n",
    "    kernelized = sum([krnl_var**2 for krnl_var in kernelized])\n",
    "    return torch.exp(-kernelized / (2*(krnl_sigma**2))) / (math.sqrt(krnl_sigma*torch.pi)*krnl_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs3/well/margulies/users/cpy397/contrastive-learning\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MatData(\"vectorized_matrices_la5c.npy\", \"hopkins_covars.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, test_indices = train_test_split(np.arange(len(dataset)), test_size = 0.2, random_state=42) #train_size = 5\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "test_dataset = Subset(dataset, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelizedSupCon(nn.Module):\n",
    "    \"\"\"Supervised contrastive loss: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\n",
    "    Based on: https://github.com/HobbitLong/SupContrast\"\"\"\n",
    "    def __init__(self, method: str, krnl_sigma = None, temperature: float=0.07, contrast_mode: str='all',\n",
    "                 base_temperature: float=0.07, kernel: callable=None, delta_reduction: str='sum'):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "        self.method = method\n",
    "        self.kernel = kernel\n",
    "        self.delta_reduction = delta_reduction\n",
    "        self.krnl_sigma = krnl_sigma\n",
    "\n",
    "        if kernel is not None and method == 'supcon':\n",
    "            raise ValueError('Kernel must be none if method=supcon')\n",
    "        \n",
    "        if kernel is None and method != 'supcon':\n",
    "            raise ValueError('Kernel must not be none if method != supcon')\n",
    "\n",
    "        if delta_reduction not in ['mean', 'sum']:\n",
    "            raise ValueError(f\"Invalid reduction {delta_reduction}\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__} ' \\\n",
    "               f'(t={self.temperature}, ' \\\n",
    "               f'method={self.method}, ' \\\n",
    "               f'kernel={self.kernel is not None}, ' \\\n",
    "               f'delta_reduction={self.delta_reduction}, ' \\\n",
    "               f'krnl_sigma={self.krnl_sigma}'\n",
    "                \n",
    "\n",
    "    def forward(self, features, labels=None):\n",
    "        \"\"\"Compute loss for model. If `labels` is None, \n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, n_features]. \n",
    "                input has to be rearranged to [bsz, n_views, n_features] and labels [bsz],\n",
    "            labels: ground truth of shape [bsz].\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "        device = features.device\n",
    "\n",
    "        if len(features.shape) != 3:\n",
    "            raise ValueError('`features` needs to be [bsz, n_views, n_feats],'\n",
    "                             '3 dimensions are required')\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        n_views = features.shape[1]\n",
    "\n",
    "        if labels is None:\n",
    "            mask = torch.eye(batch_size, device=device)\n",
    "        \n",
    "        else:\n",
    "            # labels = labels.view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError(f'Num of labels does not match num of features: {labels.shape[0]} vs. {batch_size}')\n",
    "            \n",
    "            if self.kernel is None:\n",
    "                mask = torch.eq(labels, labels.T)\n",
    "            else:\n",
    "                mask = self.kernel(labels, self.krnl_sigma)\n",
    "            \n",
    "        view_count = features.shape[1]\n",
    "        features = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == 'one':\n",
    "            features = features[:, 0]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == 'all':\n",
    "            features = features\n",
    "            anchor_count = view_count\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
    "\n",
    "        # Tile mask\n",
    "        mask = mask.repeat(anchor_count, view_count)\n",
    "\n",
    "        # Inverse of torch-eye to remove self-contrast (diagonal)\n",
    "        inv_diagonal = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size*n_views, device=device).view(-1, 1),\n",
    "            0\n",
    "        )\n",
    "\n",
    "        # compute similarity\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(features, features.T),\n",
    "            self.temperature\n",
    "        )\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "        alignment = logits \n",
    "\n",
    "        # base case is:\n",
    "        # - supcon if kernel = none \n",
    "        # - y-aware is kernel != none\n",
    "        uniformity = torch.exp(logits) * inv_diagonal \n",
    "\n",
    "        if self.method == 'threshold':\n",
    "            repeated = mask.unsqueeze(-1).repeat(1, 1, mask.shape[0]) # repeat kernel mask\n",
    "\n",
    "            delta = (mask[:, None].T - repeated.T).transpose(1, 2) # compute the difference w_k - w_j for every k,j\n",
    "            delta = (delta > 0.).float()\n",
    "\n",
    "            # for each z_i, repel only samples j s.t. K(z_i, z_j) < K(z_i, z_k)\n",
    "            uniformity = uniformity.unsqueeze(-1).repeat(1, 1, mask.shape[0])\n",
    "\n",
    "            if self.delta_reduction == 'mean':\n",
    "                uniformity = (uniformity * delta).mean(-1)\n",
    "            else:\n",
    "                uniformity = (uniformity * delta).sum(-1)\n",
    "    \n",
    "        elif self.method == 'expw':\n",
    "            # exp weight e^(s_j(1-w_j))\n",
    "            uniformity = torch.exp(logits * (1 - mask)) * inv_diagonal\n",
    "\n",
    "        uniformity = torch.log(uniformity.sum(1, keepdim=True))\n",
    "        # positive mask contains the anchor-positive pairs\n",
    "        # excluding <self,self> on the diagonal\n",
    "        positive_mask = mask * inv_diagonal\n",
    "\n",
    "        log_prob = alignment - uniformity # log(alignment/uniformity) = log(alignment) - log(uniformity)\n",
    "        log_prob = (positive_mask * log_prob).sum(1) / positive_mask.sum(1) # compute mean of log-likelihood over positive\n",
    " \n",
    "        # loss\n",
    "        loss = - (self.temperature / self.base_temperature) * log_prob\n",
    "        return loss.mean()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dim_feat = 499500 # vectorized mat, diagonal discarded\n",
    "# input_dim_target = 59\n",
    "# # the rest is arbitrary\n",
    "# hidden_dim_feat_1 = 1024\n",
    "# hidden_dim_feat_2 = 512\n",
    "# hidden_dim_target_1 = 24\n",
    "# hidden_dim_target_2 = 8\n",
    "# output_dim = 2\n",
    "# num_epochs = 1000\n",
    "\n",
    "input_dim_feat = 499500 # vectorized mat, diagonal discarded\n",
    "input_dim_target = 60\n",
    "# the rest is arbitrary\n",
    "hidden_dim_feat = 1000\n",
    "hidden_dim_target = 30\n",
    "output_dim = 2\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "temperature = 0.07\n",
    "base_temperature = 0.07 # too low values return nan loss\n",
    "\n",
    "lr = 0.01 # too low values return nan loss\n",
    "kernel = gaussian_kernel\n",
    "batch_size = 5 # too low values return nan loss\n",
    "dropout_rate = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x, krnl_sigma = 10):\n",
    "    kernelized = []\n",
    "    for var in x.T:\n",
    "        krnl_var = var.unsqueeze(-1).T - var.unsqueeze(-1)\n",
    "        kernelized.append(krnl_var)\n",
    "    kernelized = sum([krnl_var**2 for krnl_var in kernelized])\n",
    "    return torch.exp(-kernelized / (2*(krnl_sigma**2))) / (math.sqrt(krnl_sigma*torch.pi)*krnl_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_train_dataset = standardize_dataset(train_dataset)\n",
    "std_train_loader = DataLoader(standardized_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "standardized_test_dataset = standardize_dataset(test_dataset)\n",
    "std_test_loader = DataLoader(standardized_test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Mean Loss 2.4841293573379515\n",
      "Epoch 1 | Mean Loss 1.949931001663208\n",
      "Epoch 2 | Mean Loss 1.5125928163528441\n",
      "Epoch 3 | Mean Loss 1.4424650430679322\n",
      "Epoch 4 | Mean Loss 1.409980058670044\n",
      "Epoch 5 | Mean Loss 1.3905206203460694\n",
      "Epoch 6 | Mean Loss 1.3910221815109254\n",
      "Epoch 7 | Mean Loss 1.389836883544922\n",
      "Epoch 8 | Mean Loss 1.3897409439086914\n",
      "Epoch 9 | Mean Loss 1.3901603937149047\n",
      "Epoch 10 | Mean Loss 1.3881892681121826\n",
      "Epoch 11 | Mean Loss 1.3872150182724\n",
      "Epoch 12 | Mean Loss 1.3871769905090332\n",
      "Epoch 13 | Mean Loss 1.386087918281555\n",
      "Epoch 14 | Mean Loss 1.386769962310791\n",
      "Epoch 15 | Mean Loss 1.3852758646011352\n",
      "Epoch 16 | Mean Loss 1.3862598419189454\n",
      "Epoch 17 | Mean Loss 1.3867480039596558\n",
      "Epoch 18 | Mean Loss 1.3864712953567504\n",
      "Epoch 19 | Mean Loss 1.3843076705932618\n",
      "Epoch 20 | Mean Loss 1.3835798263549806\n",
      "Epoch 21 | Mean Loss 1.3831315279006957\n",
      "Epoch 22 | Mean Loss 1.3850612878799438\n",
      "Epoch 23 | Mean Loss 1.3880046367645265\n",
      "Epoch 24 | Mean Loss 1.3785629510879516\n",
      "Epoch 25 | Mean Loss 1.375819706916809\n",
      "Epoch 26 | Mean Loss 1.376909565925598\n",
      "Epoch 27 | Mean Loss 1.375400757789612\n",
      "Epoch 28 | Mean Loss 1.3706636428833008\n",
      "Epoch 29 | Mean Loss 1.3614473342895508\n",
      "Epoch 30 | Mean Loss 1.3780274391174316\n",
      "Epoch 31 | Mean Loss 1.388389778137207\n",
      "Epoch 32 | Mean Loss 1.3645437955856323\n",
      "Epoch 33 | Mean Loss 1.35564227104187\n",
      "Epoch 34 | Mean Loss 1.365780758857727\n",
      "Epoch 35 | Mean Loss 1.4057063102722167\n",
      "Epoch 36 | Mean Loss 1.3746114015579223\n",
      "Epoch 37 | Mean Loss 1.4068069219589234\n",
      "Epoch 38 | Mean Loss 1.3450296878814698\n",
      "Epoch 39 | Mean Loss 1.366129422187805\n",
      "Epoch 40 | Mean Loss 1.3686051607131957\n",
      "Epoch 41 | Mean Loss 1.3738075017929077\n",
      "Epoch 42 | Mean Loss 1.3615477561950684\n",
      "Epoch 43 | Mean Loss 1.3657212257385254\n",
      "Epoch 44 | Mean Loss 1.3529983282089233\n",
      "Epoch 45 | Mean Loss 1.3524850130081176\n",
      "Epoch 46 | Mean Loss 1.3413442373275757\n",
      "Epoch 47 | Mean Loss 1.3474775552749634\n",
      "Epoch 48 | Mean Loss 1.3356578588485717\n",
      "Epoch 49 | Mean Loss 1.3699704885482789\n",
      "Epoch 50 | Mean Loss 1.3902085304260254\n",
      "Epoch 51 | Mean Loss 1.3923493862152099\n",
      "Epoch 52 | Mean Loss 1.3583141326904298\n",
      "Epoch 53 | Mean Loss 1.3626039505004883\n",
      "Epoch 54 | Mean Loss 1.393023419380188\n",
      "Epoch 55 | Mean Loss 1.346091389656067\n",
      "Epoch 56 | Mean Loss 1.3511422395706176\n",
      "Epoch 57 | Mean Loss 1.396298360824585\n",
      "Epoch 58 | Mean Loss 1.3397773504257202\n",
      "Epoch 59 | Mean Loss 1.3533906936645508\n",
      "Epoch 60 | Mean Loss 1.33655743598938\n",
      "Epoch 61 | Mean Loss 1.3544950485229492\n",
      "Epoch 62 | Mean Loss 1.3443808078765869\n",
      "Epoch 63 | Mean Loss 1.3517762899398804\n",
      "Epoch 64 | Mean Loss 1.359166669845581\n",
      "Epoch 65 | Mean Loss 1.3359941005706788\n",
      "Epoch 66 | Mean Loss 1.3447691679000855\n",
      "Epoch 67 | Mean Loss 1.329976224899292\n",
      "Epoch 68 | Mean Loss 1.339403796195984\n",
      "Epoch 69 | Mean Loss 1.3314698219299317\n",
      "Epoch 70 | Mean Loss 1.3459400415420533\n",
      "Epoch 71 | Mean Loss 1.351373553276062\n",
      "Epoch 72 | Mean Loss 1.3461022615432738\n",
      "Epoch 73 | Mean Loss 1.3482893705368042\n",
      "Epoch 74 | Mean Loss 1.3756390333175659\n",
      "Epoch 75 | Mean Loss 1.3458452463150024\n",
      "Epoch 76 | Mean Loss 1.3468303203582763\n",
      "Epoch 77 | Mean Loss 1.3172993898391723\n",
      "Epoch 78 | Mean Loss 1.3486639499664306\n",
      "Epoch 79 | Mean Loss 1.323304867744446\n",
      "Epoch 80 | Mean Loss 1.3424687385559082\n",
      "Epoch 81 | Mean Loss 1.3518834590911866\n",
      "Epoch 82 | Mean Loss 1.3524860382080077\n",
      "Epoch 83 | Mean Loss 1.3408132314682006\n",
      "Epoch 84 | Mean Loss 1.3840412855148316\n",
      "Epoch 85 | Mean Loss 1.3409241676330566\n",
      "Epoch 86 | Mean Loss 1.3501145124435425\n",
      "Epoch 87 | Mean Loss 1.3510194778442384\n",
      "Epoch 88 | Mean Loss 1.4023403644561767\n",
      "Epoch 89 | Mean Loss 1.3414223432540893\n",
      "Epoch 90 | Mean Loss 1.3635286808013916\n",
      "Epoch 91 | Mean Loss 1.3411445140838623\n",
      "Epoch 92 | Mean Loss 1.3827399253845214\n",
      "Epoch 93 | Mean Loss 1.3402197122573853\n",
      "Epoch 94 | Mean Loss 1.339453387260437\n",
      "Epoch 95 | Mean Loss 1.3432597398757935\n",
      "Epoch 96 | Mean Loss 1.3473730087280273\n",
      "Epoch 97 | Mean Loss 1.3388136863708495\n",
      "Epoch 98 | Mean Loss 1.3317023515701294\n",
      "Epoch 99 | Mean Loss 1.399094247817993\n",
      "Training target estimator\n",
      "Training target estimator\n",
      "0.26523134 0.9557951330158874\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = MLP(input_dim_feat, input_dim_target, hidden_dim_feat, hidden_dim_target, output_dim, dropout_rate = 0).to(device)\n",
    "criterion_pft = KernelizedSupCon(method='expw', temperature=1, base_temperature=1, kernel=kernel, krnl_sigma = 10)\n",
    "criterion_ptt = KernelizedSupCon(method='expw', temperature=5, base_temperature=5, kernel=kernel, krnl_sigma = 5)\n",
    "\n",
    "# criterion = CustomKernelizedSupCon(temperature = temperature, base_temperature = base_temperature, kernel = kernel)\n",
    "# criterion = CustomSupCon('exp',temperature = temperature, base_temperature = base_temperature, kernel = kernel)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    batch_losses = []\n",
    "    for batch_num, (features, targets) in enumerate(std_train_loader):\n",
    "        features, targets = features.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out_feat, out_target = model(features, targets)\n",
    "        loss = criterion_pft(out_feat.unsqueeze(1), targets)\n",
    "        loss += criterion_ptt(out_target.unsqueeze(1), targets)\n",
    "        loss += torch.nn.functional.mse_loss(out_feat, out_target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        batch_losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch} | Mean Loss {sum(batch_losses)/len(batch_losses)}')\n",
    "\n",
    "\n",
    "mape_train, _ = compute_target_score(model, std_train_loader, std_test_loader, device, 'mape')\n",
    "r2_train, _ = compute_target_score(model, std_train_loader, std_test_loader, device, 'r2')\n",
    "# results_cv.append(['Overall', mape_train, r2_train, mape_val, r2_val])\n",
    "print(mape_train, r2_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = pd.DataFrame(results_cv, columns=['Fold', 'Train_MAPE', 'Train_R2', 'Val_MAPE', 'Val_R2'])\n",
    "# results_df.to_csv('cv_results_hopkins.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "# features = torch.vstack([test_dataset[i][0] for i in range(len(test_loader))])\n",
    "# targets = torch.vstack([test_dataset[i][1] for i in range(len(test_loader))])\n",
    "# features_mean, features_std, targets_mean, targets_std = compute_global_stats(test_dataset)\n",
    "# standardized_features = (features - features_mean) / features_std\n",
    "# standardized_targets = (targets - targets_mean) / targets_std\n",
    "# standardized_test_dataset = TensorDataset(standardized_features, standardized_targets)\n",
    "# test_loader = DataLoader(standardized_test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Loss:   0.42\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_losses = []\n",
    "emb_features = [] # saving the embedded features for each batch\n",
    "emb_targets = []\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    for batch_num, (features, targets) in enumerate(std_test_loader):\n",
    "        features = features.to(device).float()\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        out_feat, out_target = model(features, targets)\n",
    "        emb_features.append(out_feat.cpu())\n",
    "        emb_targets.append(out_target.cpu())\n",
    "        loss = criterion_pft(out_feat.unsqueeze(1), out_target)\n",
    "        test_losses.append(loss.item())\n",
    "        total_loss += loss.item() * features.size(0)\n",
    "        total_samples += features.size(0)\n",
    "        \n",
    "    test_losses =np.array(test_losses)\n",
    "    average_loss = total_loss / total_samples\n",
    "    print('Mean Test Loss: %6.2f' % (average_loss))\n",
    "    #np.save(f\"losses/test_losses_batch{batch_num}.npy\", test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_features = torch.row_stack(emb_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_targets = torch.row_stack(emb_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_features = pd.DataFrame(emb_features,columns = [\"Dim_1\", \"Dim_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_targets = pd.DataFrame(emb_targets,columns = [\"Dim_1\", \"Dim_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_features[\"sub\"] = np.arange(1, len(emb_features) +1)\n",
    "emb_targets[\"sub\"] = np.arange(1, len(emb_targets) +1)\n",
    "emb_features[\"Type\"] = 'Features'\n",
    "emb_targets[\"Type\"] = 'Targets'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs3/well/margulies/users/cpy397/.local/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/gpfs3/well/margulies/users/cpy397/.local/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/gpfs3/well/margulies/users/cpy397/.local/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/gpfs3/well/margulies/users/cpy397/.local/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/gpfs3/well/margulies/users/cpy397/.local/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/gpfs3/well/margulies/users/cpy397/.local/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f6ebd48f460>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# time to cry\n",
    "# I probably messed up the original loss. Went over it multiple times\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "sns.scatterplot(emb_features, x = 'Dim_1', y = 'Dim_2', s = 100, alpha = 0.8, hue = 'sub', palette = 'nipy_spectral', label='Features')\n",
    "sns.scatterplot(emb_targets, x = 'Dim_1', marker = 'v', y = 'Dim_2', s = 100, alpha = 0.8, hue = 'sub', palette = 'nipy_spectral', label='Targets')\n",
    "plt.xlim(-1.2, 1.2)\n",
    "plt.ylim(-1.2, 1.2)\n",
    "\n",
    "feature_handle = mlines.Line2D([], [], color='black', marker='o', linestyle='None', markersize=10, label='Features')\n",
    "target_handle = mlines.Line2D([], [], color='black', marker='v', linestyle='None', markersize=10, label='Targets')\n",
    "\n",
    "plt.legend(handles=[feature_handle, target_handle])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
