{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from cmath import isinf\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import math\n",
    "from cmath import isinf\n",
    "from helper_classes import MatData, CustomContrastiveLoss\n",
    "from utils_v import standardize_dataset, compute_target_score\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_attribute(model, attribute_name):\n",
    "    # Check if the model is wrapped with DataParallel\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        # Access attributes of the original model\n",
    "        return getattr(model.module, attribute_name)\n",
    "    else:\n",
    "        # Access attributes directly\n",
    "        return getattr(model, attribute_name)\n",
    "    \n",
    "def define_model(trial):\n",
    "        \n",
    "    dropout_rate = trial.suggest_float('dropout_rate',0.1, 0.5, step = 0.1)\n",
    "    hidden_dim_feat = trial.suggest_int('hidden_dim_feat', 100, 1100, step = 200)\n",
    "    hiddem_dim_target = 30\n",
    "    input_dim_feat = 499500\n",
    "    input_dim_target = 60\n",
    "    output_dim = 2\n",
    "\n",
    "\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self, input_dim_feat, input_dim_target, hidden_dim_feat, hidden_dim_target, output_dim):\n",
    "    #     def __init__(self, input_dim_feat, hidden_dim_feat, output_dim):\n",
    "            super(MLP, self).__init__()\n",
    "            self.hidden_dim_feat = hidden_dim_feat\n",
    "            self.dropout_rate = dropout_rate\n",
    "            self.feat_mlp = nn.Sequential(\n",
    "                nn.Linear(input_dim_feat, self.hidden_dim_feat),\n",
    "                nn.BatchNorm1d(self.hidden_dim_feat),\n",
    "                nn.ReLU(), # add more layers?\n",
    "                nn.Dropout(p=self.dropout_rate),\n",
    "                nn.Linear(self.hidden_dim_feat, output_dim)\n",
    "            )\n",
    "            self.target_mlp = nn.Sequential(\n",
    "                nn.Linear(input_dim_target, hidden_dim_target),\n",
    "                nn.BatchNorm1d(hidden_dim_target),\n",
    "                nn.ReLU(), # add more layers?\n",
    "                nn.Dropout(p=self.dropout_rate),\n",
    "                nn.Linear(hidden_dim_target, output_dim)\n",
    "            )\n",
    "            \n",
    "        def forward(self, x, y):\n",
    "            features = self.feat_mlp(x)\n",
    "            targets = self.target_mlp(y)\n",
    "            features = nn.functional.normalize(features, p=2, dim=1)\n",
    "            targets = nn.functional.normalize(targets, p=2, dim=1)\n",
    "            return features, targets\n",
    "        \n",
    "    return MLP(input_dim_feat, input_dim_target, hidden_dim_feat, hiddem_dim_target, output_dim)\n",
    "    \n",
    "def objective(trial, num_epochs):\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    num_workers = 4 * torch.cuda.device_count() if torch.cuda.is_available() else 4\n",
    "\n",
    "    model = define_model(trial)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs.\")\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    sigma = trial.suggest_float('sigma', 0.1, 1.0, step = 0.1)\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-1)\n",
    "    batch_size = trial.suggest_int('batch_size', 5, 30, step = 5)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    dataset = MatData(\"vectorized_matrices_la5c.npy\", \"hopkins_covars.npy\")\n",
    "    train_indices, test_indices = train_test_split(np.arange(len(dataset)), test_size = 0.2, random_state=42) #train_size = 5\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    test_dataset = Subset(dataset, test_indices)\n",
    "    \n",
    "    standardized_train_dataset = standardize_dataset(train_dataset)\n",
    "    std_train_loader = DataLoader(standardized_train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    \n",
    "    standardized_test_dataset = standardize_dataset(test_dataset)\n",
    "    std_test_loader = DataLoader(standardized_test_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = CustomContrastiveLoss(sigma = sigma)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        batch_losses = []\n",
    "        for batch_num, (features, targets) in enumerate(std_train_loader):\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out_feat, out_target = model(features, targets)\n",
    "            loss = criterion(out_feat, out_target)\n",
    "            loss.backward()\n",
    "            batch_losses.append(loss.item())\n",
    "            optimizer.step()\n",
    "        # print(f'Epoch {epoch} | Mean Loss {sum(batch_losses)/len(batch_losses)}')\n",
    "\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    emb_features = [] # saving the embedded features for each batch\n",
    "    emb_targets = []\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "        for batch_num, (features, targets) in enumerate(std_test_loader):\n",
    "            features = features.to(device).float()\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            out_feat, out_target = model(features, targets)\n",
    "            emb_features.append(out_feat.cpu())\n",
    "            emb_targets.append(out_target.cpu())\n",
    "            loss = criterion(out_feat, out_target)\n",
    "            test_losses.append(loss.item())\n",
    "            total_loss += loss.item() * features.size(0)\n",
    "            total_samples += features.size(0)\n",
    "            \n",
    "        test_losses =np.array(test_losses)\n",
    "        average_loss = total_loss / total_samples\n",
    "\n",
    "        print(f'Trial {trial}: Mean Test Loss: %6.2f' % (average_loss))\n",
    "\n",
    "    mape_train, mape_test = compute_target_score(model, std_train_loader, std_test_loader, device, 'mape')\n",
    "    r2_train,  r2_test = compute_target_score(model, std_train_loader, std_test_loader, device, 'r2')\n",
    "\n",
    "    print(f'MAPE on Train: {mape_train} | MAPE on Test: {mape_test}')\n",
    "    print(f'R2 on Train: {r2_train} | R2 on Test: {r2_test}')\n",
    "\n",
    "    with open('results/optim_results.csv', 'a') as f:\n",
    "        # create the csv writer\n",
    "        writer = csv.writer(f)\n",
    "        # if the csv file is empty, write the header\n",
    "        if f.tell() == 0:\n",
    "            writer.writerow(['hidden_dim_feat',\n",
    "                                'dropout_rate',\n",
    "                                'sigma',\n",
    "                                'lr',\n",
    "                                'weight_decay',\n",
    "                                'batch_size',\n",
    "                                'optimizer_name',\n",
    "                                'mean_test_loss',\n",
    "                                'mape_test',\n",
    "                                'r2_test'])\n",
    "            \n",
    "        writer.writerow([get_model_attribute(model, 'hidden_dim_feat'),\n",
    "                            get_model_attribute(model, 'dropout_rate'),\n",
    "                            sigma,\n",
    "                            lr,\n",
    "                            weight_decay,\n",
    "                            batch_size,\n",
    "                            optimizer_name,\n",
    "                            average_loss,\n",
    "                            mape_test,\n",
    "                            r2_test])\n",
    "            \n",
    "    return average_loss, mape_test, r2_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(directions=['minimize', 'minimize', 'maximize'])\n",
    "study.optimize(lambda trial: objective(trial, num_epochs=100), n_trials=1000)\n",
    "best_hyperparams = study.best_trial.params\n",
    "\n",
    "with open('results/best_hyperparameters.json', 'w') as f:\n",
    "    json.dump(best_hyperparams, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
